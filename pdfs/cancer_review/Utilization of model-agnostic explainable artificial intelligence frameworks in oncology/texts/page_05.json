{
    "page": {
        "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/pages/page_05.png",
        "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/pages_ordered/page_05.png",
        "image_width": 2481,
        "image_height": 3249,
        "regions_num": 8,
        "page_idx": 5
    },
    "regions": [
        {
            "idx": 1,
            "thing": "text",
            "score": 99.89,
            "box": [
                188.1,
                351.6,
                1204.5,
                1603.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/crops_ordered/page_05/region_1_text.png",
            "text": "plots that summarize all predictions within the datasets.\nXAI also has been used to examine prognostication in\nindividual predictions (i.e., patients). Both SHAP and\nLIME have this functionality implemented. In a study by\nJansen et a/., both SHAP and LIME are used to explain\nan XGB model of 10-year overall survival in breast cancer\npatients (17). In this study, the authors use LIME to model\nindividual patients and predictions of overall survival, which\nexplains how individual patients’ characteristics yield their\nprediction for 10-year overall survival. They did the same\nwith SHAP, but as detailed in Table 1, SHAP has improved\nfunctionality in illustrating global impact of features in all\npatients, so they also presented a summary plot. Lastly,\nthey compared all individual patient explanations produced\nby LIME and SHAP, demonstrating agreement in 87.8%\nto 99.9% (95.4% overall) of cases depending on the feature\nexamined. This study highlights key distinctions between\nLIME and SHAP; both approaches can yield consistent\nlocal results, but SHAP is able to examine global trends\nwithin models. The same approach, examining individual\npatients and overall trends has been performed on models\npredicting survival with nasopharyngeal cancer tumor\nburden (18).\n"
        },
        {
            "idx": 2,
            "thing": "text",
            "score": 99.91,
            "box": [
                188.5,
                1602.7,
                1204.6,
                2801.6
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/crops_ordered/page_05/region_2_text.png",
            "text": "Lastly, perhaps the way XAI was used most commonly\nfor prognostication and oncology in general was to\ndelineate global feature importance by summarizing all\nmodel predictions and outputting each feature’s mean\nimpact across all predictions. This permits a general\nunderstanding of how the overall model functions, similar\nto the summary statistics generated by regression models.\nMoncada-Torres et a/. used XAI to explain a general model\nof overall survival in breast cancer (19). Importantly,\nmodels such as Cox regression are routinely used for such\napplications, with outputs being readily interpretable by\nmost end-users. ML algorithms, namely the XGB tree\nalgorithm significantly outperformed Cox regression.\nHowever, the standard output of such an algorithm is not\nas readily understandable in terms of how its output is\ncomputed. In this example, the authors used the SHAP\nframework to identify feature importance within the\nmodel, which can be compared to intuition to build trust\nin the model. By using the SHAP framework, the study\nwas able to illustrate the prognostic significance of multiple\ncommonly used variables, thereby facilitating adoption of\nalgorithms such as XGB.\n"
        },
        {
            "idx": 3,
            "thing": "text",
            "score": 99.86,
            "box": [
                189.6,
                2800.8,
                1203.6,
                2963.5
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/crops_ordered/page_05/region_3_text.png",
            "text": "Several other studies have used similar approaches to the\naforementioned study to create powerful prognostic models\nto illustrate nuanced interactions that influence prognosis,\n"
        },
        {
            "idx": 4,
            "thing": "text",
            "score": 99.89,
            "box": [
                1287.5,
                350.7,
                2303.0,
                786.2
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/crops_ordered/page_05/region_4_text.png",
            "text": "with XAI used to explain their ML model on a global level.\nAdditional examples include predicting 30-day mortality\nfollowing colorectal cancer surgery (20), 5-year survival and\nesophageal cancer (21), characterizing influence of ethnicity\non outcomes and multiple myeloma (22), predicting hospital\nlength of stay (23), and risk of skeletal related events\nfollowing discontinuation of denosumab among patients\nwith bone metastases (24).\n"
        },
        {
            "idx": 5,
            "thing": "title",
            "score": 98.1,
            "box": [
                1288.2,
                868.5,
                1455.8,
                922.2
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/crops_ordered/page_05/region_5_title.png",
            "text": "Diagnosis\n"
        },
        {
            "idx": 6,
            "thing": "text",
            "score": 99.89,
            "box": [
                1287.2,
                950.7,
                2303.2,
                1276.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/crops_ordered/page_05/region_6_text.png",
            "text": "An additional area of research interest is ways to improve\ndiagnosis of cancer, where ML models have proven to\nbe valuable tools, but given the high stakes of a cancer\ndiagnosis, it is important that the predictions of such\nmodels are both accurate and easy to explain to clinicians\nand patients, representing a great opportunity for XAI.\n"
        },
        {
            "idx": 7,
            "thing": "text",
            "score": 99.89,
            "box": [
                1287.0,
                1278.0,
                2303.0,
                2202.9
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/crops_ordered/page_05/region_7_text.png",
            "text": "In one such application, Suh et a/. explored a model\npredicting prostate cancer in general as well as clinically\nsignificant prostate cancer prior to prostate biopsy (25).\nIn this study, the authors used XAI frameworks for one of\nthe reasons detailed in the previous section (understanding\nglobal feature importance), as well as a new one (feature\nselection/construction for building clinically-relevant tools).\nWhen explained using SHAP summary visualizations,\nthe authors identified important features predicting\nprostate cancer and clinically significant prostate cancer,\nwhich included known predictive factors such as PSA and\nGleason score, and how changing these factors individually\ninfluenced risk. These visualizations facilitated translation to\na risk calculator, via aiding selection of salient features, that\ncould be deployed as a data driven risk estimator (https://\nboramae-pcrc.appspot.com/) that would be generally\napplicable to the oncology clinic.\n"
        },
        {
            "idx": 8,
            "thing": "text",
            "score": 99.93,
            "box": [
                1287.3,
                2203.5,
                2303.1,
                2963.5
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Utilization of model-agnostic explainable artificial intelligence frameworks in oncology/crops_ordered/page_05/region_8_text.png",
            "text": "In another study, Kwong et a/. reported on a model that\npredicted side-specific extraprostatic extension and pre-\nprostatectomy patients (26), again using SHAP summaries\nto gauge global feature importance. Furthermore,\nthe authors examined the non-linear relationships\nbetween relevant factors and probability of site-specific\nextraprostatic extension with dependence plots, which is\nof clinical relevance given that features such as PPCs were\nrelatively noncontributory until reaching approximately\n75% based on inspection of dependence plots. Though\nthese conclusions are qualitative in nature, they inform\nquantitative hypotheses that can inform other statistical\napproaches and overall clinical intuition. Thus, the XAI\ncomplemented the ML model by not only providing\n"
        }
    ]
}