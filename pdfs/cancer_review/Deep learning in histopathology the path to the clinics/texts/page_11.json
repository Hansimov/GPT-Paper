{
    "page": {
        "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Deep learning in histopathology the path to the clinics/pages/page_11.png",
        "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Deep learning in histopathology the path to the clinics/pages_ordered/page_11.png",
        "image_width": 2481,
        "image_height": 3507,
        "regions_num": 3,
        "page_idx": 11
    },
    "regions": [
        {
            "idx": 1,
            "thing": "text",
            "score": 84.32,
            "box": [
                299.8,
                294.9,
                2195.3,
                1114.3
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Deep learning in histopathology the path to the clinics/crops_ordered/page_11/region_1_text.png",
            "text": "[H2] Validation of CPATH algorithms. Algorithm validation is crucial to understand the usefulness of\nCPATH algorithms for broad applications and to collect evidence on the safety and accuracy of\nalgorithms for regulatory approval. Different levels of validation can be used during algorithm\ndevelopment (Figure 2). Typically, CPATH algorithms are validated in multiple ways during\ndevelopment. As part of the actual algorithm construction, the training process is monitored using a\nset of cases that are held apart from the rest of the dataset and are therefore not used for model\ntraining (often referred to as ‘validation set’, which is usually relatively small). Deviations between the\nresults obtained with the training data and the validation set may indicate overtraining and suggests\nfurther action is required (e.g., use additional techniques such as data augmentation or model\nregularization, or reduce the complexity of the deep learning architecture).\n"
        },
        {
            "idx": 2,
            "thing": "text",
            "score": 84.28,
            "box": [
                301.1,
                1209.4,
                2194.8,
                2032.6
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Deep learning in histopathology the path to the clinics/crops_ordered/page_11/region_2_text.png",
            "text": "Many CPATH studies use a fully independent set of cases (a ‘test set’) to subsequently assess\nperformance of the final model. In most studies, these are from the same data source (so-called\n‘internal validation’) and as such have characteristics which are very similar to the cases used for\ntraining. If the training data set is of limited size, sometimes ‘cross validation’ is used rather than\napplying fully independent hold-out sets. In cross-validation, multiple models are trained with\ndifferent non-overlapping subsets of cases for testing and training, and an average performance score\nis given. Using cases that were not used for model training but were held separately from the rest of\nthe dataset for performance assessment is good practice to arrive at a first indication of how well the\nalgorithm works, but should be regarded as a first step only towards a realistic assessment of the\nusefulness in clinical practice [158].\n"
        },
        {
            "idx": 3,
            "thing": "text",
            "score": 85.47,
            "box": [
                300.3,
                2134.0,
                2194.6,
                3042.3
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Deep learning in histopathology the path to the clinics/crops_ordered/page_11/region_3_text.png",
            "text": "A next step, which has been used in several studies [100], [62], [72], [68], is to validate the CPATH\nalgorithm using an entirely separate set of cases from a source that was not included in the training\ndata (known as external validation) (Figure 2). Such validation gives an indication of how well the\nalgorithm performs in a new diagnostic situation, and can uncover problems with generalizability\n[118], [119]. The availability of publicly accessible benchmark datasets [54] [73] may be very helpful\nfor this purpose, as it allows fair comparison between different CPATH algorithms [120]. Such datasets\nmay also support regulatory approval [148]. However, even good performance on an external data set\nis not proof of the clinical usefulness of algorithms, and should not be regarded or reported as such\n[121]. Some of the hype around the promises of Al in the medical domain may in fact result from the\noverly optimistic interpretation of results of external validation study. As with any innovation in health\ncare, well-conducted prospective studies are required to provide the evidence necessary to truly\n"
        }
    ]
}