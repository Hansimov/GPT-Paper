{
    "page": {
        "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/pages/page_08.png",
        "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/pages_ordered/page_08.png",
        "image_width": 2481,
        "image_height": 3308,
        "regions_num": 23,
        "page_idx": 8
    },
    "regions": [
        {
            "idx": 1,
            "thing": "text",
            "score": 99.79,
            "box": [
                157.2,
                234.2,
                1202.6,
                320.6
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_01_text.png",
            "text": "and used this attention to further amplify relevant areas and sup-\npress irrelevant areas.\n"
        },
        {
            "idx": 2,
            "thing": "text",
            "score": 99.97,
            "box": [
                156.7,
                321.7,
                1205.1,
                756.5
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_02_text.png",
            "text": "In medical imaging, Schlemper et al. (2019) used trainable at-\ntention and introduced grid attention. The rationale behind this\nwas that most objects of interest in medical images are highly\nlocalized. By using grid attention, the trainable attention cap-\ntured the anatomical information in medical images. They demon-\nstrated high performance for both segmentation and localization,\nby adding the attention gates to a UNET (Ronneberger et al., 2015)\nand a variant of VGG (Simonyan and Zisserman, 2014). The atten-\ntion coefficients were used to explain on which areas of the image\nthe network focused.\n"
        },
        {
            "idx": 3,
            "thing": "title",
            "score": 99.55,
            "box": [
                156.9,
                802.9,
                706.7,
                844.4
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_03_title.png",
            "text": "3.1.2. Perturbation-based approaches\n"
        },
        {
            "idx": 4,
            "thing": "title",
            "score": 99.31,
            "box": [
                157.7,
                889.8,
                569.4,
                931.5
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_04_title.png",
            "text": "3.1.2.1. Occlusion sensitivity\n"
        },
        {
            "idx": 5,
            "thing": "text",
            "score": 99.98,
            "box": [
                156.3,
                932.6,
                1205.4,
                1280.3
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_05_text.png",
            "text": "Perturbation-based techniques perturb the input image to as-\nsess the importance of certain areas of that image for the task un-\nder consideration. Zeiler and Fergus (2014) used an occlusion sen-\nsitivity analysis to visualize which parts of the image were most\nimportant for classification. For example, they showed that an im-\nage of a dog holding a tennis ball was correctly classified by the\ndog’s breed, except if the face of the dog was occluded, which\nyielded the incorrect classification ‘tennis ball’.\n"
        },
        {
            "idx": 6,
            "thing": "title",
            "score": 99.46,
            "box": [
                157.5,
                1325.6,
                1097.1,
                1368.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_06_title.png",
            "text": "3.1.2.2. Local interpretable model-agnostic explanations (LIME)\n"
        },
        {
            "idx": 7,
            "thing": "text",
            "score": 99.98,
            "box": [
                157.7,
                1370.0,
                1206.1,
                1979.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_07_text.png",
            "text": "Ribeiro et al. (2016) introduced Local Interpretable Model-\nagnostic Explanations (LIME). LIME provides local explanation by\nreplacing a complex model locally with simpler models, for exam-\nple by approximating a CNN by a linear model. By perturbing the\ninput data, the output of the complex model changes. LIME uses\nthe simpler model to learn the mapping between the perturbed\ninput data and the change in output. The similarity of the per-\nturbed input to the original input is used as a weight, to ensure\nthat explanations provided by the simple models with highly per-\nturbed inputs have less effect on the final explanation. In images,\nRibeiro et al. (2016) implemented the perturbations using super-\npixelsxxxxxxXXxxxx is inclluded in the hyperlink\"?>Achanta et al.,\n2012), rather than individual pixels, to show which regions were\nimportant for explaining a classification.\n"
        },
        {
            "idx": 8,
            "thing": "text",
            "score": 99.95,
            "box": [
                155.9,
                1980.3,
                1204.6,
                2108.5
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_08_text.png",
            "text": "LIME has been used by several researchers in medical image\nanalysis. For example, Malhi et al. (2019) used LIME to explain\nwhich areas in gastral endoscopy images contained bloody regions.\n"
        },
        {
            "idx": 9,
            "thing": "title",
            "score": 98.86,
            "box": [
                158.7,
                2155.0,
                636.9,
                2196.9
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_09_title.png",
            "text": "3.1.2.3. Meaningful perturbation\n"
        },
        {
            "idx": 10,
            "thing": "text",
            "score": 99.97,
            "box": [
                156.3,
                2197.9,
                1205.0,
                2546.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_10_text.png",
            "text": "Fong and Vedaldi (2017) introduced meaningful perturbation,\nwhere they perturbed the input image to detect changes in the\npredictions of a trained neural network. Rather than using pertur-\nbations such as occlusion sensitivity that block out parts of the\nimage, they suggested simulating naturalistic or plausible effects,\nleading to more meaningful perturbations, and consequently to\nmore meaningful explanations. They opted for three types of local\nperturbations, namely a constant value, noise, or blurring.\n"
        },
        {
            "idx": 11,
            "thing": "text",
            "score": 99.97,
            "box": [
                157.6,
                2546.1,
                1205.3,
                3113.5
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_11_text.png",
            "text": "Uzunova et al. (2019) stated that the perturbations proposed by\nFong and Vedaldi (2017) were not suited for medical images. Re-\nplacing areas of a medical image with a constant value is implau-\nsible, and medical images naturally tend to be noisy and blurry.\nThey proposed to replace pathological regions with a healthy tis-\nSue equivalent using a variational autoencoder (VAE). They showed\nthat the perturbations by the VAE pinpoint pathological regions\nin diverse imaging studies as optical coherence tomography im-\nages of the eye (pathology consisted of intraretinal fluid, subretinal\nfluid, and pigment epithelium detachments), and MRI of the brain\n(pathology consisted of stroke lesions). Furthermore, they showed\nthat using a VAE yielded better localization of pathology compared\nwith using simple blurring or constant-value perturbations.\n"
        },
        {
            "idx": 12,
            "thing": "text",
            "score": 99.96,
            "box": [
                1277.1,
                235.7,
                2325.5,
                581.7
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_12_text.png",
            "text": "Lenis et al. (2020) used similar reasoning as\nUzunova et al. (2019), and used inpainting to replace pathological\nregions with healthy tissue equivalents. They showed that the\nperturbations created by inpainting outperformed backpropagation\nand Grad-CAM in pinpointing masses in breast mammography\nand tuberculosis on chest X-rays, based on the Hausdorff distance\nbetween thresholded heatmaps derived from the saliency maps\nand the ground truth labels at pixel level.\n"
        },
        {
            "idx": 13,
            "thing": "title",
            "score": 99.22,
            "box": [
                1278.0,
                638.2,
                1827.6,
                680.7
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_13_title.png",
            "text": "3.1.2.4. Prediction difference analysis\n"
        },
        {
            "idx": 14,
            "thing": "text",
            "score": 99.96,
            "box": [
                1276.2,
                682.8,
                2324.6,
                1204.7
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_14_text.png",
            "text": "Zintgraf et al. (2017) adapted prediction difference analysis\n(Robnik-Sikonja and Kononenko, 2008) for generating saliency\nmaps. If each pixel in an image is considered a feature, predic-\ntion difference analysis assigns a relevance value to each pixel, by\nmeasuring how the prediction changes if the pixel is considered\nunknown. Zintgraf et al. (2017) expanded this by adding condi-\ntional sampling, which means that they only analyzed pixels that\nare hard to predict by simply investigating neighboring pixels, and\nby adding multivariable analysis, which means that they analyzed\npatches of connected pixels instead of single pixels. They included\nan analysis of brain MRI of patients with HIV versus healthy con-\ntrols, yielding explanation of the classifier’s decision.\n"
        },
        {
            "idx": 15,
            "thing": "text",
            "score": 99.96,
            "box": [
                1276.8,
                1205.5,
                2326.0,
                1509.3
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_15_text.png",
            "text": "Seo et al. (2020) used prediction difference analysis in combi-\nnation with superpixels (or supervoxels for 3D) on multiple scales.\nThese multiscale supervoxel-based saliency maps provided expla-\nnations that the authors described as visually pleasing since they\nfollow image edges. The saliency maps explained which regions\nwere informative for a classifier to distinguish between Alzheimer’s\ndisease patients and normal controls.\n"
        },
        {
            "idx": 16,
            "thing": "title",
            "score": 99.21,
            "box": [
                1278.4,
                1565.5,
                2033.8,
                1607.8
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_16_title.png",
            "text": "3.1.3. Multiple instance learning-based approaches\n"
        },
        {
            "idx": 17,
            "thing": "text",
            "score": 99.97,
            "box": [
                1277.3,
                1608.5,
                2325.8,
                1913.6
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_17_text.png",
            "text": "Multiple instance learning can be used for visualizing explana-\ntions. In multiple instance learning, training sets consist of bags\nof instances (Dietterich et al., 1997). These bags are labeled, but\nthe instances are not. In medical image analysis, multiple instance\nlearning can for example be done using a patch-based approach:\nAn image represents the bag, and patches from that image repre-\nsent the instances (Cheplygina et al., 2019).\n"
        },
        {
            "idx": 18,
            "thing": "text",
            "score": 99.97,
            "box": [
                1277.2,
                1914.0,
                2324.0,
                2436.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_18_text.png",
            "text": "Several researchers have used this approach to pinpoint which\ninstances in the bag are responsible for the classification. For ex-\nample, Schwab et al. (2020) localized critical findings in chest\nX-ray using such a patch-based approach. Each image patch re-\nceived a prediction, and the predictions were overlaid on the im-\nage to visualize on which areas the classifier based its decision.\nAratijo et al. (2020) used multiple instance learning to explain\nwhich areas of a fundus photograph were important for diabetic\nretinopathy. They assessed the severity of the disease using an or-\ndinal scale with grades from 0 to 5. Using a patch-based approach,\nthey provided visual explanation maps for each diabetic retinopa-\nthy grade.\n"
        },
        {
            "idx": 19,
            "thing": "title",
            "score": 98.85,
            "box": [
                1279.5,
                2491.5,
                1637.1,
                2533.6
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_19_title.png",
            "text": "3.2. Textual explanation\n"
        },
        {
            "idx": 20,
            "thing": "text",
            "score": 99.97,
            "box": [
                1277.9,
                2577.9,
                2325.2,
                2839.8
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_20_text.png",
            "text": "Textual explanation is a form of XAI that provides textual de-\nscriptions. Such descriptions include relatively simple characteris-\ntics (e.g. ‘spiculated mass’), up to entire medical reports. We will\ndescribe three types of textual explanation: image captioning, im-\nage captioning with visual explanation, and testing with concept\nattribution.\n"
        },
        {
            "idx": 21,
            "thing": "text",
            "score": 99.86,
            "box": [
                1276.9,
                2840.7,
                2324.6,
                2927.3
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_21_text.png",
            "text": "An overview of papers using textual explanation in medical\nimaging is shown in Table 3.\n"
        },
        {
            "idx": 22,
            "thing": "title",
            "score": 98.92,
            "box": [
                1279.0,
                2982.7,
                1624.8,
                3025.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_22_title.png",
            "text": "3.2.1. Image captioning\n"
        },
        {
            "idx": 23,
            "thing": "text",
            "score": 99.95,
            "box": [
                1275.9,
                3027.7,
                2325.4,
                3112.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis/crops_ordered/page_08/region_23_text.png",
            "text": "Vinyals et al. (2015) provided textual explanation for images us-\ning an end-to-end image captioning framework. They coupled a\n"
        }
    ]
}