{
    "pdf_filename": "Explainable medical imaging AI needs human-centered design a systematic review.pdf",
    "pdf_fullpath": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review.pdf",
    "pages_num": 15,
    "pages": [
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_01.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_01.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 10,
                "page_idx": 1
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "title",
                    "score": 99.46,
                    "box": [
                        151.4,
                        409.2,
                        2291.0,
                        598.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_01_title.png",
                    "text": "Explainable medical imaging AI needs human-centered design:\nguidelines and evidence from a systematic review\n"
                },
                {
                    "idx": 2,
                    "thing": "text",
                    "score": 92.58,
                    "box": [
                        149.4,
                        632.2,
                        1592.2,
                        677.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_02_text.png",
                    "text": "Haomin Chen@!”, Catalina Gomez'?, Chien-Ming Huang@®' and Mathias Unberath @! ™\n"
                },
                {
                    "idx": 3,
                    "thing": "text",
                    "score": 99.24,
                    "box": [
                        197.9,
                        763.6,
                        2245.8,
                        1487.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_03_text.png",
                    "text": "Transparency in Machine Learning (ML), often also referred to as interpretability or explainability, attempts to reveal the working\nmechanisms of complex models. From a human-centered design perspective, transparency is not a property of the ML model but\nan affordance, i.e., a relationship between algorithm and users. Thus, prototyping and user evaluations are critical to attaining\nsolutions that afford transparency. Following human-centered design principles in highly specialized and high stakes domains, such\nas medical image analysis, is challenging due to the limited access to end users and the knowledge imbalance between those users\nand ML designers. To investigate the state of transparent ML in medical image analysis, we conducted a systematic review of the\nliterature from 2012 to 2021 in PubMed, EMBASE, and Compendex databases. We identified 2508 records and 68 articles met the\ninclusion criteria. Current techniques in transparent ML are dominated by computational feasibility and barely consider end users,\ne.g. clinical stakeholders. Despite the different roles and knowledge of ML developers and end users, no study reported formative\nuser research to inform the design and development of transparent ML models. Only a few studies validated transparency claims\nthrough empirical user evaluations. These shortcomings put contemporary research on transparent ML at risk of being\nincomprehensible to users, and thus, clinically irrelevant. To alleviate these shortcomings in forthcoming research, we introduce the\nINTRPRT guideline, a design directive for transparent ML systems in medical image analysis. The INTRPRT guideline suggests human-\ncentered design principles, recommending formative user research as the first step to understand user needs and domain\nrequirements. Following these guidelines increases the likelihood that the algorithms afford transparency and enable stakeholders\nto capitalize on the benefits of transparent ML.\n"
                },
                {
                    "idx": 4,
                    "thing": "text",
                    "score": 7.7,
                    "box": [
                        200.4,
                        1509.9,
                        1416.9,
                        1550.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_04_text.png",
                    "text": "npj Digital Medicine (2022)5:156; https://doi.org/10.1038/s41746-022-00699-2\n"
                },
                {
                    "idx": 5,
                    "thing": "title",
                    "score": 95.04,
                    "box": [
                        150.5,
                        1708.2,
                        411.1,
                        1748.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_05_title.png",
                    "text": "INTRODUCTION\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 99.93,
                    "box": [
                        148.2,
                        1756.6,
                        1184.6,
                        2951.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_06_text.png",
                    "text": "There have been considerable research thrusts to develop\nMachine Learning (ML) models in the healthcare domain that\nassist clinical stakeholders'. However, translating these ML models\nfrom the bench to the bedside to support clinical stakeholders\nduring routine care brings substantial challenges, among other\nreasons, because of the high stakes involved in most decisions\nthat impact human lives. When stakeholders interact with ML\ntools to reach decisions, they may be persuaded to follow ML’s\nrecommendations that may be incorrect or promote unintended\nbiases against vulnerable populations, all of which can have\ndreadful consequences”. These circumstances motivate the need\nfor trustworthy ML systems in healthcare and have sparked efforts\nto specify the different requirements that ML algorithms should\nfulfill. Most of these recent efforts focus on achieving a certain on-\ntask performance requirement but neglect that for assisted\ndecision making not ML system performance alone, but human-\nML team performance is the most pertinent to patient outcome.\nHow to achieve adequate human-machine teaming performance,\nhowever, is debated. While some argue that rigorous algorithmic\nvalidation, e.g., similar to the evaluation of drugs, tests, or devices,\ndemonstrates safe and reliable operation and may thus be\nsufficient for successful human-machine teaming“, others reason\nthat transparency in an ML model, e.g., by revealing its working\nmechanisms and presenting a proper interface, is necessary to\ninvoke user trust and achieve the desired human-machine\nteaming performance®~’. The growing interest and convergence\nof recent works on the importance and need of transparency have\nstressed that not addressing the opacity of ML techniques might\n"
                },
                {
                    "idx": 7,
                    "thing": "text",
                    "score": 25.98,
                    "box": [
                        149.1,
                        3133.7,
                        1100.9,
                        3170.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_07_text.png",
                    "text": "Published in partnership with Seoul National University Bundang Hospital\n"
                },
                {
                    "idx": 8,
                    "thing": "text",
                    "score": 99.94,
                    "box": [
                        1259.2,
                        1707.8,
                        2293.3,
                        2458.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_08_text.png",
                    "text": "hinder their adoption of in healthcare, limiting the potential\npositive impacts?’*'*. The inability to make the decision making\nprocess transparent might affect the misuse and disuse of ML\nmodels in the clinical domain, as the utility of the model might be\nlimited if it does not reveal the reasoning process, limitations, and\nbiases. We believe that this dichotomy is artificial in that, first,\nrigorous validation and transparency are not mutually exclusive,\nand second, both approaches augment an ML model with\nadditional information in hopes to justify (in other words, make\ntransparent) the recommendation’s validity which is hypothesized\nto achieve certain human-factors engineering goals such as\nunderstandability, reliability, trust and etc. However, as we will\nhighlight in detail through a systematic review, current\napproaches that aim at advancing human factors goals of ML\nsystems rely on developers’ intuition rather than considering\nwhether these mechanisms affect users’ experience with the\nsystem and their ability to act on ML model's outputs.\n"
                },
                {
                    "idx": 9,
                    "thing": "text",
                    "score": 99.94,
                    "box": [
                        1259.0,
                        2458.3,
                        2292.7,
                        2942.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_09_text.png",
                    "text": "Designing ML algorithms that are transparent is fundamentally\ndifferent from merely designing ML algorithms. The desire for\ntransparency adds a layer of complexity that is not necessarily\ncomputational. Rather, it involves human factors, namely the users\nto whom the ML algorithm should be transparent. As a\nconsequence, transparency of an algorithm is not a property of\nthe algorithm but a relationship between the transparent ML\nalgorithm and the user processing the information. Such relation-\nship can be understood as an affordance, a concept that is\ncommonly employed when designing effective Human-Computer\nInteractions (HCls)'?, and we argue that transparency in ML\n"
                },
                {
                    "idx": 10,
                    "thing": "text",
                    "score": 96.26,
                    "box": [
                        150.4,
                        3028.4,
                        2294.2,
                        3062.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_01/region_10_text.png",
                    "text": "\"Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA. “These authors contributed equally: Haomin Chen, Catalina Gomez. “email: unberath@jhu.edu\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_02.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_02.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 9,
                "page_idx": 2
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "figure",
                    "score": 99.73,
                    "box": [
                        250.8,
                        248.3,
                        2292.0,
                        1140.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_02/region_1_figure.png"
                },
                {
                    "idx": 2,
                    "thing": "text",
                    "score": 99.89,
                    "box": [
                        189.4,
                        1165.3,
                        2333.1,
                        1353.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_02/region_2_text.png",
                    "text": "Fig. 1 Schematic representation of the INTRPRT guideline within the main stages of a human-centered design process. The blue boxes\ndemonstrate the process from understanding end users and their context to the validation of the developed system, which ultimately might\nresult in large scale deployment. The guidelines are summarized below and are located within the design phases based on the aspects\npertinent to each one and the corresponding themes of each guideline are listed on the left. Opportunities for iterative design are illustrated\nwith the dashed arrows.\n"
                },
                {
                    "idx": 3,
                    "thing": "text",
                    "score": 99.69,
                    "box": [
                        189.6,
                        1399.9,
                        1223.7,
                        1482.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_02/region_3_text.png",
                    "text": "algorithms should be viewed as such. There are several\nconsequences from this definition:\n"
                },
                {
                    "idx": 4,
                    "thing": "list",
                    "score": 99.79,
                    "box": [
                        188.0,
                        1508.2,
                        1223.6,
                        1839.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_02/region_4_list.png",
                    "text": "@ Developing transparent ML algorithms is not purely computa-\ntional.\n\n@ Specific design choices on the mechanisms to achieve\nexplanations or interpretations may be suitable for one user\ngroup, but not for another.\n\n@ Creating transparent ML systems without prior groundwork to\nestablish that it indeed affords transparency may result in\nmisspent effort.\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        188.4,
                        1857.6,
                        1222.8,
                        2396.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_02/region_5_text.png",
                    "text": "Given the user- and context-dependent nature of transparency,\nit is essential to understand the target audience and to validate\ndesign choices through iterative empirical user studies to ensure\nthat design choices of transparent models are grounded in a deep\nunderstanding of the target users and their context. In addition, to\nmaintain a user-centered approach to design from the early\nstages, rapid prototyping with users provides feedback on the\ncurrent, low- to high-fidelity embodiment of the system that is\ngoing to be built eventually. Involving users early by exposing\nthem to low-fidelity prototypes that mimic final system behavior\nallows designers to explore multiple alternatives before commit-\nting to one pre-determined approach that may not be under-\nstandable nor of interest to end users.\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        188.5,
                        2397.2,
                        1222.9,
                        2938.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_02/region_6_text.png",
                    "text": "However, following a human-centered design approach to build\ntransparent ML systems for highly specialized and high stakes\ndomains, such as healthcare, is challenging. The barriers are\ndiverse and include: (1) the high knowledge mismatch between\nML developers and the varied stakeholders in medicine, including\nproviders, administrators, or patients; (2) availability restrictions or\nethical concerns that limit accessibility of potential target users for\niterated empirical tests in simulated setups for formative research\nor validation; (3) challenges inherent to clinical problems,\nincluding the complex nature of medical data (e.g., unstructured\nor high dimensional) and decision making tasks from multiple\ndata sources; and last but not least, (4) the lack of ML designers’\ntraining in design thinking and human factors engineering.\n"
                },
                {
                    "idx": 7,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        189.2,
                        2939.8,
                        1224.4,
                        3063.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_02/region_7_text.png",
                    "text": "Starting from the considerations around designing and validat-\ning transparent ML for healthcare presented above, we investigate\nthe current state of transparent ML in medical image analysis, a\n"
                },
                {
                    "idx": 8,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        1298.0,
                        1401.8,
                        2332.3,
                        1857.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_02/region_8_text.png",
                    "text": "trailblazing application area for ML in healthcare due to the\nabundance and structure of data. Through a systematic review\nbased on these aspects, we first identify major shortcomings in\nthe design and validation processes of developing transparent ML\nmodels. These deficiencies include the absence of formative user\nresearch, the lack of empirical user studies, and in general, the\nomission of considering ML transparency as contingent on the\ntargeted users and contexts. Together, these shortcomings of\ncontemporary practices in transparent ML development put the\nresulting solutions at substantial risk of being unintelligible to the\ntarget users, and consequently, irrelevant.\n"
                },
                {
                    "idx": 9,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1297.6,
                        1859.8,
                        2333.8,
                        3064.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_02/region_9_text.png",
                    "text": "This paper aims to encourage model designers to actively\nconsider and work closely with the end users during the design,\nconstruction, and validation of ML models for medical imaging\nproblems. Acknowledging the barriers to widespread adoption of\nhuman-centered design techniques to develop transparent ML in\nhealthcare and grounded in our systematic review of the\nliterature, we further propose the INTRPRT guideline to help model\ndesigners for developing transparent ML for medical image\nanalysis step by step. Figure 1 summarizes our guideline within\na human-centered design process. The guideline aims at high-\nlighting the need to ground and justify design choices in a solid\nunderstanding of the users and their context when adding\ntransparency or other human factors-based goals to ML systems\nfor medical image analysis. By raising awareness of the user- and\ncontext-dependent nature of transparency, designers should\nconsider a trade-off between efforts to (1) better ground their\napproaches on user needs and domain requirements and (2)\ncommit to technological development and validation of possibly\ntransparent systems. In this way, the guideline may increase the\nlikelihood for algorithms that advance to the technological\ndevelopment stage to afford transparency, because they are well\ngrounded and justified in user and context understanding. This\nmay mitigate misspent efforts in developing complex systems\nwithout prior formative user research, and help designers make\naccurate claims about transparency and other human factors\nengineering goals when building and validating the model. To the\nbest of our knowledge, we provide the first guidelines for models\nthat afford transparency and involve end users in the design\nprocess for medical image analysis.\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_03.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_03.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 8,
                "page_idx": 3
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "figure",
                    "score": 99.75,
                    "box": [
                        468.5,
                        247.2,
                        1969.6,
                        986.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_03/region_1_figure.png"
                },
                {
                    "idx": 2,
                    "thing": "text",
                    "score": 99.81,
                    "box": [
                        150.4,
                        1015.2,
                        2293.6,
                        1167.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_03/region_2_text.png",
                    "text": "Fig. 2 Illustrative examples of different techniques used in transparent ML systems for classification and segmentation tasks from the\nsystematic review. Within each task, a non-interpretable model generates the task outcome from the input image (top). The use of clinical\nknowledge or computer vision information as priors attempts to add transparency in the outcome generation process (bottom). Images\nretrieved from the ORIGA'2” and BraTS2020 datasets'7°.\n"
                },
                {
                    "idx": 3,
                    "thing": "title",
                    "score": 99.72,
                    "box": [
                        151.6,
                        1264.7,
                        1069.9,
                        1348.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_03/region_3_title.png",
                    "text": "AN OVERVIEW OF CURRENT TRENDS IN TRANSPARENT\nMACHINE LEARNING DEVELOPMENT\n"
                },
                {
                    "idx": 4,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        149.1,
                        1357.4,
                        1184.2,
                        2771.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_03/region_4_text.png",
                    "text": "Compared to developing generic ML algorithms, designing and\nvalidating transparent ML algorithms in medical imaging tasks\nrequires consideration of human factors and clinical context. We\ngroup these additional considerations into six themes according\nto the initial review, iteratively defined prior to data extraction and\nabbreviated to I/NTRPRT; the themes are incorporation (IN),\ninterpretability (IN), target (T), reporting (R), prior (PR), and task\n(T). Incorporation refers to the communication and cooperation\nbetween designers and end users before and during the\nconstruction of the transparent model. Formative user research\nis one possible strategy that can help designers to understand end\nusers’ needs and background knowledge'*'®, but other\napproaches exist'®. interpretability considers the technicalities of\nalgorithmic realization of a transparent ML system. Figure 2\nprovides illustrative examples of some of these techniques. Target\ndetermines the end users of the transparent ML algorithms.\nReporting summarizes all aspects pertaining to the validation of\ntransparent algorithms. This includes task performance evaluation\nas well as the assessment of technical correctness and human\nfactors of the proposed transparency technique (e.g., intelligibility\nof the model output, trust, or reliability). Prior refers to previously\npublished, otherwise public, or empirically established sources of\ninformation about target users and their context. This prior\nevidence can be used to conceptualize and justify design choices\naround achieving transparency. Finally, task specifies the con-\nsidered medical image analysis task, such as prediction, segmen-\ntation, or super resolution, and thus determines the clinical\nrequirements on performance. We emphasize that these themes\nshould not be considered in isolation because they interact with\nand are relevant to each other. For example, technical feasibility of\ninnovative transparency mechanisms based on the desired task\nmay influence both, the priors that will be considered during\ndevelopment as well as the incorporation of target users to\nidentify and validate alternatives.\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        150.6,
                        2773.5,
                        1183.8,
                        3063.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_03/region_5_text.png",
                    "text": "Having identified and refined the themes iteratively after an\ninitial review, we structured the systematic review according to\nthe six themes. We identify and summarize dominant trends\namong the 68 included studies aiming to design transparent ML\nfor medical image analysis. In the incorporation theme, cross-\ndisciplinary study teams may constitute a first step towards\nincorporating target users during ML design, however, only 33 of\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        1259.0,
                        1268.5,
                        2293.3,
                        2511.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_03/region_6_text.png",
                    "text": "the included articles were authored by multidisciplinary clinician-\nengineering teams. More importantly, no paper introduced\nformative user research to understand user needs and contextual\nconsiderations before model construction, which is reflected in\nthe lack of justifying the prior theme. Around half of the selected\narticles (n=28) chose clinical priors and guidelines as an\ninspiration for transparent systems. In the target theme, we found\nthat only 30 of the included articles specified end users, and all of\nthese papers were aimed at clinical care providers, a stark\nimbalance considering the variety of stakeholders. In the task\ntheme, prediction tasks were by far the most common application\nfor transparent ML algorithm design (57/68). In the interpretability\ntheme, methods relying on clinical guidelines resulted in\nalgorithms that adopted multiple sub-steps of a clinical guideline\nto build the model and generate outcomes, while methods that\nwere based on computer vision techniques for transparency most\ncommonly relied on post-hoc explanations. In the Reporting\ntheme, the methods used for assessing transparency varied with\nthe problem formulation and transparency design, and included\nhuman perception, qualitative visualizations, quantitative metrics,\nand empirical user studies; we note that an evaluation with end\nusers was highly uncommon (only 3 of the 68 included studies).\nHowever, no paper considered the six themes comprehensively.\nMore importantly, there is no evidence that any papers considered\nthe dependency and interaction between different themes. The\nreviewed literature further supports that one guideline consider-\ning all themes and the interaction between them is highly desired\nin the medical image analysis community to construct transparent\nML models following human-centered design practices.\n"
                },
                {
                    "idx": 7,
                    "thing": "title",
                    "score": 97.97,
                    "box": [
                        1258.6,
                        2583.9,
                        1594.1,
                        2626.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_03/region_7_title.png",
                    "text": "INTRPRT GUIDELINE\n"
                },
                {
                    "idx": 8,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1257.9,
                        2635.5,
                        2292.2,
                        3063.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_03/region_8_text.png",
                    "text": "We distilled a set of guidelines for designing transparent ML\nmodels according to the interaction and relevancy among the six\nthemes, which is proposed here as INTRPRT guideline. The INTRPRT\nguideline provides suggestions for designing and _ validating\ntransparent ML systems in healthcare in hopes to increase the\nlikelihood that the resulting algorithms indeed afford transpar-\nency for the designated end users. The guidelines also address the\nchallenges of following a human-centered design approach in the\nhealthcare domain, propose potential solutions, and apply to\ndifferent kinds of transparency ML algorithms. To further illustrate\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_04.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_04.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 15,
                "page_idx": 4
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "figure",
                    "score": 99.19,
                    "box": [
                        584.3,
                        249.1,
                        1933.4,
                        697.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_01_figure.png"
                },
                {
                    "idx": 2,
                    "thing": "text",
                    "score": 99.88,
                    "box": [
                        189.6,
                        724.7,
                        2331.9,
                        873.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_02_text.png",
                    "text": "Fig. 3 Relationships of components in ML systems. a Shows the relationship between the clinical scenario and requirements in non-\ntransparent ML systems, where the system may not be human-facing, and as such, meeting quantitative performance requirements is\nsufficient. b Shows the relationship between the clinical scenario, requirements, and end users in transparent ML systems, as they arise in a\nhuman-centered system that seeks to enable users to accomplish a certain task.\n"
                },
                {
                    "idx": 3,
                    "thing": "text",
                    "score": 99.74,
                    "box": [
                        189.5,
                        917.7,
                        1223.0,
                        997.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_03_text.png",
                    "text": "the INTRPRT guideline, we introduce a case study (see Supple-\nmentary information A).\n"
                },
                {
                    "idx": 4,
                    "thing": "title",
                    "score": 99.66,
                    "box": [
                        190.5,
                        1054.8,
                        1079.5,
                        1135.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_04_title.png",
                    "text": "Guideline 1: specify the clinical scenario, constraints,\nrequirements, and end users\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        188.8,
                        1146.7,
                        1222.9,
                        1849.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_05_text.png",
                    "text": "The first step to designing any ML algorithm for healthcare is to\nwell define the clinical scenario, the constraints the solution will\nhave to abide by, and all hard or soft requirements the algorithm\nneeds to meet for the clinical task to be addressed adequately (cf.\nFig. 3). For ML algorithms that do not attempt to be transparent, it\nis essential but sufficient to assess whether the envisioned ML\nalgorithm design will satisfy the clinical constraints and require-\nments, e.g., an acceptable classification accuracy in allowable\nprocessing time. In addition, when designing transparent ML\nalgorithms it is equally critical to determine and characterize the\nend users. It is of particular importance to investigate end user\ncharacteristics specifically in the clinical context of the chosen\ntask. This is because, depending on the task, stakeholders have\nvaried interest, prior knowledge, responsibilities, and require-\nments'’. Deep understanding of the role target users play in the\nchosen clinical task and their unique needs is critical in\ndetermining how to achieve transparency (Guideline 2).\n"
                },
                {
                    "idx": 6,
                    "thing": "title",
                    "score": 99.67,
                    "box": [
                        190.0,
                        1906.6,
                        1222.6,
                        1986.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_06_title.png",
                    "text": "Guideline 2: justify the choice of transparency and determine\nthe level of evidence\n"
                },
                {
                    "idx": 7,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        188.8,
                        1998.7,
                        1223.0,
                        2783.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_07_text.png",
                    "text": "There exists a wide gap in domain expertise and contextualization\nbetween target users and ML model designers in most use cases\nin healthcare. Furthermore, there are multiple “transparency”\ntechniques and choices, such as the transparent working\nmechanism or user-centered interactions in practice. Simply\nselecting a “transparency” technique, without incorporating and\nconsulting target users puts the resulting ML models at risk of not\nachieving the desired transparency. The human-centered design\napproach addresses this challenge through iterative empirical\nstudies that over time guide the development and refinement of\nthe technical approach such that, upon completion, the design\nchoices are well justified by empirical target user feedback. This\napproach may not always be feasible in healthcare due to\naccessibility and availability barriers of target users. To address this\nlimitation while still enabling technological progress in transpar-\nent ML, we introduce four distinct levels of evidence. These levels\nallow designers to classify the level of confidence one may have\nthat the specific design choices will indeed result in a model that\naffords transparency.\n"
                },
                {
                    "idx": 8,
                    "thing": "text",
                    "score": 99.88,
                    "box": [
                        189.8,
                        2786.7,
                        1222.5,
                        2905.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_08_text.png",
                    "text": "The levels of evidence are based on increasingly thorough\napproaches to understand the chosen end users in context of the\nenvisioned task:\n"
                },
                {
                    "idx": 9,
                    "thing": "list",
                    "score": 99.58,
                    "box": [
                        189.6,
                        2935.3,
                        1224.4,
                        3056.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_09_list.png",
                    "text": "@ Level 0: No evidence. No dedicated investigations about the\nend users are performed to develop transparent ML systems.\n@ Level 1: One-way evidence. Formative user research\n"
                },
                {
                    "idx": 10,
                    "thing": "list",
                    "score": 99.76,
                    "box": [
                        1298.6,
                        921.0,
                        2332.9,
                        1910.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_10_list.png",
                    "text": "techniques, such as surveys and diary studies, are only\nperformed once without further feedback from end users\nabout the findings extracted from the research phase,\nresulting in one-way evidence. Such user research suffers\nrisks of potential bias in concluding about justification of\ntransparency because there is no opportunity for dialog, i.e.,\ndesigners may ask irrelevant questions or target users may\nprovide non-insightful, potentially biased responses.\n\n@ Level 2: Public evidence. Public evidence refers to information\nabout target user knowledge, preference, or behavior that is\npublic domain and vetted in a sensible way. Public evidence\nincludes clinical best practice guidelines, Delphi consensus\nreports, peer-reviewed empirical studies of closely related\napproaches in large cohorts, or well documented socio-\nbehavioral phenomena.\n\n@ Level 3: Iteratively developed evidence. Iteratively developed\nevidence is transparency evidence that is iteratively refined\nthrough user feedback where designers and end _ users\ncommunicate with each other throughout method develop-\nment. The purpose of iteratively validating and refining the\ncurrent transparency mechanism is to identify any potential\nbias in the assumptions that motivate the transparency\ntechnique while ensuring that it is understandable to\nend users.\n"
                },
                {
                    "idx": 11,
                    "thing": "text",
                    "score": 99.94,
                    "box": [
                        1297.2,
                        1929.5,
                        2331.8,
                        2095.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_11_text.png",
                    "text": "Being actively cognizant of the level of evidence that supports\nthe development enables trading off development efforts\nbetween ML method development vs. gathering richer evidence\nin support of the intended developments.\n"
                },
                {
                    "idx": 12,
                    "thing": "title",
                    "score": 99.78,
                    "box": [
                        1298.5,
                        2153.2,
                        2329.8,
                        2233.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_12_title.png",
                    "text": "Guideline 3: clarify how the model follows the justification of\ntransparency\n"
                },
                {
                    "idx": 13,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1297.5,
                        2243.9,
                        2333.3,
                        2877.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_13_text.png",
                    "text": "This guideline is designed to ensure that the transparency\ntechnique used in the ML model is indeed consistent with the\nassumptions made during its justification. While complying with\nthis guideline is trivial if the model is developed in a human-\ncentered design approach (Level 3: Iteratively developed evi-\ndence), in all other cases designers should be explicit about the\nintellectual proximity of the developed technical approach to the\nmotivating evidence. To this end, after specifying which\ncomponents of the ML model require transparency for users to\ncapitalize on the intended benefits, it is desirable for the method\nto be as simple as possible so that it can be easily derived from\nand linked to the justification of transparency. Once confirmed\nthat the envisioned model is indeed consistent with the\njustification, computational development of the model, including\ntraining, refinement, and validation, begins.\n"
                },
                {
                    "idx": 14,
                    "thing": "title",
                    "score": 99.3,
                    "box": [
                        1298.1,
                        2932.9,
                        2313.6,
                        2970.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_14_title.png",
                    "text": "Guideline 4: determine how to communicate with end users\n"
                },
                {
                    "idx": 15,
                    "thing": "text",
                    "score": 99.95,
                    "box": [
                        1298.5,
                        2982.6,
                        2330.1,
                        3063.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_04/region_15_text.png",
                    "text": "In addition to content (Guidelines 1, 2, and 3), seemingly\nperipheral factors on the presentation of information may play a\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_05.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_05.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 13,
                "page_idx": 5
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        149.1,
                        241.8,
                        1184.6,
                        780.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_01_text.png",
                    "text": "disproportionate role in the perception of transparency. It is well\nknown that factors like format (e.g., text, images, plots)'®, channel\n(e.g. graphical interface)'?, and interactivity (e.g. whether users\ncan provide feedback or refine model outputs) can drastically\naffect users’ experience and performance????, and therefore,\nmust be aligned with the goal of transparent system develop-\nment. Clearly, the selection of presentation mode should be\nincorporated early and supported by some degree of evidence,\nthat emerges naturally when following human-centered design\nprinciples but requires justification if not (as posited for\ntransparency in Guideline 2). Ultimately, users’ experience with\nthe systems plays an important role in their willingness to adopt it\nin a real setup?!2?,\n"
                },
                {
                    "idx": 2,
                    "thing": "title",
                    "score": 98.89,
                    "box": [
                        149.6,
                        835.3,
                        1099.1,
                        876.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_02_title.png",
                    "text": "Guideline 5: report task performance of the ML systems\n"
                },
                {
                    "idx": 3,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        148.9,
                        885.8,
                        1184.2,
                        1464.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_03_text.png",
                    "text": "Similar to ordinary ML models, the transparent system must be\nevaluated quantitatively using appropriate metrics that well reflect\nthe desired performance. In addition, the data used to evaluate\nthe algorithm and its relevance regarding the clinical target task\nmust be specified. Metrics and evaluation protocols should be\nselected to well determine the model's abilities in regard to the\nclinical requirements specified per Guideline 1. Reporting task\nperformance of the algorithm in standalone deployment is\nimportant as a baseline for empirical studies in which users may\ninteract and collaborate with the system to complete a task, and\nteam performance (human + ML system) metrics can be\nmeasured. Such comparisons are relevant to the goal of improving\nteam performance when integrating intelligent systems to assist\nhumans in complex tasks??**,\n"
                },
                {
                    "idx": 4,
                    "thing": "title",
                    "score": 99.66,
                    "box": [
                        150.9,
                        1520.3,
                        1176.9,
                        1603.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_04_title.png",
                    "text": "Guideline 6: assess correctness and human factors of system\ntransparency\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        149.1,
                        1612.8,
                        1184.0,
                        2605.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_05_text.png",
                    "text": "In addition to task performance, transparent ML systems must be\nevaluated with respect to their transparency claims. This validation\nis generally necessary even for Level 3 (iteratively developed\nevidence) justified transparency mechanisms, because user\nstudies in the design phase commonly rely on mock-up\nprototypes of the envisioned system, and therefore, may have\ndifferent modes of failure than the final ML system. Within this\nguideline, we distinguish two types of evaluation: (1) Validating\nthe correctness of the transparency technique, which objectively\nassesses whether the information supplied to achieve transpar-\nency is in agreement with the justification. Achieving correctness\nis particularly important for systems that rely on post-hoc\nexplanations, since explanations may rely on a second model\nthat is distinct from the ML algorithm generating recommenda-\ntions. (2) Validating the effectiveness of transparency in a human-\ncentered approach, to demonstrate that the transparent ML\nsystem applied to relevant data samples and in cooperation with\ntarget users achieves the desired goals. This empirical evaluation\ndetermines the efficacy of human factors engineering. The\ndimensions that are often considered include users’ trust'®,\nreliance, satisfaction?*, mental model formation*?, and system\nacceptance. Reporting of these user studies should include details\nof the experimental design, participant sample, and techniques to\nanalyze the results.\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        150.4,
                        2608.2,
                        1183.9,
                        3063.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_06_text.png",
                    "text": "Transparency of ML algorithms for medical image analysis is\ncommonly motivated by the desire of automating complex tasks\nwhile retaining a clear interface for human interaction, e.g., to\nimprove trust, avoid over-reliance, or increase acceptance.\nHowever, achieving these design goals through transparency\nrequires the development of transparent ML algorithms that are\nintelligible by the envisioned end users. In design thinking,\naligning technological developments with user needs is accom-\nplished through user involvement in the design process and\niterative user testing, which is largely infeasible in healthcare due\nto varied barriers to end user involvement. We propose a design\n"
                },
                {
                    "idx": 7,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1258.9,
                        241.8,
                        2292.7,
                        863.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_07_text.png",
                    "text": "and evaluation framework where ML designers actively consider\nend users’ needs, knowledge, and requirements, allowing\ndesigners to classify the reliability of their understanding of end\nuser needs using four levels of evidence. Explicitly thinking about\nthe confidence one may have in the assumptions about end users\nthat drive transparent ML system development may mitigate the\nrisks of developing solutions that are unintelligible to the target\nusers, and therefore, neither achieve the desired human factors\nengineering goals nor benefit clinical practice. Similarly, quantify-\ning the level of evidence currently available to motivate\ntransparency claims then allows developers to trade-off resources\nbetween technical ML developments and additional formative\nresearch of their target users to ensure that the resulting systems\nare fit to meet the requirements of the clinical task but foremost\nthe target users.\n"
                },
                {
                    "idx": 8,
                    "thing": "title",
                    "score": 98.55,
                    "box": [
                        1258.2,
                        937.7,
                        1710.1,
                        979.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_08_title.png",
                    "text": "RESULTS AND DISCUSSION\n"
                },
                {
                    "idx": 9,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1257.9,
                        989.5,
                        2292.2,
                        1361.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_09_text.png",
                    "text": "We discuss the INTRPRT guideline in the context of the key\nobservations from different themes of our systematic review to\nidentify opportunities to improve the design of transparent ML\nsystems for medical image analysis. Each discussion point focuses\non one or more themes that we introduced before. Furthermore,\nthe second last subsection presents successful examples of ML\nsystems designed with clinical end users and the last subsection\nalso includes the comparison of our guideline and systematic\nreview with existing literature.\n"
                },
                {
                    "idx": 10,
                    "thing": "title",
                    "score": 99.71,
                    "box": [
                        1258.3,
                        1416.9,
                        2232.6,
                        1499.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_10_title.png",
                    "text": "Importance of formative user research and empirical user\ntesting\n"
                },
                {
                    "idx": 11,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1259.1,
                        1508.1,
                        2292.9,
                        2793.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_11_text.png",
                    "text": "Both formative user research (theme incorporation and prior) and\nempirical user testing (theme reporting) are critical to ensure that\nsolutions meet user needs (theme target). On the one hand,\nformative user research helps designers navigate and understand\nend users’ domain practice and needs. On the other hand,\nempirical user testing assesses whether the designed algorithm\nindeed achieves the human factors engineering goals, such as\naffording transparency, promoting trust, or avoiding over-reliance.\nAdditionally, early user involvement in the design process using\nprototypes of increasing fidelity provides opportunities to review\nand iterate over design choices. From our systematic review of the\nliterature presented in the “Detailed Analysis of Findings during\nSystematic Review” section, we find that although most con-\ntemporary studies on transparent ML formulate human factor\nengineering goals, no study reported formative user research or\nempirical testing to inform and validate design choices. We must\nconclude that contemporary research efforts in medical image\nanalysis have disproportionately prioritized the technological\ndevelopment of algorithmic solutions that alter or augment the\npredictions of complex ML systems with the implicit—though\nunfortunately often explicit—assumption that those changes\nwould achieve transparency. However, because of the substantial\nknowledge imbalance between ML engineers and target users\namong other reasons detailed above, it is unlikely that, without\nformative user research or empirical tests, those systems truly\nafford transparency or achieve the promised human factors\nengineering goals. While demonstrating the computational\nfeasibility of advanced transparency techniques is certainly of\ninterest, grounding the need for these techniques in solid\nunderstanding of the target users should be the first step for\nmost, if not all, such developments.\n"
                },
                {
                    "idx": 12,
                    "thing": "title",
                    "score": 99.11,
                    "box": [
                        1258.1,
                        2848.6,
                        1865.4,
                        2889.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_12_title.png",
                    "text": "General assessment of transparency\n"
                },
                {
                    "idx": 13,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1258.4,
                        2898.5,
                        2292.0,
                        3063.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_05/region_13_text.png",
                    "text": "In addition to user involvement or formative research during the\ndesign phase (theme incorporation and prior), upon completion of\nML development, system transparency needs to be empirically\nvalidated (theme reporting). During the literature review, we\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_06.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_06.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 10,
                "page_idx": 6
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        189.2,
                        242.3,
                        1223.9,
                        697.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_01_text.png",
                    "text": "observed that hardly any study reported quantitative empirical\nuser evaluations as part of final method validation, and many of\nthe included articles limited analysis of transparency goals to\nqualitative analysis by presenting a limited number of illustrative\nexamples, e.g., pixel-attribution visualizations. While such analysis\nmay suggest fidelity of the transparency design to the cause of the\nprediction in those few select samples, its utility beyond is unclear.\nIn cases where no empirical user evaluation is conducted, neither\nduring conceptualization nor during development, claims around\nsystem transparency or human factors are at high risk of being\noptimistic and should be avoided.\n"
                },
                {
                    "idx": 2,
                    "thing": "title",
                    "score": 99.73,
                    "box": [
                        189.3,
                        752.5,
                        1034.6,
                        835.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_02_title.png",
                    "text": "Transparent machine learning systems for diverse\nstakeholders\n"
                },
                {
                    "idx": 3,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        188.6,
                        845.9,
                        1222.9,
                        1674.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_03_text.png",
                    "text": "The purpose of adding transparency to an ML model varies across\nend users and their context, which we covered in the target\ntheme. The current literature on transparent ML for medical image\nanalysis focuses heavily on care providers. In fact, all of the\nincluded articles that explicitly specify end users targeted\nclinicians, such as radiologists, pathologists, and physicians.\nHowever, clear opportunities for transparent ML systems exist\nfor other clinical stakeholders, such as other care team members\nincluding nurses or techs, healthcare administrators, insurance\nproviders, or patients. Designing transparent ML systems for these\nstakeholder groups will likely require different approaches, both\ntechnological as well in regards to human factors engineering,\nbecause these target users are likely to exhibit distinct needs,\nrequirements, prior knowledge, and expectations. In light of\nrecent articles that question the utility of transparency in high\nstakes clinical decision making tasks?°, driving transparent ML\ndevelopment using a “human factors first’ mindset while\nexpanding target user considerations to more diverse stakeholder\ngroups may increase the likelihood of transparent ML having an\nimpact on some aspects of the healthcare system.\n"
                },
                {
                    "idx": 4,
                    "thing": "title",
                    "score": 99.36,
                    "box": [
                        188.9,
                        1727.4,
                        1157.4,
                        1768.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_04_title.png",
                    "text": "Transparency for tasks with and without human baseline\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        188.4,
                        1777.5,
                        1222.7,
                        3064.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_05_text.png",
                    "text": "Clearly specifying and formulating the medical task that the ML\nsystem solves is fundamental to determine the assistance that it\ncan provide to clinical practice. Along with the disproportionate\nconsideration of clinicians as end users goes a disproportionate\nfocus on clinical tasks that are routinely performed in current\nclinical practice (n = 60/68) by those target users. One motivation\nfor investigating transparency in such tasks is the existence of\nclear and systematic clinical workflows and guidelines, e.g., the\nBreast Imaging Reporting and Data System (BI-RADS) system for\nmammography, the AO/OTA Tile grading of pelvic fractures, or\nother easily intelligible covariates associated with outcomes. The\navailability of such human-defined baselines that are already used\nfor clinical decision making provides immediate Level 2 evidence\nof transparency for ML systems attempting their replication. In\naddition, it facilitates data collection and annotation, because\nintermediate outputs that may be required to build such system\nare known a priori. Conversely, justifying specific attempts at\nachieving transparency is much more complicated for tasks that\ndo not readily have human-based baselines or clinical best\npractice guidelines. Some such tasks may already be performed in\nclinical practice, such as segmentation or super-resolution, the\ninterpretation of which may be ambiguous and result in high\nvariability among observers’. Other tasks may be beyond the\ncurrent human understanding of the underlying mechanisms that\nenable ML-based prediction, e.g., ethnicity prediction from chest\nX-ray”® or various tasks in digital pathology?**° In these scenarios,\nwhile it may be possible to derive some justification from the\nliterature, e.g., how target users generally approach tasks of the\nkind, achieving even Level 2 justification is difficult if not\nimpossible. Empirically validating the envisioned mechanisms for\ntransparency with respect to their ability to afford transparency\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 99.86,
                    "box": [
                        1297.3,
                        242.4,
                        2332.0,
                        323.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_06_text.png",
                    "text": "and achieve the human factors engineering goals is thus\nparamount when attempting to benefit such tasks.\n"
                },
                {
                    "idx": 7,
                    "thing": "title",
                    "score": 99.73,
                    "box": [
                        1297.0,
                        378.0,
                        2299.4,
                        461.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_07_title.png",
                    "text": "Successful examples of machine learning systems designed\nwith clinical end users\n"
                },
                {
                    "idx": 8,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1297.7,
                        470.9,
                        2333.2,
                        1840.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_08_text.png",
                    "text": "Early identification and direct communication with end users, as it\nis emphasized in the target and incorporation themes, allows ML\ndesigners to bridge the knowledge gap and design for users in\nhighly specialized contexts. By following human-centered design\nand HCI practices, previous works have illustrated ways to\nincorporate end users in the design process of ML systems for\nclinical applications. For instance, target users were consulted in\nthe design of an ML tool in an image retrieval system for medical\ndecision making'*, enabling the team to design a system that\npreserves human agency to guide the search process. Through an\niterative design process, functional prototypes of different\nrefinement techniques based on documented user needs were\nimplemented and further validated in a user study. To enable\nusers to explore and understand an Artificial Intelligence (Al)\nenabled analysis tool for Chest X-ray (CXR) images, a user-centered\niterative design assessed the utility of potential explanatory\ninformation in the Al system!'®. Users’ needs during their\nintroduction to an Al-based assistance system for digital\npathology were identified through open-ended interviews and a\nqualitative laboratory study?'. Iterative co-design processes were\nfollowed to identify clinicians’ perceptions of ML tools for real\nclinical workflows, e.g., antidepressant treatment decisions'® and\nphenotype diagnosis in the intensive care unit”. Determining the\nefficacy of envisioned ML systems or ML-enabled interaction\nparadigms in empirical user studies before committing resources\nto their fully-fledged implementation has become common\npractice in human-centered Al, e.g.,7'7234, with many studies\nconsidering tasks that are related to medical image analysis'>*>.\nIncreasing the acceptance of empirical formative user research as\nan integral component of human-centered ML design for\nhealthcare tasks, including medical image analysis, will be critical\nin ensuring that the assumptions on which human-centered\nsystems are built hold in the real world.\n"
                },
                {
                    "idx": 9,
                    "thing": "title",
                    "score": 99.73,
                    "box": [
                        1297.9,
                        1893.0,
                        2315.0,
                        1977.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_09_title.png",
                    "text": "Increasing demand for guidelines to build machine learning\nsystems\n"
                },
                {
                    "idx": 10,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1297.8,
                        1985.3,
                        2333.1,
                        3063.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_06/region_10_text.png",
                    "text": "Motivated by advances in Al technologies and the wide range of\napplications in which it can be used to assist humans, there are\nongoing efforts to guide the design and evaluation of Al-infused\nsystems that people can interact with (theme target). Generally\napplicable design guidelines were compiled and iteratively refined\nby HCI experts to design and evaluate human-Al interactions*°.\nAlthough these guidelines are relevant and suitable for a wide\nrange of common Al-enabled systems, more nuanced guidelines\nare desirable for domains where study participants cannot be\nrecruited nor interviewed in abundance. Similarly, previous\nattempts to guide the design of effective transparency mechan-\nisms acknowledge that real stakeholders involved should be\nconsidered and understood'”373”. Starting from the identification\nof diverse design goals according to users’ needs and their level of\nexpertise on Al technology, and a categorization of evaluation\nmeasures for Explainable Artificial Intelligence (XAI) systems*®,\naddressed the multidisciplinary efforts needed to build such\nsystems. A set of guidelines, summarized in a unified framework,\nsuggests iterative design and evaluation loops to account for both\nalgorithmic and human factors of XAI systems. However, similar to\nref. 3°, these guidelines are intended for generic applications, e.g.,\nloan and insurance rate prediction’? and personalized advertise-\nments“°, and do not consider additional challenges, barriers, and\nlimitations when developing algorithms for domains that exhibit\nusers with very specific needs and in highly specific contexts, such\nas healthcare. Other considerations to build interpretable Al\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_07.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_07.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 6,
                "page_idx": 7
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        149.2,
                        240.6,
                        1184.7,
                        905.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_07/region_1_text.png",
                    "text": "systems have been identified from a multidisciplinary perspec-\ntive'?. For instance, the approach presented in ref. #1 summarized\nfour guidelines that included the application domain, technical\nimplementation, and human-centered requirements in terms of\nthe capabilities of human understanding. A requirements list\nformulated as a “fact sheet” was introduced in ref. *? to\ncharacterize and assess explainable systems along five key\ndimensions: functional, operational, usability, safety and valida-\ntion. While the five dimensions allow to systematically compare\nand contrast explainability approaches theoretically and practi-\ncally, the properties that were included failed to consider where\nand how to formulate the justification of transparency. Formative\nuser research and validation of the justification of transparency are\nespecially essential in healthcare, where a huge knowledge\nimbalance exists between ML designers and end users of Al\nsystems.\n"
                },
                {
                    "idx": 2,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        148.2,
                        906.0,
                        1184.6,
                        2357.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_07/region_2_text.png",
                    "text": "Considering potential uses of Al in clinical setups, there have\nbeen efforts to define guidelines for the development and\nreporting of medical ML systems. For instance, guidelines for\nclinical trials that involve Al were proposed in ref. *?, including\nitems such as the description of intended users, how the Al\nintervention was integrated, how the Al outputs contributed to\ndecision-making, among others. While specifying these items is\nalso relevant for creating transparent systems, these guidelines do\nnot include requirements in dimensions unique to the transpar-\nency of an algorithm, such as its justification and validation.\nGuidelines for the initial clinical use of Al systems were formulated\nin ref. “+, highlighting the importance to assess the actual impact\nof an algorithm on its users’ decisions at an early stage. This\nrecommendation of an early and formative evaluation is aligned\nwith our guideline with respect to formative user research during\nthe initial stages to support design choices for transparency.\nConcerned with the reproducibility and reliability of medical ML\nstudies, a set of practical guidelines as a checklist or questions has\nbeen collected for authors and reviews to assess the methodo-\nlogical soundness of contributions*®, to promote standard\nreporting practices*®, and for clinicians to assess algorithm\nreadiness for routine care*’. Besides the general reporting items\nregarding the problem definition, data, model, and validation,\nthese checklists consider the definition of the target user and the\navailability of interpretability information and support for related\nclaims; however, these are questions to be solved once the\ntransparency technique has been incorporated and might lack an\nappropriate justification and not achieve the intended goals. By\nconsidering the reason to demand explainability in advance,\nwhich is determined by the application domain and target users\nof the Al system, model designers can determine the importance\nand usefulness of the properties offered by certain explainability\ntechniques. To choose among available explainability techniques,\na framework with recommendations regarding mostly technical\naspects for researchers was proposed in ref. ®.\n"
                },
                {
                    "idx": 3,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        148.9,
                        2359.8,
                        1184.3,
                        3062.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_07/region_3_text.png",
                    "text": "With the trend that ML is more popular in clinical decision\nmaking tasks due to its performance, recent surveys and\nsystematic reviews have aimed to summarize existing literature\nto create transparent ML in healthcare. However, these surveys\nfailed to consider all the themes proposed in this paper and each\naspect of transparent ML is reviewed in_ isolation. More\nimportantly, current reviews mainly focus on the existing\ntransparency techniques and evaluation, ignoring how and where\njustification of transparency emerges. For example, a survey\ncategorized research works related to the interpretability of ML in\ngeneral, and then applied the same categories to interpretability\nin the medical field*®. In addition to providing an overall\nperspective of the different interpretable algorithms that are\navailable in the medical field, the survey identified the recurring\nassumption of having interpretable models without human\nsubject tests, questioning the utility within medical practices\nand whether ML designs consider actual medical needs. More\n"
                },
                {
                    "idx": 4,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1258.9,
                        241.3,
                        2293.0,
                        1444.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_07/region_4_text.png",
                    "text": "specifically, there have been surveys focused uniquely on\ntransparent techniques for medical imaging. The interpretability\nmethods to explain deep learning models were categorized in\ndetail based on technical similarities, along with the progress\nmade on the corresponding evaluation approaches in ref. °.\nAnother overview of deep learning-based XAI in medical image\nanalysis is presented in ref. *°, considering a variety of techniques\nthat were adapted or developed to generate visual, textual, and\nexample-based explanations in the medical domain. Some of the\nobserved trends and remarks in this survey match our perspective\nand recommendations in the design of transparent methods for\nmedical imaging, including the lack of evaluation as a standard\npractice, the user-dependent nature of explanations, and the\nimportance of active collaboration with experts to include domain\ninformation. Instead of proposing a general perspective in a broad\nrange of healthcare problems, some reviews focus on specific\ntopics of medical image analysis. Transparent ML for human\nexperts in cancer diagnosis with Al is reviewed in ref. '° with a\nfocus on 2 aspects: ML model characteristics that are important in\ncancer prediction and treatment; and the application of ML in\ncancer cases. These two aspects are similar to our proposed theme\n“Interpretability” and “task”, but we summarize the two themes in\nthe general medical image analysis area instead of limiting to\ncancer studies, include more on recent studies (starting from\n2012), and focus on more recent ML techniques such as\nConvolution Neural Networks (CNNs). Likewise, transparent ML\nin cancer detection is also reviewed in ref. °° and structured\nfollowing the same aspects of generic transparent ML techniques,\nsuch as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1259.2,
                        1445.6,
                        2293.3,
                        1983.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_07/region_5_text.png",
                    "text": "The guidelines and systematic review of the state of the field\npresented here aim at emphasizing the need for formative user\nresearch and empirical user studies to firmly establish the validity\nof assumptions on which human factors engineering goals\n(including transparency) are based; a natural first step in human-\ncentered Al or HCl, but not yet in medical image analysis. As\nmethods for the human-centered development of transparent ML\nfor medical image analysis mature, the guidelines presented here\nmay require refinements to better reflect the challenges faced\nthen. At the time of writing, supported by the findings of the\nsystematic review, we believe that the lack of explicit formative\nresearch is the largest barrier to capitalizing on the benefits of\ntransparent ML in medical image analysis.\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1259.3,
                        1984.3,
                        2293.0,
                        3063.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_07/region_6_text.png",
                    "text": "To conclude, transparency is an affordance of transparent ML\nsystems, i.e, a relationship between models and end users.\nTherefore, especially in contexts where there exists a high\nknowledge gap between ML developers and the envisioned end\nusers, developing transparent ML algorithms without explicitly\nconsidering and involving end users may result in products that\nare unintelligible in the envisioned context and irrelevant in\npractice. Efforts to build ML systems that afford transparency in\nthe healthcare context should go beyond computational\nadvances, which—based on the findings of our systematic\nreview—is not common practice in the context of transparent\nML for medical image analysis. While many of the approaches\nclaimed transparency or derivative accomplishments in human\nfactors engineering, they did so even without defining target\nusers, engaging in formative user research, or reporting rigorous\nvalidation. Consequently, for most of the recently proposed\nalgorithms, it remains unclear whether they truly afford transpar-\nency or advance human factors engineering goals. We acknowl-\nedge that building systems that afford transparency by involving\nend users in the design process is challenging for medical image\nanalysis and related healthcare tasks. In this context, we propose\nthe INTRPRT guideline that emphasize the importance of user and\ncontext understanding for transparent ML design, but provide\nalternatives to empirical studies for formative user research. By\nfollowing these guidelines, ML designers must actively consider\ntheir end users throughout the entire design process. We hope\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_08.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_08.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 18,
                "page_idx": 8
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "text",
                    "score": 99.93,
                    "box": [
                        188.3,
                        241.6,
                        1224.6,
                        368.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_01_text.png",
                    "text": "that these design directives will catalyze forthcoming efforts to\nbuild transparent ML systems for healthcare that demonstrably\nachieve the desired human factors engineering goals.\n"
                },
                {
                    "idx": 2,
                    "thing": "title",
                    "score": 98.08,
                    "box": [
                        189.6,
                        443.5,
                        359.5,
                        485.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_02_title.png",
                    "text": "METHODS\n"
                },
                {
                    "idx": 3,
                    "thing": "title",
                    "score": 99.01,
                    "box": [
                        189.3,
                        495.3,
                        817.6,
                        535.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_03_title.png",
                    "text": "Search strategy and selection criteria\n"
                },
                {
                    "idx": 4,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        189.3,
                        546.7,
                        1223.9,
                        845.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_04_text.png",
                    "text": "The aim of the systematic review is to survey the current state of\ntransparent ML methods for medical image analysis. Because ML\ntransparency as major research thrust has emerged following the\nomnipresence of highly complex ML models, such as deep CNNs,\nwe limited our analysis to records that appeared after January\n2012, which pre-dates the onset of the ongoing surge of interest\nin learning-based image processing?'.\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        189.4,
                        848.3,
                        1223.9,
                        1234.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_05_text.png",
                    "text": "We conducted a systematic literature review in accordance with\nthe Preferred Reporting Items for Systematic reviews and Meta-\nAnalyses (PRISMA) method°?. We searched PubMed, EMBASE, and\nCompendex databases to find articles pertinent to transparent ML\n(including but not limited to explainable and interpretable ML) for\nmedical imaging by screening titles, abstracts, and keywords of all\navailable records from January 2012 through July 2021. Details of\nthe search terms and strategy can be found in Supplementary\ninformation B.\n"
                },
                {
                    "idx": 6,
                    "thing": "title",
                    "score": 98.78,
                    "box": [
                        189.1,
                        1290.4,
                        447.5,
                        1330.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_06_title.png",
                    "text": "Study selection\n"
                },
                {
                    "idx": 7,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        188.6,
                        1341.6,
                        1222.5,
                        1857.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_07_text.png",
                    "text": "Following the removal of duplicates (1731 remained), studies were\nfirst pre-screened using the title and abstract. Studies that did not\ndescribe transparent methods nor medical imaging problems\nwere immediately excluded (217 remained). We then proceeded\nto full-text review, where each study was examined to determine\nwhether the study presented and evaluated a transparent ML\ntechnique for medical image analysis. Failure to comply with the\ndescribed inclusion/exclusion criteria resulted in the study’s\nremoval from further consideration. Detailed statistics and a\ncomplete description of the pre-screening and full-text review can\nbe found in Supplementary information C and Fig. 4. 68 articles\nwere included for information extraction.\n"
                },
                {
                    "idx": 8,
                    "thing": "figure",
                    "score": 99.86,
                    "box": [
                        230.2,
                        1906.8,
                        1181.2,
                        2888.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_08_figure.png"
                },
                {
                    "idx": 9,
                    "thing": "text",
                    "score": 99.92,
                    "box": [
                        189.9,
                        2912.7,
                        1223.7,
                        3062.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_09_text.png",
                    "text": "Fig. 4 PRISMA diagram for transparent ML in medical imaging.\nThe flow diagram shows the number of records identified, of studies\nexcluded and the reasons for exclusion, and of studies included in\nour systematic review.\n"
                },
                {
                    "idx": 10,
                    "thing": "title",
                    "score": 98.23,
                    "box": [
                        1298.7,
                        241.7,
                        1708.6,
                        282.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_10_title.png",
                    "text": "Data extraction strategy\n"
                },
                {
                    "idx": 11,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1298.0,
                        292.6,
                        2333.3,
                        1132.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_11_text.png",
                    "text": "For the 68 selected articles that met the inclusion criteria, two\nauthors (H.C. and C.G.) performed detailed data extraction to\nsummarize important information related to the six themes\ndescribed in INTRPRT Guideline Section. A data extraction\ntemplate was developed by all authors and is summarized in\nSupplementary information D. Every one of the 68 articles was\nanalyzed and coded by both authors independently and one\nauthor (H.C.) merged the individual reports into a final consensus\ndocument. Despite our efforts to broadly cover all relevant search\nterms regarding transparent ML in medical imaging, we acknowl-\nedge that the list may not be exhaustive. There are vast numbers\nof articles that have imbued transparency in their methodology,\nbut transparency (or contemporary synonyms thereof, such as\nexplainability or interpretability) is not explicitly mentioned in the\ntitle, abstract, or keywords of these articles, and often not even in\nthe body of the text°?. This fact makes intractable to identify all\narticles about transparent ML methods. Finally, the review is\nlimited to published manuscripts, long articles and novel\napproaches. Publication bias may have resulted in the exclusion\nof works relevant to this review.\n"
                },
                {
                    "idx": 12,
                    "thing": "title",
                    "score": 98.86,
                    "box": [
                        1298.0,
                        1207.0,
                        2245.7,
                        1290.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_12_title.png",
                    "text": "DETAILED ANALYSIS OF FINDINGS DURING SYSTEMATIC\nREVIEW\n"
                },
                {
                    "idx": 13,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1298.3,
                        1300.3,
                        2332.1,
                        1552.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_13_text.png",
                    "text": "The data extraction template for studies included in the\nsystematic review was structured using the six themes of the\nINTRPRT guideline, the adequacy of which was confirmed during\ndata extraction. Therefore, we summarize our findings for each\ntheme and provide details of the extraction results for each article\nin Tables 2&3 in Supplementary information D.\n"
                },
                {
                    "idx": 14,
                    "thing": "title",
                    "score": 98.6,
                    "box": [
                        1298.7,
                        1607.2,
                        1582.8,
                        1645.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_14_title.png",
                    "text": "IN: incorporation\n"
                },
                {
                    "idx": 15,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1298.1,
                        1655.8,
                        2333.3,
                        2539.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_15_text.png",
                    "text": "A common trend among included studies (n = 33) was that the\npresented methods were developed by multidisciplinary clinician-\nengineering teams, as was evidenced by the incorporation of\nclinical specialists, such as physicians, radiologists, or pathologists,\nin the study team and on the author lists. In light of the current\nbias towards clinicians as end users of transparent ML algorithms,\nthis observation suggests that designers may have communicated\nwith a limited subset of the intended end users. However, no\nformative user research is explicitly described or introduced in\nthese articles to systematically understand the end users before\nimplementing the model. Further, we found that incorporating\nclinical experts did not have a considerable impact on whether\nclinical priors or standard or care guidelines (i.e., Level 2 evidence)\nwere used to build the ML system (39%/44% articles with/without\nthe incorporation of end users use clinical priors). Regarding the\ntechnical approach to provide transparency, the incorporation of\nmedical experts motivated designers to incorporate prior knowl-\nedge directly into the model structure and/or inference for\nmedical imaging (73%/64% articles with/without the incorpora-\ntion of end users do not need a second model to generate\ntransparency).\n"
                },
                {
                    "idx": 16,
                    "thing": "title",
                    "score": 98.12,
                    "box": [
                        1299.0,
                        2595.6,
                        1608.6,
                        2634.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_16_title.png",
                    "text": "IN: interpretability\n"
                },
                {
                    "idx": 17,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1298.4,
                        2646.2,
                        2332.0,
                        2980.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_17_text.png",
                    "text": "Transparency of ML systems was achieved through various\ntechniques, including attention mechanisms (n=15), use of\nhuman-understandable features (n = 11), a combination of deep\nneural networks and transparent traditional ML methods (n = 7),\nvisualization approaches (n=5), clustering methods (n= 4),\nuncertainty estimation/confidence calibration (n= 3), relation\nanalysis between outputs and hand-crafted features (n = 3), and\nother custom techniques (n = 20).\n"
                },
                {
                    "idx": 18,
                    "thing": "text",
                    "score": 99.92,
                    "box": [
                        1299.2,
                        2982.0,
                        2331.9,
                        3063.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_08/region_18_text.png",
                    "text": "The use of an attention mechanism was the most common\ntechnique for adding transparency. Attention mechanisms\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_09.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_09.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 14,
                "page_idx": 9
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        149.0,
                        240.8,
                        1185.0,
                        787.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_01_text.png",
                    "text": "enabled the generation of pixel-attribution methods” to visualize\npixel-level importance for a specific class of interest?°-©°. In\nsegmentation tasks, where clinically relevant abnormalities and\norgans are usually of small sizes, features from different resolution\nlevels were aggregated to compute attention and generate more\naccurate outcomes, as demonstrated in multiple applications, e.g.,\nmulti-class segmentation in fetal Magnetic Resonance Imagings\n(MRIs)°® and multiple sclerosis segmentation in MRIs°'. Clinical\nprior knowledge was also inserted into the attention mechanism\nto make the whole system more transparent. For instance,°° split\nbrain MRIs into 96 clinically important regions and used a genetic\nalgorithm to calculate the importance of each region to evaluate\nAlzheimer’s Disease (AD).\n"
                },
                {
                    "idx": 2,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        149.0,
                        790.0,
                        1184.0,
                        1294.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_02_text.png",
                    "text": "Human-understandable features, e.g., hand-crafted low-dimen-\nsional features or clinical variables (age, gender, etc.) were\nfrequently used to establish transparent systems. There existed\ntwo main ways to use human-understandable features in medical\nimaging: (1) Extracting hand-crafted features, e.g., morphological\nand radiomic features, from predicted segmentation masks\ngenerated by a non-transparent model’°-’? followed by analysis\nof those hand-crafted features using a separate classification\nmodule; (2) Directly predicting human-understandable features\ntogether with the main classification and detection task®°**. In\nthese approaches, all tasks usually shared the same network\narchitecture and parameter weights.\n"
                },
                {
                    "idx": 3,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        149.9,
                        1296.3,
                        1183.7,
                        1714.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_03_text.png",
                    "text": "Instead of explicitly extracting or predicting human-\nunderstandable features, other articles further analyzed deep\nencoded features with human-understandable techniques by\nfollowing clinical knowledge. Techniques such as decision trees\nwere constructed based on clinical taxonomy for hierarchical\nlearning’*®°-°°. Rule-based algorithms?' and regression meth-\nods®*? were used to promote transparency of the prediction?\ncreated a Graphical Convolution Network (GCN) based on clinical\nknowledge to model the correlations among colposcopic images\ncaptured around five key time slots during a visual examination.\n"
                },
                {
                    "idx": 4,
                    "thing": "text",
                    "score": 99.94,
                    "box": [
                        150.0,
                        1716.6,
                        1183.3,
                        1925.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_04_text.png",
                    "text": "We also identified various other methods to create transparent\nsystems. These methods can be categorized as visualization-\nbased, feature-based, region importance-based, and architecture\nmodification-based methods. Each approach is discussed in\ndetail below.\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        149.8,
                        1926.8,
                        1183.8,
                        2388.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_05_text.png",
                    "text": "Visualization-based methods provide easy-to-understand illus-\ntrations by overlaying the original images with additional visual\nlayouts generated from transparency techniques. There existed\ntwo main visualization-based methods: (1) Visualizing pixel-\nattribution maps: These maps may be generated using gradient-\nbased importance analysis?*°, pixel-level predicted probability”®,\nor a combination of different levels of feature maps?”’”®. (2) Latent\nfeature evolution: Encoded features were evolved according to\nthe gradient ascent direction so that the decoded image (e.g.,\ngenerated with an auto-encoder technique®’) gradually change\nfrom one class to another!°°'°!,\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        150.0,
                        2391.0,
                        1184.1,
                        2810.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_06_text.png",
                    "text": "Feature-based methods directly analyze encoded features in an\nattempt to make the models transparent. Various feature-based\ntransparency method were proposed for transparent learn-\ning'°2-'% first encoded images to deep features and then\nclustered samples based on these deep features for prediction\nor image grouping tasks. Feature importance was also well-\nstudied to identify features that are most relevant for a specific\nclass by feature perturbation'°*'°° and gradients'©”'°8 identified\nand removed features with less importance for final prediction\nthrough feature ranking.\n"
                },
                {
                    "idx": 7,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        150.2,
                        2812.1,
                        1183.9,
                        3063.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_07_text.png",
                    "text": "As an alternative to measure feature contribution, input region\nimportance was also analyzed to reveal sub-region relevance to\neach prediction class. Image occlusion with blank  sub-\nregions'°?-''' and healthy-looking sub-regions''* was used to\nfind the most informative and relevant sub-regions for classifica-\ntion and detection tasks.\n"
                },
                {
                    "idx": 8,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1259.3,
                        240.8,
                        2293.0,
                        822.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_08_text.png",
                    "text": "Other approaches modified the network architecture according\nto relevant clinical knowledge to make the whole system\ntransparent?’ pruned the architecture according to the degree\nof scale invariance at each layer in the network''? created ten\nbranches with shared weights for ten ultrasound images to mimic\nthe clinical workflow of liver fibrosis stage prediction''* aggre-\ngated information from all three views of mammograms and used\ntraditional methods to detect nipple and muscle direction, which\nwas followed by a grid alignment according to the nipple and\nmuscle direction for left and right breasts''? proposed to learn\nrepresentations of the underlying anatomy with a convolutional\nauto-encoder by mapping the predicted and ground truth\nsegmentation maps to a low dimensional representation to\nregularize the training objective of the segmentation network.\n"
                },
                {
                    "idx": 9,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1258.3,
                        822.6,
                        2292.4,
                        1153.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_09_text.png",
                    "text": "Some other methods used the training image distribution to\nachieve transparency in classification''® used _ similar-looking\nimages (nearest training images in feature space) to classify\ntesting images with majority votes. Causal inference with plug-in\nclinical prior knowledge also introduced transparency directly to\nautomatic systems''”-''?. Confidence calibration and uncertainty\nestimation methods were also used to generate additional\nconfidence information for end users!7°-'?,\n"
                },
                {
                    "idx": 10,
                    "thing": "title",
                    "score": 97.48,
                    "box": [
                        1258.7,
                        1208.9,
                        1418.3,
                        1250.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_10_title.png",
                    "text": "T: targets\n"
                },
                {
                    "idx": 11,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1259.1,
                        1257.6,
                        2293.4,
                        1797.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_11_text.png",
                    "text": "A striking observation was that none of the selected articles aimed\nat building transparent systems for users other than care\nproviders. Less than half of the articles explicitly specified\nclinicians as the intended end users of the system (n = 30). From\nthe remaining 38 articles, 17 articles implied that the envisioned\nend users would be clinicians, while the remaining 21 did not\nspecify the envisioned target users. Articles that were more\nexplicit about their end users were more likely to rely on clinical\nprior knowledge (Level 2 evidence) in model design. In total, 47%\nof articles that specified or implied clinicians as end users\nimplemented clinical prior knowledge in the transparent systems\nwhile only 18% of articles without end user information use\nclinical prior knowledge.\n"
                },
                {
                    "idx": 12,
                    "thing": "title",
                    "score": 98.25,
                    "box": [
                        1257.7,
                        1851.3,
                        1460.3,
                        1892.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_12_title.png",
                    "text": "R: reporting\n"
                },
                {
                    "idx": 13,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1259.1,
                        1900.2,
                        2293.2,
                        2729.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_13_text.png",
                    "text": "Evaluating different properties of a transparent algorithm besides\ntask-related metrics, especially its performance in regards to\nachieving the desired human factors engineering goals, comple-\nments the assessment of the ML model's intended purpose. We\nidentified that the quality of the transparency component is\ncurrently being evaluated through four main approaches. The first\none involves metrics based on human perception, such as the\nmean opinion score introduced in ref. ''> to capture two expert\nparticipants’ rating of the model’s outcome quality and similarity\nto the ground truth on a 5-point scale. Using two study\nparticipants, pathologists’ feedback was also requested in ref. '°”\nto assess their agreement with patch-based visualizations that\ndisplay features relevant for normal and abnormal tissue. The level\nof agreement was not formally quantified, but reported as a\nqualitative description. Similarly, one study participant was\ninvolved in a qualitative assessment of explanations quality in\nref. ®'1°8. These evaluations are different from empirical user\nstudies as they are limited to a few individuals and were mostly\nused to subjectively confirm the correctness of the transparent\ncomponent.\n"
                },
                {
                    "idx": 14,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        1258.5,
                        2732.7,
                        2292.1,
                        3063.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_14_text.png",
                    "text": "The second approach attempted to quantify the quality of\nexplanations for a specific purpose (functionally-grounded evalua-\ntion'?). For instance, some articles evaluated the localization ability\nof post-hoc explanations by defining an auxiliary task, such as\ndetection®”** or segmentation®”®°*''? of anatomical structures\nrelated to the main task. They then contrasted relevant regions\nidentified by the model with ground truth annotations. These\nquantitative measures (dice score, precision, recall) allowed for\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_10.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_10.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 10,
                "page_idx": 10
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        189.3,
                        241.4,
                        1224.3,
                        656.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_01_text.png",
                    "text": "further comparisons with traditional explanations methods. Simi-\nlarly''®, defined a multi-task learning framework for image classifica-\ntion and retrieval, evaluating retrieval precision and providing a\nconfidence score based on the retrieved neighbors as an attempt to\ncheck the learned embedding space. Capturing relevant features\nconsistent with human intuition was proposed in ref. '°° by\nmeasuring the fraction of reference features recovered, which were\ndefined according to a guideline. Overall, the evaluation of\nexplanations through auxiliary tasks required additional manual\nefforts to get the necessary ground truth annotations.\n"
                },
                {
                    "idx": 2,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        188.6,
                        656.7,
                        1222.8,
                        1652.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_02_text.png",
                    "text": "Properties of the explanation itself were also quantified as their\nusefulness to identify risky and safe predictions at a voxel-level for\nthe main task by thresholding on their predictive uncertainty\nvalues'*?. Other properties of explanations, such as their correct-\nness (accuracy of rules), completeness (fraction of the training set\ncovered) and compactness (size in bytes) were measured in ref. °”.\nA measure related to completeness was defined in ref. °° and\naimed to capture the proportion of training images represented\nby the learned visual concepts, in addition to two other metrics:\nthe inter- and intra-class diversity and the faithfulness of\nexplanations computed by perturbing relevant patches and\nmeasuring the drop in classification confidence. Other articles\nfollowed a similar approach to validate relevant pixels or features\nidentified with a transparent method; for example, in ref. % a\ndeletion curve was constructed by plotting the dice score vs. the\npercentage of pixels removed and ref. °° defined a recall rate\nwhen the model proposes certain number of informative\nchannels?’ proposed to evaluate the consistency of visualization\nresults and the outputs of a CNN by computing the L7 error\nbetween predicted class scores and explanation pixel-attribution\nmaps. In summary, while the methods grouped in this theme are\ncapable of evaluating how well a method aligns with it's intended\nmechanism of transparency, they fall short of capturing any\nhuman factors-related aspects of transparency design.\n"
                },
                {
                    "idx": 3,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        188.8,
                        1654.8,
                        1222.7,
                        2357.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_03_text.png",
                    "text": "The third, and most common approach, involved a qualitative\nvalidation of the transparent systems (n= 40) by showing pixel-\nattribution visualizations overlaid with the input image or rankings\nof feature relevance, along with narrative observations on how these\nvisualizations may relate to the main task. These qualitative\nnarratives might include comparisons with other visualization\ntechniques in terms of the highlighted regions or the granularity/\nlevel of details. Furthermore, following a retrospective analysis, the\nconsistency between the identified relevant areas/features and prior\nclinical knowledge in a specific task was a common discussion item\nin 37% of all the articles (n = 25); refer to articles®?:2°8%'19'!7 for\nexamples. While grounding of feature visualizations in the relevant\nclinical task is a commendable effort, the methods to generate the\noverlaid information have been criticized in regards to their fidelity\nand specificity>*'**, Further, as was the case for methods that\nevaluate the fidelity of transparency information, these methods do\nnot inherently account for human factors.\n"
                },
                {
                    "idx": 4,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        188.4,
                        2361.0,
                        1222.8,
                        3063.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_04_text.png",
                    "text": "Lastly, transparent systems can be directly evaluated through\nuser studies on the target population, in which the end users\ninteract with the developed ML system to complete a task based\non a specific context. In ref. °°, the evaluation was centered on the\nutility of example-based and feature-based explanations for\nradiologists (8 study participants) to understand the Al decision\nprocess. Users’ understanding was evaluated as the accuracy to\npredict the Al's diagnosis for a target image and a binary\njudgment on whether they certify the Al for similar images (and\njustify using multiple-choice options). Users’ agreement with the\nAl's predictions was measured as well. The empirical evidence\nsuggested that explanations enabled radiologists to develop\nappropriate trust by making an accurate prediction and judgment\nof the Al's recommendations. Even though radiologists could\ncomplete the task by themselves, a comparison with the team\nperformance was not included, nor the performance of the Al\nmodel in standalone operation. An alternative evaluation of\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        1298.6,
                        242.2,
                        2333.2,
                        1287.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_05_text.png",
                    "text": "example-based explanation usefulness was performed in ref. '*',\nin which pathologists (14 study participants) determined the\nacceptability of a decision support tool by rating adjectives related\nto their perceived objectivity, details, reliability, and quality of the\nsystem. Compared to a CNN without explanations, the subjective\nratings were more positive towards the explainable systems.\nHowever, neither the team (expert + Al) nor expert baseline\nperformance was evaluated. The benefit of involving a dermatol-\nogist to complete an image grouping task was demonstrated in\nref. '°2, in which domain knowledge was used to constrain\nupdates of the algorithm’s training, resulting in a better grouping\nperformance than a fully automated method. The user evaluation\nonly measured the task performance. These studies that explicitly\ninvolve target users to identify whether the envisioned human\nfactors engineering goals were met stand out from the large body\nof work that did not consider empirical user tests. It is, however,\nnoteworthy that even these exemplary studies are based on very\nsmall sample sizes that may not be sufficiently representative of\nthe target users. Careful planning of the study design (including\nhypothesis statement, experimental design and_ procedure,\nparticipants, and measures) that allows to properly evaluate\nwhether the system achieves the intended goals by adding\ntransparency to the ML system is fundamental, especially\nconsidering the resources needed and challenges involved in\nconducting user testing in the healthcare domain.\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1297.9,
                        1289.9,
                        2332.0,
                        1662.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_06_text.png",
                    "text": "Even though there were articles that assessed human factors-\nrelated properties of the transparency mechanism, a striking\nmajority of articles did not report metrics beyond performance in\nthe main task (n=49) or did not discuss the transparency\ncomponent at all (n = 9). Task performance was evaluated in the\nmajority of the articles, 91% (n = 62), and most of them contrasted\nthe performance of the transparent systems with a non-\ntransparent baseline (n = 41). Of those, 36 works (88%) reported\nimproved performance and 5 (12%) comparable results.\n"
                },
                {
                    "idx": 7,
                    "thing": "title",
                    "score": 97.52,
                    "box": [
                        1298.7,
                        1719.2,
                        1460.4,
                        1757.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_07_title.png",
                    "text": "PR: priors\n"
                },
                {
                    "idx": 8,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        1298.4,
                        1769.8,
                        2332.7,
                        2143.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_08_text.png",
                    "text": "We differentiate two types of priors that can be used as a source of\ninspiration to devise transparent ML techniques: (1) Priors based on\ndocumented knowledge, and especially clinical guidelines consider-\ning the unvaried end user specification identified above; and (2)\nPriors based on computer vision concepts. Most (93%) articles that\nincorporated clinical knowledge priors (n = 28) directly implemen-\nted these priors into the model structure and/or inference, while\nonly 68% articles with computer vision priors (n = 40) provided\ntransparency by the model itself and/or the inference procedure.\n"
                },
                {
                    "idx": 9,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        1296.7,
                        2145.7,
                        2333.0,
                        2895.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_09_text.png",
                    "text": "A direct way to include clinical knowledge priors was through\nthe prediction, extraction, or use of human-understandable\nfeatures. Morphological features, e.g., texture, shape and edge\nfeatures were frequently considered and used to support the\ntransparency of ML_ systems’®’%73:75.7681,83,93 Biomarkers for\nspecific problems, e.g., end-diastolic volume (EDV) in cardiac\nMRI’®:72 and mean diameter, consistency, and margin of\npulmonary nodules®° were commonly computed to establish\ntransparency. For problems with a well-established image report-\ning and diagnosis systems, routinely-used clinical features, e.g.,\nLiver Imaging Reporting and Data System (LI-RADS) features for\nHepatocellular carcinoma (HCC) classification®* or BI-RADS for\nbreast mass®* suggested that the ML systems may be intuitively\ninterpretable to experts that are already familiar with these\nguidelines. Human-understandable features relevant to the task\ndomain were extracted from pathology images, e.g., area and\ntissue structure features’°. Radiomic features were also computed\nto establish the transparency of ML systems’*'”°.\n"
                },
                {
                    "idx": 10,
                    "thing": "text",
                    "score": 99.95,
                    "box": [
                        1298.5,
                        2898.2,
                        2332.4,
                        3062.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_10/region_10_text.png",
                    "text": "Besides human-understandable features, clinical knowledge can\nbe used to guide the incorporation of transparency within a\nmodel. Some articles (n=11) mimicked or started from\nclinical guidelines and workflows to construct the ML\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_11.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_11.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 13,
                "page_idx": 11
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        148.6,
                        238.5,
                        1184.8,
                        866.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_01_text.png",
                    "text": "systemso>:7481,82,106,113-115,118,119,74,113,1 14 followed the clinical work-\nflow to encode multiple sources of images and fused the encoded\ninformation for the final prediction. Other works followed the\nspecific clinical guidelines of the problems to create transparent\nsystems® split brain MRIs into 96 clinical meaningful regions as\nwould be done in established clinical workflows and analyze all the\nregions separately. Some other clinical knowledge priors were also\npresented®°**°°'2 established a hierarchical label structure accord-\ning to clinical taxonomy for image classification’’ leveraged the\ntransparency from the correlation between the changes of\npolarization characteristics and the pathological development of\ncervical precancerous lesions. Clinical knowledge from human\nexperts was used to refine an image grouping algorithm through\nan interactive mechanism in which experts iteratively provided\ninputs to the model'%.\n"
                },
                {
                    "idx": 2,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        148.4,
                        870.5,
                        1183.4,
                        1745.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_02_text.png",
                    "text": "Priors that were derived from computer vision concepts rather\nthan the clinical workflow were usually not specific or limited to a\nsingle application. The justification of transparency with computer\nvision priors was more general than that with clinical knowledge\npriors. Image visualization-based techniques to achieve transpar-\nency were most commonly considered in image classification\nproblems. Common ways of retrieving relevance information\nwere: Visual relevancy through attention®>-°*°°©8; region occlu-\nsion by blank areas'°”\"\"' or healthy-looking regions''; and other\ntechniques such as supervision of activated image regions by\nclinically relevant areas®®8%9794.9597,°8° and image similarity”®.\nFeature-based computer vision transparency priors focused on the\nimpact of feature evolution or perturbation on the decoded\noutput. Encoded features were evolved according to the gradient\nascent direction to create the evolution of the decoded image\nfrom one class to the other8”''°'. Some articles directly\nanalyzed the feature sensitivity to the final prediction by feature\nperturbation'°''!°°\"!° and importance analysis’”'°7'°8, feature\ndistribution'®*'°> or image distribution based on encoded\nfeatures'°?''®, Confidence calibration and uncertainty estimation\nalso increased the transparency of the ML systems'7°-'22,\n"
                },
                {
                    "idx": 3,
                    "thing": "text",
                    "score": 99.97,
                    "box": [
                        148.6,
                        1747.3,
                        1184.3,
                        2625.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_03_text.png",
                    "text": "Even though we attempted to identify the type of prior\nevidence used to justify the development of a specific algorithm\nin each ML system, none of the included articles formally\ndescribed the process to formulate such priors to achieve\ntransparency in the proposed system. While the use of clinical\nguidelines and routine workflows may provide Level 2 evidence in\nsupport of the method affording transparency if the end users are\nmatched with those priors, relying solely on computer vision\ntechniques may not provide the same level of justification. This is\nbecause computer vision algorithms are often developed as an\nanalysis tool for ML developers to verify model correctness, but\nare not primarily designed nor evaluated for use in end user-\ncentered interfaces. The lack of justification and formal processes\nto inform design choices at the early stages of model develop-\nment results in substantial risk of creating transparent systems\nthat rely on inaccurate, unintelligible, or irrelevant insights for end\nusers. Being explicit about the assumptions and evidence\navailable in support of the envisioned transparent ML system is\nparamount to build fewer but better-justified transparent ML\nsystems that are more likely to live up to expectations in final user\ntesting, the resources for which are heavily constrained.\n"
                },
                {
                    "idx": 4,
                    "thing": "title",
                    "score": 98.03,
                    "box": [
                        151.0,
                        2680.6,
                        262.0,
                        2720.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_04_title.png",
                    "text": "T: task\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 99.98,
                    "box": [
                        149.5,
                        2729.7,
                        1183.9,
                        3063.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_05_text.png",
                    "text": "Various types of medical image analysis tasks were explored in the\nincluded articles. Most of the articles (n = 57) proposed transparent\nML algorithms for classification and detection problems, such as\nimage classification and abnormality detection. Three-dimensional\n(3D) radiology images (n= 24) and pathological images (n = 15)\nwere the most popular modalities involved in the development of\ntransparent algorithms. The complex nature of both 3D imaging in\nradiology and pathological images makes image analysis tasks more\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 99.93,
                    "box": [
                        1259.1,
                        242.4,
                        2292.5,
                        1071.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_06_text.png",
                    "text": "time consuming than 2D image analysis that is more prevalent in\nother specialities, such as dermatology, which motivates transpar-\nency as an alternative to complete human image analysis to save\ntime while retaining trustworthiness. In detail, classification pro-\nblems in 3D radiological images and pathological images included\nabnormality detection in computed tomography (CT)\nsca 1Ns2029.6,73,75,90,95,1 06, MRIs22:00:65:77-79:83,84,98,1 00,105,110,112,11 7\npathology images°??7-99.6269-71 ,77,104,107-109,116,121 and positron\nemission tomography (PET) images®*?. Mammography dominated\nthe 2D radiology image applications’°2'87879294114.119.125 mainly\nfocusing on breast cancer classification and mass detection. For\nother 2D radiology image applications**''®, aimed at pneumonia\nand pneumothorax prediction from chest X-rays and''? created a\ntransparent model for liver fibrosis stage prediction in liver\nultrasound images. Classification and detection tasks were explored\nin other clinical specialities, including melanoma® and skin lesion\ngrade prediction®®*°*” in dermatology, glaucoma detection from\nfundus images®’*°” and retinopathy diagnosis''' in ophthalmol-\nogy, and polyp classification from colonoscopy images in\ngastroenterology®®'”°.\n"
                },
                {
                    "idx": 7,
                    "thing": "text",
                    "score": 99.95,
                    "box": [
                        1258.3,
                        1073.9,
                        2291.5,
                        1445.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_07_text.png",
                    "text": "Segmentation was another major application field (n=9).\nResearch about transparency mainly focused on segmentation\nproblems for brain and cardiac MRIs°’4+6”7289103115. Other\nsegmentation problems included mass segmentation in mammo-\ngrams’°, cardiac segmentation in ultrasound\", liver tumor seg-\nmentation in hepatic CT images, and skin lesion segmentation in\ndermatological images>*®. There also existed other applications, e.g.,\nimage grouping in dermatological images'°? and image enhance-\nment (super resolution task) in brain MRIs'?? and cardiac MRIs''°.\n"
                },
                {
                    "idx": 8,
                    "thing": "text",
                    "score": 99.96,
                    "box": [
                        1259.3,
                        1447.5,
                        2293.0,
                        2274.3
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_08_text.png",
                    "text": "Most of the application tasks were routinely performed by\nhuman experts in current clinical practice (n = 60). A much smaller\nsample of articles (n= 4) aimed to build transparent systems for\nmuch more difficult tasks where no human baseline exists, e.g.,\n5-class molecular phenotype classification from Whole Slide Images\n(WSls)’°®8, 5-class polyp classification from colonoscopy images'”°,\ncardiac resynchronization therapy response prediction from cardiac\nMRIs®?, and super resolution of brain MRIs'22. The remaining articles\n(n=4) did not include explicit information on whether human\nbaselines and established criteria exist for the envisioned applica-\ntion, e.g., magnification level and nuclei area prediction in breast\ncancer histology images”®, age estimation in brain MRIs°°, AD status\nin Diffusion Tensor Images (DTIs), and risk of sudden cardiac death\nprediction in cardiac MRIs’°. As previously mentioned, tasks that are\nroutinely performed in clinical evidence may have robust human\nbaselines and clinical guidelines to guide transparent ML develop-\nment. Applications that are beyond the current possibilities,\nhowever, require a more nuanced and human-centered approach\nthat should involve the target end users as early as possible to\nverify that the assumptions that drive transparency are valid.\n"
                },
                {
                    "idx": 9,
                    "thing": "title",
                    "score": 95.64,
                    "box": [
                        1258.4,
                        2358.6,
                        1594.0,
                        2398.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_09_title.png",
                    "text": "DATA AVAILABILITY\n"
                },
                {
                    "idx": 10,
                    "thing": "text",
                    "score": 99.75,
                    "box": [
                        1258.6,
                        2410.9,
                        2292.0,
                        2667.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_10_text.png",
                    "text": "Figure 2 contains images from the ORIGA'*’ and BraTS2020 datasets'*°. The ORIGA\ndataset is a public dataset at Kaggle website (https://www.kaggle.com/datasets/\nsshikamaru/glaucoma-detection/metadata). The BraTS2020 dataset is also a public\ndataset at Kaggle website (https://www.kaggle.com/datasets/awsaf49/brats2020-\ntraining-data). The authors declare that all the data included in this study are\navailable within the paper and its Supplementary Information files. Please contact\nauthor HC to request the data.\n"
                },
                {
                    "idx": 11,
                    "thing": "text",
                    "score": 85.91,
                    "box": [
                        1258.1,
                        2719.2,
                        2150.4,
                        2803.6
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_11_text.png",
                    "text": "Received: 15 April 2022; Accepted: 29 September 2022;\nPublished online: 19 October 2022\n"
                },
                {
                    "idx": 12,
                    "thing": "title",
                    "score": 89.14,
                    "box": [
                        1259.7,
                        2936.3,
                        1467.3,
                        2975.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_12_title.png",
                    "text": "REFERENCES\n"
                },
                {
                    "idx": 13,
                    "thing": "list",
                    "score": 89.1,
                    "box": [
                        1286.8,
                        2989.2,
                        2291.4,
                        3057.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_11/region_13_list.png",
                    "text": "1. Topol, E. J. High-performance medicine: the convergence of human and artificial\nintelligence. Nat. Med. 25, 44-56 (2019).\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_12.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_12.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 2,
                "page_idx": 12
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "list",
                    "score": 97.56,
                    "box": [
                        204.9,
                        249.0,
                        1224.8,
                        3050.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_12/region_1_list.png",
                    "text": "2. Obermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. Dissecting racial bias in\nan algorithm used to manage the health of populations. Science 366, 447-453\n(2019).\n\n3. Ghassemi, M., Oakden-Rayner, L. & Beam, A. L. The false hope of current\napproaches to explainable artificial intelligence in health care. Lancet Digital\nHealth 3, e745-e750 (2021).\n\n4. McCoy, L. G., Brenna, C. T., Chen, S. S., Vold, K. & Das, S. Believing in black boxes:\nMachine learning for healthcare does not need explainability to be evidence-\nbased. J. Clin. Epidemiol. 142, 252-257 (2022).\n\n5. Vellido, A. The importance of interpretability and visualization in machine\nlearning for applications in medicine and health care. Neural Comput. Appl. 32,\n18069-18083 (2020).\n\n6. Char, D. S., Abramoff, M. D. & Feudtner, C. Identifying ethical considerations\nfor machine learning healthcare applications. Am. J. Bioethics 20, 7-17\n(2020).\n\n7. Holzinger, A., Langs, G. Denk, H., Zatloukal, K. & Muller, H. Causability and\nexplainability of artificial intelligence in medicine. Wiley Interdiscip. Rev.: Data\nMining Knowl. Discov. 9, e€1312 (2019).\n\n8. Markus, A. F., Kors, J. A. & Rijnbeek, P. R. The role of explainability in creating\ntrustworthy artificial intelligence for health care: a comprehensive survey of the\nterminology, design choices, and evaluation strategies. J. Biomed. Inf. 113,\n103655 (2021).\n\n9. Salahuddin, Z., Woodruff, H. C., Chatterjee, A. & Lambin, P. Transparency of deep\nneural networks for medical image analysis: A review of interpretability meth-\nods. Comput. Biology Med. 140, 105111 (2022).\n\n10. Banegas-Luna, A. J. et al. Towards the interpretability of machine learning\npredictions for medical applications targeting personalised therapies: A cancer\ncase survey. Int. J. Mol. Sci. 22, 4394 (2021).\n\n11. Ploug, T. & Holm, S. The four dimensions of contestable ai diagnostics-a patient-\ncentric approach to explainable ai. Artif. Intell. Med. 107, 101901 (2020).\n\n12. Amann, J., Blasimme, A., Vayena, E., Frey, D. & Madai, V. |. Explainability for\nartificial intelligence in healthcare: a multidisciplinary perspective. BMC Med. Inf.\nDec. Making 20, 1-9 (2020).\n\n13. Norman, D. A. Affordance, conventions, and design. /nteractions 6, 38-43 (1999).\n\n14. Cai, C. J. et al. Human-centered tools for coping with imperfect algorithms\nduring medical decision-making. In Proceedings of the 2019 CHI Conference on\nHuman Factors in Computing Systems, 1-14 (2019).\n\n15. Xie, Y., Chen, M., Kao, D., Gao, G. & Chen, X. Chexplain: Enabling physicians to\nexplore and understand data-driven, Al-enabled medical imaging analysis. In\nProceedings of the 2020 CHI Conference on Human Factors in Computing Systems,\n1-13 (2020).\n\n16. Jacobs, M. et al. Designing Al for trust and collaboration in time-constrained\nmedical decisions: A sociotechnical lens. In Proceedings of the 2021 CHI Con-\nference on Human Factors in Computing Systems, 1-14 (2021).\n\n17. Suresh, H., Gomez, S. R., Nam, K. K. & Satyanarayan, A. Beyond expertise and\nroles: A framework to characterize the stakeholders of interpretable machine\nlearning and their needs. In Proceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems, 1-16 (2021).\n\n18. Lai, V. & Tan, C. On human predictions with explanations and predictions of\nmachine learning models: A case study on deception detection. In Proceed-\nings of the conference on fairness, accountability, and transparency, 29-38\n(2019).\n\n19. Eiband, M. et al. Bringing transparency design into practice. In 23rd international\nconference on intelligent user interfaces, 211-223 (2018).\n\n20. Wang, X. & Yin, M. Are explanations helpful? a comparative study of the effects\nof explanations in Al-assisted decision-making. In 26th International Conference\non Intelligent User Interfaces, 318-328 (2021).\n\n21. Cheng, H.-F. et al. Explaining decision-making algorithms through ui: Strategies\nto help non-expert stakeholders. In Proceedings of the 2019 chi conference on\nhuman factors in computing systems, 1-12 (2019).\n\n22. Smith-Renner, A. et al. No explainability without accountability: An empirical\nstudy of explanations and feedback in interactive ml. In Proceedings of the 2020\nCHI Conference on Human Factors in Computing Systems, 1-13 (2020).\n\n23. Bansal, G. et al. Does the whole exceed its parts? the effect of ai explanations on\ncomplementary team performance. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems, 1-16 (2021).\n\n24. Bansal, G. et al. Beyond accuracy: The role of mental models in human-Al team\nperformance. Proceedings of the AAAI Conference on Human Computation and\nCrowdsourcing 7, 2-11 (2019).\n\n25. Nourani, M. et al. Anchoring bias affects mental model formation and user\nreliance in explainable ai systems. In 26th International Conference on Intelligent\nUser Interfaces, 340-350 (2021).\n\n26. McCoy, L. G., Brenna, C. T., Chen, S., Vold, K. & Das, S. Believing in black boxes:\nMachine learning for healthcare does not need explainability to be evidence-\nbased. J. Clin. Epidemiol 142, 252-257 (2021).\n"
                },
                {
                    "idx": 2,
                    "thing": "list",
                    "score": 98.03,
                    "box": [
                        1314.4,
                        247.8,
                        2331.0,
                        3047.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_12/region_2_list.png",
                    "text": "27. Deeley, M. et al. Segmentation editing improves efficiency while reducing inter-\nexpert variation and maintaining accuracy for normal brain tissues in the pre-\nsence of space-occupying lesions. Phys. Med. Biol. 58, 4071 (2013).\n\n28. Banerjee, |. et al. Reading race: Ai recognises patient's racial identity in medical\nimages. preprint at https://arxiv.org/abs/2107.10356 (2021).\n\n29. Liu, T. A. et al. Gene expression profile prediction in uveal melanoma using deep\nlearning: A pilot study for the development of an alternative survival prediction\ntool. Ophthalmol. Retina 4, 1213-1215 (2020).\n\n30. Lu, M. Y. et al. Deep learning-based computational pathology predicts origins\nfor cancers of unknown primary. preprint at https://arxiv.org/abs/2006.13932\n(2020).\n\n31. Cai, C. J., Winter, S., Steiner, D., Wilcox, L. & Terry, M. \"hello Al\": Uncovering the\nonboarding needs of medical practitioners for human-ai collaborative decision-\nmaking. Proce. ACM Human-Comput. Interaction 3, 1-24 (2019).\n\n32. Wang, D., Yang, Q., Abdul, A. & Lim, B. Y. Designing theory-driven user-centric\nexplainable Al. In Proceedings of the 2019 CHI conference on human factors in\ncomputing systems, 1-15 (2019).\n\n33. Nourani, M., King, J. & Ragan, E. The role of domain expertise in user trust and\nthe impact of first impressions with intelligent systems. In Proceedings of the\nAAAI Conference on Human Computation and Crowdsourcing, vol. 8, 112-121\n(2020).\n\n34. Bucinca, Z., Malaya, M. B. & Gajos, K. Z. To trust or to think: Cognitive forcing\nfunctions can reduce overreliance on ai in ai-assisted decision-making. Pro-\nceedings of the ACM on Human-Computer Interaction 5, 1-21 (2021).\n\n35. Gaube, S. et al. Do as ai say: susceptibility in deployment of clinical decision-aids.\nNPJ Digital Medicine 4, 1-8 (2021).\n\n36. Amershi, S. et al. Guidelines for human-Al interaction. In Proceedings of the 2019\nchi conference on human factors in computing systems, 1-13 (2019).\n\n37. Liao, Q. V., Gruen, D. & Miller, S. Questioning the Al: informing design practices\nfor explainable ai user experiences. In Proceedings of the 2020 CHI Conference on\nHuman Factors in Computing Systems, 1-15 (2020).\n\n38. Mohseni, S., Zarei, N. & Ragan, E. D. A multidisciplinary survey and framework for\ndesign and evaluation of explainable Al systems. ACM Trans. Interactive Intell.\nSyst. 11, 1-45 (2021).\n\n39. Chen, J., Kallus, N., Mao, X., Svacha, G. & Udell, M. Fairness under unawareness:\nAssessing disparity when protected class is unobserved. In Proceedings of the\nconference on fairness, accountability, and transparency, 339-348 (2019).\n\n40. Datta, A, Tschantz, M. C. & Datta, A. Automated experiments on ad privacy\nsettings: A tale of opacity, choice, and discrimination. preprint at https://\narxiv.org/abs/1408.6491 (2014).\n\n41. Leslie, D. Understanding artificial intelligence ethics and safety: A guide for the\nresponsible design and implementation of ai systems in the public sector.\nAvailable at SSRN 3403301 (2019).\n\n42. Sokol, K. & Flach, P. Explainability fact sheets: a framework for systematic\nassessment of explainable approaches. In Proceedings of the 2020 Conference on\nFairness, Accountability, and Transparency, 56-67 (2020).\n\n43. Liu, X., Rivera, S. C., Moher, D., Calvert, M. J. & Denniston, A. K. Reporting\nguidelines for clinical trial reports forinterventions involving artificial intelli-\ngence: the consort-Al extension. BMJ 370, m3164, https://doi.org/10.1136/\nbmj.m3164 (2020).\n\n44. DECIDE-Al Steering Group. DECIDE-Al: new reporting guidelines to bridge the\ndevelopment-to-implementation gap in clinical artificial intelligence. Nat. Med.\n27, 186-187 (2021).\n\n45. Cabitza, F. & Campagner, A. The need to separate the wheat from the chaff in\nmedical informatics: Introducing a comprehensive checklist for the (self)-\nassessment of medical ai studies (2021).\n\n46. Hernandez-Boussard, T., Bozkurt, S., loannidis, J. P. & Shah, N. H. Minimar\n(minimum information for medical ai reporting): developing reporting stan-\ndards for artificial intelligence in health care. J. Am. Med. Inf. Assoc. 27,\n2011-2015 (2020).\n\n47. Scott, |, Carter, S. & Coiera, E. Clinician checklist for assessing suitability of\nmachine learning applications in healthcare. BMJ Health & Care Informatics 28\n(2021).\n\n48. Tjoa, E. & Guan, C. A survey on explainable artificial intelligence (xai): Toward\nmedical xai. EEE Trans. Neural Netw. Learn. Syst. 32, 4793-4813 (2020).\n\n49. van der Velden, B. H., Kuijf, H. J., Gilhuijs, K. G. & Viergever, M. A. Explainable\nartificial intelligence (xai) in deep learning-based medical image analysis. Med.\nImage Anal 79, 102470 (2022).\n\n50. Gulum, M. A., Trombley, C. M. & Kantardzic, M. A review of explainable deep\nlearning cancer detection models in medical imaging. Appl. Sci. 11, 4573 (2021).\n\n51. Krizhevsky, A., Sutskever, |. & Hinton, G. E. Imagenet classification with deep\nconvolutional neural networks. Commun. ACM 60, 84-90 (2017).\n\n52. Moher, D., Liberati, A., Tetzlaff, J., Altman, D. G. & Group, P. Preferred reporting\nitems for systematic reviews and meta-analyses: the prisma statement. PLoS\nMed. 6, €1000097 (2009).\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_13.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_13.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 2,
                "page_idx": 13
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "list",
                    "score": 97.41,
                    "box": [
                        166.0,
                        249.2,
                        1182.5,
                        3012.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_13/region_1_list.png",
                    "text": "53. Rudin, C. Stop explaining black box machine learning models for high stakes\ndecisions and use interpretable models instead. Nat. Mach. Intell. 1, 206-215\n(2019).\n\n54. Molnar, C. Interpretable machine learning (Lulu. com, 2020).\n\n55. Abdel Magid, S. et al. Channel Embedding for Informative Protein Identification\nfrom Highly Multiplexed Images. 23rd International Conference on Medical Image\nComputing and Computer-Assisted Intervention, MICCAI 2020, October 4, 2020 -\nOctober 8, 2020 12265 LNCS, 3-13 (2020).\n\n56. Afshar, P. et al. MIXCAPS: A capsule network-based mixture of experts for lung\nnodule malignancy prediction. Pattern Recognition 116, https://doi.org/10.1016/\nj.patcog.2021.107942NS (2021).\n\n57. Fan, M., Chakraborti, T., Chang, E. |. C., Xu, Y. & Rittscher, J. Microscopic Fine-\nGrained Instance Classification Through Deep Attention. 23rd International\nConference on Medical Image Computing and Computer-Assisted Intervention,\nMICCAI 2020, October 4, 2020 - October 8, 2020 12265 LNCS, 490-499 (2020).\n\n58. Graziani, M., Lompech, T., Muller, H., Depeursinge, A. & Andrearczyk, V. Inter-\npretable CNN Pruning for Preserving Scale-Covariant Features in Medical Ima-\nging. 3rd International Workshop on Interpretability of Machine Intelligence in\nMedical Image Computing, iMIMIC 2020, the 2nd International Workshop on\nMedical Image Learning with Less Labels and Imperfect Data, MIL3ID 2020, and the\n5th International Workshop o 12446 LNCS, 23-32 (2020).\n\n59. An, F., Li, X. & Ma, X. Medical Image Classification Algorithm Based on Visual\nAttention Mechanism-MCNN. Oxidative Medicine and Cellular Longevity 2021,\nhttps://www.embase.com/search/results?subaction=viewrecord&id=\nL2011217895&from=exporthttps://doi.org/10.1155/2021/6280690 (2021).\n\n60. He, S. et al. Multi-channel attention-fusion neural network for brain age esti-\nmation: Accuracy, generality, and interpretation with 16,705 healthy MRIs across\nlifespan. Med. Image Anal. 72, https://www.embase.com/search/results?\nsubaction=viewrecord&id=L2012117928&from=exporthttps://doi.org/10.1016/\nj.media.2021.102091 (2021).\n\n61. Hou, B., Kang, G., Xu, X. & Hu, C. Cross Attention Densely Connected Networks\nfor Multiple Sclerosis Lesion Segmentation. 2079 IEEE International Conference\non Bioinformatics and Biomedicine, BIBM 2019, November 18, 2019 - November 21,\n2019 2356-2361, https://doi.org/10.1109/BIBM47256.2019.8983149NS (2019).\n\n62. Huang, Y. & Chung, A. C. S. Evidence localization for pathology images using\nweakly supervised learning. 22nd International Conference on Medical Image\nComputing and Computer-Assisted Intervention, MICCAI 2019, October 13, 2019 -\nOctober 17, 2019 11764 LNCS, 613-621 (2019).\n\n63. Morvan, L. et al. Learned Deep Radiomics for Survival Analysis with Attention.\n3rd International Workshop on Predictive Intelligence in Medicine, PRIME 2020, held\nin conjunction with the Medical Image Computing and Computer Assisted Inter-\nvention, MICCAI 2020, October 8, 2020 - October 8, 2020 12329 LNCS, 35-45\n(2020).\n\n64. Saleem, H., Shahid, A. R. & Raza, B. Visual interpretability in 3D brain tumor\nsegmentation network. Comput. Biology Med. 133, https://www.embase.com/\nsearch/results?subaction=viewrecord&id=L2011734982&\nfrom=exporthttps://doi.org/10.1016/j.compbiomed.2021.104410 (2021).\n\n65. Shahamat, H. & Saniee Abadeh, M. Brain MRI analysis using a deep learning\nbased evolutionary approach. Neural Netw. 126, 218-234 (2020).\n\n66. Singla, S. et al. Subject2Vec: generative-discriminative approach from a set of\nimage patches to a vector. Med. Image Comput. Comput. Assist Interv. 11070,\n502-510 (2018).\n\n67. Sun, J., Darbehani, F., Zaidi, M. & Wang, B. SAUNet: Shape Attentive U-Net for\nInterpretable Medical Image Segmentation. 23rd International Conference on\nMedical Image Computing and Computer-Assisted Intervention, MICCAI 2020,\nOctober 4, 2020 - October 8, 2020 12264 LNCS, 797-806 (2020).\n\n68. Xu, X. et al. Automatic glaucoma detection based on transfer induced attention\nnetwork. Biomed. Eng. Online 20, 39 (2021).\n\n69. Yang, H., Kim, J.-Y.,, Kim, H. & Adhikari, S. P. Guided soft attention network for\nclassification of breast cancer histopathology images. /EEE Trans. Med. Imaging\n39, 1306-1315 (2020).\n\n70. Diao, J. A. et al. Human-interpretable image features derived from densely\nmapped cancer pathology slides predict diverse molecular phenotypes.\nNat. Commun. 12, https://www.embase.com/search/results?subaction=\nviewrecord&id=L2010776995&from=exporthttps://doi.org/10.1038/s41467-\n021-21896-9 (2021).\n\n71. Dong, Y. et al. A Polarization-imaging-based machine learning framework for\nquantitative pathological diagnosis of cervical precancerous lesions. /EEE\nTrans. Med. Imaging. https://www.embase.com/search/results?subaction=viewr\necord&id =L6355383098&from=exporthttps://doi.org/10.1109/TMI.2021.3\n097200 (2021).\n\n72. Giannini, V., Rosati, S., Regge, D. & Balestra, G. Texture features and artificial\nneural networks: A way to improve the specificity of a CAD system for multi-\nparametric MR prostate cancer. 14th Mediterranean Conference on Medical and\n"
                },
                {
                    "idx": 2,
                    "thing": "list",
                    "score": 98.41,
                    "box": [
                        1272.5,
                        249.7,
                        2292.8,
                        3011.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_13/region_2_list.png",
                    "text": "Biological Engineering and Computing, MEDICON 2016, March 31, 2016 - April 2,\n2016 57, 296-301 (2016).\n\n73. Loveymi, S., Dezfoulian, M. H. & Mansoorizadeh, M. Generate structured radi-\nology report from CT images using image annotation techniques: preliminary\nresults with liver CT. J. Dig. Imaging 33, 375-390 (2020).\n\n74. MacCormick, |. J. C. et al. Accurate, fast, data efficient and interpretable glau-\ncoma diagnosis with automated spatial analysis of the whole cup to disc profile.\nPLoS ONE 14, https://Awww.embase.com/search/results?subaction=viewre\ncord&id=L625837308&from=exporthttps://doi.org/10.1371/journal.pone.\n0209409 (2019).\n\n75. Kunapuli, G. et al. A decision-support tool for renal mass classification. J. Digit.\nImaging 31, 929-939 (2018).\n\n76. Shen, T., Wang, J., Gou, C. & Wang, F.-Y. Hierarchical fused model with deep\nlearning and type-2 fuzzy learning for breast cancer diagnosis. /EEE Trans. Fuzzy\nSyst. 28, 3204-3218 (2020).\n\n77. Li, J., Shi, H. & Hwang, K.-S. An explainable ensemble feedforward method with\nGaussian convolutional filter. Knowl.-Based Syst. 225, https://doi.org/10.1016/\nj-knosys.2021.107103NS (2021).\n\n78. Puyol-Anton, E. et al. Assessing the impact of blood pressure on cardiac function\nusing interpretable biomarkers and variational autoencoders. 70th International\nWorkshop on Statistical Atlases and Computational Models of the Heart, STACOM\n2019, held in conjunction with the 22nd International Conference on Medical\nImage Computing and Computer Assisted Intervention, MICCAI 2019, October 13,\n2019 12009 LNCS, 22-30 (2020).\n\n79. Wongvibulsin, S., Wu, K. C. & Zeger, S. L. Improving clinical translation of\nmachine learning approaches through clinician-tailored visual displays of black\nbox algorithms: development and validation. JMIR Med. Inform. 8, e15791\n(2020).\n\n80. Lin, Y., Wei, L., Han, S. X., Aberle, D. R. & Hsu, W. EDICNet: An end-to-end\ndetection and interpretable malignancy classification network for pulmonary\nnodules in computed tomography. Medical Imaging 2020: Computer-Aided\nDiagnosis, February 16, 2020 - February 19, 2020 11314, The Society of\nPhoto-Optical Instrumentation Engin. https://doi.org/10.1117/12.2551220NS\n(2020).\n\n81. Kim, S. T., Lee, H., Kim, H. G. & Ro, Y. M. ICADx: Interpretable computer aided\ndiagnosis of breast masses. Medical Imaging 2018: Computer-Aided Diagnosis,\nFebruary 12, 2018 - February 15, 2018 10575, DECTRIS Ltd.; The Society of\nPhoto-Optical Instrum. https://doi.org/10.1117/12.2293570NS (2018).\n\n82. Kim, S. T., Lee, J.-H., Lee, H. & Ro, Y. M. Visually interpretable deep network for\ndiagnosis of breast masses on mammograms. Phys. Med. Biology 63, 235025\n(2018).\n\n83. Puyol-Anton, E. et al. Interpretable deep models for cardiac resynchronisation\ntherapy response prediction. Med. Image Comput. Comput. Assist Interv 2020,\n284-293 (2020).\n\n84. Wang, C. J. et al. Deep learning for liver tumor diagnosis part Il: convolutional\nneural network interpretation using radiologic imaging features. Eur. Radiol. 29,\n3348-3357 (2019).\n\n85. Codella, N. C. F. et al. Collaborative human-Al (CHAI): Evidence-based inter-\npretable melanoma classification in dermoscopic images. 7st International\nWorkshop on Machine Learning in Clinical Neuroimaging, MLCN 2018, 1st Inter-\nnational Workshop on Deep Learning Fails, DLF 2018, and 1st International\nWorkshop on Interpretability of Machine Intelligence in Medical Image Computing,\niMIMIC 11038 LNCS, 97-105 (2018).\n\n86. Barata, C., Celebi, M. E. & Marques, J. S. Explainable skin lesion diagnosis\nusing taxonomies. Pattern Recognition 110, https://doi.org/10.1016/\nj.patcog.2020.107413NS (2021).\n\n87. Silva, W., Fernandes, K., Cardoso, M. J. & Cardoso, J. S. Towards complementary\nexplanations using deep neural networks. Ist International Workshop on\nMachine Learning in Clinical Neuroimaging, MLCN 2018, 1st International Work-\nshop on Deep Learning Fails, DLF 2018, and 1st International Workshop on\nInterpretability of Machine Intelligence in Medical Image Computing, iMIMIC\n11038 LNCS, 133-140 (2018).\n\n88. Khaleel, M., Tavanapong, W., Wong, J., Oh, J. & De Groen, P. Hierarchical visual\nconcept interpretation for medical image classification. 34th IEEE International\nSymposium on Computer-Based Medical Systems, CBMS 2021, June 7, 2021 - June\n9, 2021 2021-June, 25-30 (2021).\n\n89. Pereira, S. et al. Enhancing interpretability of automatically extracted machine\nlearning features: application to a RBM-Random Forest system on brain lesion\nsegmentation. Med. Image Anal. 44, 228-244 (2018).\n\n90. Yan, K. et al. Holistic and comprehensive annotation of clinically significant\nfindings on diverse CT images: Learning from radiology reports and label\nontology. 32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition,\nCVPR 2019, June 16, 2019 - June 20, 2019 2019-June, 8515-8524, https://doi.org/\n10.1109/CVPR.2019.00872NS (2019).\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_14.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_14.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 5,
                "page_idx": 14
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "list",
                    "score": 97.62,
                    "box": [
                        192.8,
                        246.8,
                        1222.7,
                        3048.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_14/region_1_list.png",
                    "text": "91. Chen, H., Miao, S., Xu, D., Hager, G. D. & Harrison, A. P. Deep hiearchical multi-\nlabel classification applied to chest x-ray abnormality taxonomies. Med. Image\nAnal. 66, 101811 (2020).\n\n92. Verma, A., Shukla, P. & Verma, S. An interpretable SVM based model for cancer\nprediction in mammograms. 1st International Conference on Communication,\nNetworks and Computing, CNC 2018, March 22, 2018 - March 24, 2018 839,\n443-451 (2019).\n\n93. Li, Y. et al. Computer-aided cervical cancer diagnosis using time-lapsed colpo-\nscopic images. /EEE Trans. Med. Imaging 39, 3403-3415 (2020).\n\n94. Wang, K. et al. A dual-mode deep transfer learning (D2TL) system for breast\ncancer detection using contrast enhanced digital mammograms. //SE Trans.\nHealthcare Syst. Eng. 9, 357-370 (2019).\n\n95. Zhao, G., Zhou, B., Wang, K., Jiang, R. & Xu, M. Respond-CAM: Analyzing deep\nmodels for 3D imaging data by visualizations. 21st International Conference on\nMedical Image Computing and Computer Assisted Intervention, MICCAI 2018,\nSeptember 16, 2018 - September 20, 2018 11070 LNCS, 485-492 (2018).\n\n96. Folke, T., Yang, S. C.-H., Anderson, S. & Shafto, P. Explainable Al for medical\nimaging: Explaining pneumothorax diagnoses with Bayesian teaching. Artificial\nIntelligence and Machine Learning for Multi-Domain Operations Applications III\n2021, April 12, 2021 - April 16, 2021 11746, The Society of Photo-Optical\nInstrumentation Engin. https://doi.org/10.1117/12.2585967NS (2021).\n\n97. Liao, W. et al. Clinical interpretable deep learning model for glaucoma diagnosis.\nIEEE J. Biomed. Health Inf. 24, 1405-1412 (2020).\n\n98. Shinde, S., Chougule, T., Saini, J. & Ingalhalikar, M. HR-CAM: Precise localization\nof pathology using multi-level learning in CNNS. 22nd International Conference\non Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019,\nOctober 13, 2019 - October 17, 2019 11767 LNCS, 298-306 (2019).\n\n99. Ballard, D. H. Modular learning in neural networks. AAA! 647, 279-284 (1987).\n\n100. Biffi, C. et al. Learning interpretable anatomical features through deep gen-\nerative models: Application to cardiac remodeling. 27st International Conference\non Medical Image Computing and Computer Assisted Intervention, MICCAI 2018,\nSeptember 16, 2018 - September 20, 2018 11071 LNCS, 464-471 (2018).\n\n101. Couteaux, V., Nempont, O., Pizaine, G. & Bloch, |. Towards interpretability of\nsegmentation networks by analyzing deepDreams. 2nd International Workshop\non Interpretability of Machine Intelligence in Medical Image Computing, iMIMIC\n2019, and the 9th International Workshop on Multimodal Learning for Clinical\nDecision Support, ML-CDS 2019, held in conjunction with the 22nd Interna 11797\nLNCS, 56-63 (2019).\n\n102. Guo, X. et al. Intelligent medical image grouping through interactive learning.\nInt. J. Data Sci. Anal. 2, 95-105 (2016).\n\n103. Janik, A., Dodd, J., Ifrim, G., Sankaran, K. & Curran, K. Interpretability of a deep\nlearning model in the application of cardiac MRI segmentation with an ACDC\nchallenge dataset. Medical Imaging 2021: Image Processing, February 15, 2021 -\nFebruary 19, 2021 11596, The Society of Photo-Optical Instrumentation Engin.\nhttps://doi.org/10.1117/12.2582227NS (2021).\n\n104. Sari, C. T. & Gunduz-Demir, C. Unsupervised feature extraction via deep learning\nfor histopathological classification of colon tissue images. [EEE Trans. Med.\nImaging 38, 1139-1149 (2019).\n\n105. Venugopalan, J., Tong, L., Hassanzadeh, H. R. & Wang, M. D. Multimodal deep\nlearning models for early detection of Alzheimer’s disease stage. Sci. Rep. 11,\n3254 (2021).\n\n106. Zhu, P. & Ogino, M. Guideline-based additive explanation for computer-aided\ndiagnosis of lung nodules. 2nd International Workshop on Interpretability of\nMachine Intelligence in Medical Image Computing, iMIMIC 2019, and the 9th\nInternational Workshop on Multimodal Learning for Clinical Decision Support, ML-\nCDS 2019, held in conjunction with the 22nd Interna 11797 LNCS, 39-47 (2019).\n\n107. Pirovano, A., Heuberger, H., Berlemont, S., Ladjal, S. & Bloch, |. Improving\ninterpretability for computer-aided diagnosis tools on whole slide imaging with\nmultiple instance learning and gradient-based explanations. 3rd International\nWorkshop on Interpretability of Machine Intelligence in Medical Image Computing,\niMIMIC 2020, the 2nd International Workshop on Medical Image Learning with Less\nLabels and Imperfect Data, MIL3ID 2020, and the 5th International Workshop o\n12446 LNCS, 43-53 (2020).\n\n108. Hao, J., Kosaraju, S. C., Tsaku, N. Z., Song, D. H. & Kang, M. PAGE-Net: inter-\npretable and integrative deep learning for survival analysis using histopatho-\nlogical images and genomic data. Pacific Symposium on Biocomputing. Pacific\nSymposium on Biocomputing 25, 355-366 (2020).\n\n109. de Sousa, |. P., Vellasco, M. M. B. R. & da Silva, E. C. Approximate explanations for\nclassification of histopathology patches. Workshops of the 20th Joint European\nConference on Machine Learning and Knowledge Discovery in Databases, ECML\nPKDD, September 14, 2020 - September 18, 2020 1323, 517-526 (2020).\n\n110. Li, X., Dvornek, N. C., Zhuang, J., Ventola, P. & Duncan, J. S. Brain biomarker\ninterpretation in ASD using deep learning and fMRI. 27st International Con-\nference on Medical Image Computing and Computer Assisted Intervention, MICCAI\n2018, September 16, 2018 - September 20, 2018 11072 LNCS, 206-214 (2018).\n"
                },
                {
                    "idx": 2,
                    "thing": "list",
                    "score": 98.97,
                    "box": [
                        1303.6,
                        252.3,
                        2331.6,
                        2820.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_14/region_2_list.png",
                    "text": "111. Quellec, G. et al. ExplAln: Explanatory artificial intelligence for diabetic retino-\npathy diagnosis. Med. Image Anal. 72, https://www.embase.com/search/results?\nsubaction=viewrecord&id=L2012995582&from=exporthttps://doi.org/10.1016/\nj.media.2021.102118 (2021).\n\n112. Uzunova, H., Ehrhardt, J., Kepp, T. & Handels, H. Interpretable explanations of\nblack box classifiers applied on medical images by meaningful perturbations\nusing variational autoencoders. Medical Imaging 2019: Image Processing, Feb-\nruary 19, 2019 - February 21, 2019 10949, The Society of Photo-Optical Instru-\nmentation Engin. https://doi.org/10.1117/12.2511964NS (2019).\n\n113. Liu, J. et al. Ultrasound liver fibrosis diagnosis using multi-indicator guided deep\nneural networks. 70th International Workshop on Machine Learning in Medical\nImaging, MLMI 2019 held in conjunction with the 22nd International Conference\non Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019,\nOctober 13, 2019 - October 13, 2019 11861 LNCS, 230-237 (2019).\n\n114. Liu, Y. et al. Act like a radiologist: towards reliable multi-view correspondence\nreasoning for mammogram mass detection. /EEE Trans. Pattern Anal. Mach. Intell.\nhttps://doi.org/10.1109/TPAMI.2021.3085783NS (2021).\n\n115. Oktay, O. et al. Anatomically constrained neural networks (ACNNs): application\nto cardiac image enhancement and segmentation. /EEE Trans. Med. Imaging 37,\n384-395 (2018).\n\n116. Peng, T., Boxberg, M., Weichert, W., Navab, N. & Marr, C. Multi-task learning of a\ndeep K-nearest neighbour network for histopathological image classification\nand retrieval. 22nd International Conference on Medical Image Computing and\nComputer-Assisted Intervention, MICCAI 2019, October 13, 2019 - October 17, 2019\n11764 LNCS, 676-684 (2019).\n\n117. Liu, Y., Li, Z, Ge, Q, Lin, N. & Xiong, M. Deep Feature Selection and Causal\nAnalysis of Alzheimer's Disease. Front. Neurosci. 13, https://www.embase.com/\nsearch/results?subaction=viewrecord&id=L629992085&\nfrom=exporthttps://doi.org/10.3389/fnins.2019.01198 (2019).\n\n118. Ren, H. et al. Interpretable pneumonia detection by combining deep learning\nand explainable models with multisource data. IEEE Access 9, 95872-95883\n(2021).\n\n119. Velikova, M., Lucas, P. J. F., Samulski, M. & Karssemeijer, N. On the interplay of\nmachine learning and background knowledge in image interpretation by\nBayesian networks. Artif. Intell. Med. 57, 73-86 (2013).\n\n120. Carneiro, G. Zorron Cheng Tao Pu, L., Singh, R. & Burt, A. Deep learning\nuncertainty and confidence calibration for the five-class polyp classification\nfrom colonoscopy. Med. Image Anal. 62, https://www.embase.com/search/\nresults?subaction=viewrecord&id=L2005 189093&from=exporthttps://doi.org/\n10.1016/j.media.2020.101653 (2020).\n\n121. Sabol, P. et al. Explainable classifier for improving the accountability in decision-\nmaking for colorectal cancer diagnosis from histopathological images. J.\nBiomed. Inf. 109, https://www.embase.com/search/results?subaction=\nviewrecord&id=L2007460563&from=exporthttps://doi.org/10.1016/j jbi.2020.\n103523 (2020).\n\n122. Tanno, R. et al. Uncertainty modelling in deep learning for safer neuroimage\nenhancement: Demonstration in diffusion MRI. Neurolmage 225, https://\nwww.embase.com/search/results?subaction=viewrecord&id=L2008373754&\nfrom=exporthttps://doi.org/10.1016/j.neuroimage.2020.117366 (2021).\n\n123. Doshi-Velez, F. & Kim, B. Towards a rigorous science of interpretable machine\nlearning. preprint athttps://arxiv.org/abs/1702.08608 (2017).\n\n124. Adebayo, J. et al. Sanity checks for saliency maps. In Bengio, S. et al. (eds.)\nAdvances in Neural Information Processing Systems, vol. 31, https://\nproceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-\nPaper.pdf (Curran Associates, Inc., 2018).\n\n125. Yeche, H., Harrison, J. & Berthier, T. UBS: A dimension-agnostic metric for con-\ncept vector interpretability applied to radiomics. 2nd International Workshop on\nInterpretability of Machine Intelligence in Medical Image Computing, iMIMIC 2019,\nand the 9th International Workshop on Multimodal Learning for Clinical Decision\nSupport, ML-CDS 2019, held in conjunction with the 22nd Interna 11797 LNCS,\n12-20 (2019).\n\n126. Chen, H., Miao, S., Xu, D., Hager, G. D. & Harrison, A. P. Deep hierarchical multi-\nlabel classification of chest x-ray images. In International Conference on Medical\nImaging with Deep Learning, 109-120 (PMLR, 2019).\n\n127. Zhang, Z. et al. Origa-light: An online retinal fundus image database for glau-\ncoma analysis and research. In 2070 Annual International Conference of the IEEE\nEngineering in Medicine and Biology, 3065-3068 (IEEE, 2010).\n\n128. Menze, B. H. et al. The multimodal brain tumor image segmentation benchmark\n(brats). IEEE Trans. Med. Imaging 34, 1993-2024 (2014).\n"
                },
                {
                    "idx": 3,
                    "thing": "title",
                    "score": 97.83,
                    "box": [
                        1299.1,
                        2977.8,
                        1677.6,
                        3017.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_14/region_3_title.png",
                    "text": "ACKNOWLEDGEMENTS\n"
                },
                {
                    "idx": 4,
                    "thing": "text",
                    "score": 99.13,
                    "box": [
                        1297.3,
                        3030.7,
                        2205.6,
                        3062.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_14/region_4_text.png",
                    "text": "We acknowledge all authors for the contribution. No funding is included.\n"
                },
                {
                    "idx": 5,
                    "thing": "text",
                    "score": 9.31,
                    "box": [
                        1295.8,
                        3135.0,
                        2331.4,
                        3170.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_14/region_5_text.png",
                    "text": "Published in partnership with Seoul National University Bundang Hospital\n"
                }
            ]
        },
        {
            "page": {
                "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_15.png",
                "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_15.png",
                "image_width": 2481,
                "image_height": 3296,
                "regions_num": 11,
                "page_idx": 15
            },
            "regions": [
                {
                    "idx": 1,
                    "thing": "title",
                    "score": 96.91,
                    "box": [
                        151.4,
                        243.7,
                        583.8,
                        281.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_01_title.png",
                    "text": "AUTHOR CONTRIBUTIONS\n"
                },
                {
                    "idx": 2,
                    "thing": "text",
                    "score": 99.89,
                    "box": [
                        148.8,
                        295.8,
                        1182.8,
                        476.0
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_02_text.png",
                    "text": "All authors contributed to the conception and design of the study. H.C. and CG.\ncontributed to the literature search and data extraction. H.C. and C.G. contributed to\ndata analysis and interpretation. All authors contributed to writing the manuscript,\nand all authors approved the manuscript. All authors guaranteed the integrity of the\nwork. H.C. and C.G. contributed equally in this work and are co-first authors.\n"
                },
                {
                    "idx": 3,
                    "thing": "title",
                    "score": 96.55,
                    "box": [
                        150.0,
                        555.8,
                        544.0,
                        592.1
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_03_title.png",
                    "text": "COMPETING INTERESTS\n"
                },
                {
                    "idx": 4,
                    "thing": "text",
                    "score": 98.65,
                    "box": [
                        150.2,
                        607.9,
                        697.3,
                        637.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_04_text.png",
                    "text": "The authors declare no competing interests.\n"
                },
                {
                    "idx": 5,
                    "thing": "title",
                    "score": 96.5,
                    "box": [
                        151.1,
                        718.0,
                        616.9,
                        753.9
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_05_title.png",
                    "text": "ADDITIONAL INFORMATION\n"
                },
                {
                    "idx": 6,
                    "thing": "text",
                    "score": 91.31,
                    "box": [
                        148.8,
                        769.5,
                        1184.1,
                        836.4
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_06_text.png",
                    "text": "Supplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41746-022-00699-2.\n"
                },
                {
                    "idx": 7,
                    "thing": "text",
                    "score": 94.3,
                    "box": [
                        148.9,
                        880.8,
                        1184.8,
                        949.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_07_text.png",
                    "text": "Correspondence and requests for materials should be addressed to Mathias\nUnberath.\n"
                },
                {
                    "idx": 8,
                    "thing": "text",
                    "score": 88.73,
                    "box": [
                        149.1,
                        993.4,
                        1182.5,
                        1060.2
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_08_text.png",
                    "text": "Reprints and permission information is available at http://www.nature.com/\nreprints\n"
                },
                {
                    "idx": 9,
                    "thing": "text",
                    "score": 99.13,
                    "box": [
                        1258.5,
                        249.8,
                        2292.5,
                        317.8
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_09_text.png",
                    "text": "Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional affiliations.\n"
                },
                {
                    "idx": 10,
                    "thing": "text",
                    "score": 99.2,
                    "box": [
                        1258.0,
                        435.0,
                        2291.8,
                        836.7
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_10_text.png",
                    "text": "Open Access This article is licensed under a Creative Commons\n\n7 Attribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n"
                },
                {
                    "idx": 11,
                    "thing": "text",
                    "score": 23.74,
                    "box": [
                        1258.0,
                        918.6,
                        1522.7,
                        948.5
                    ],
                    "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_15/region_11_text.png",
                    "text": "© The Author(s) 2022\n"
                }
            ]
        }
    ]
}