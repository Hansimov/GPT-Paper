{
    "page": {
        "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_12.png",
        "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_12.png",
        "image_width": 2481,
        "image_height": 3296,
        "regions_num": 2,
        "page_idx": 12
    },
    "regions": [
        {
            "idx": 1,
            "thing": "list",
            "score": 97.56,
            "box": [
                204.9,
                249.0,
                1224.8,
                3050.2
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_12/region_1_list.png",
            "text": "2. Obermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. Dissecting racial bias in\nan algorithm used to manage the health of populations. Science 366, 447-453\n(2019).\n\n3. Ghassemi, M., Oakden-Rayner, L. & Beam, A. L. The false hope of current\napproaches to explainable artificial intelligence in health care. Lancet Digital\nHealth 3, e745-e750 (2021).\n\n4. McCoy, L. G., Brenna, C. T., Chen, S. S., Vold, K. & Das, S. Believing in black boxes:\nMachine learning for healthcare does not need explainability to be evidence-\nbased. J. Clin. Epidemiol. 142, 252-257 (2022).\n\n5. Vellido, A. The importance of interpretability and visualization in machine\nlearning for applications in medicine and health care. Neural Comput. Appl. 32,\n18069-18083 (2020).\n\n6. Char, D. S., Abramoff, M. D. & Feudtner, C. Identifying ethical considerations\nfor machine learning healthcare applications. Am. J. Bioethics 20, 7-17\n(2020).\n\n7. Holzinger, A., Langs, G. Denk, H., Zatloukal, K. & Muller, H. Causability and\nexplainability of artificial intelligence in medicine. Wiley Interdiscip. Rev.: Data\nMining Knowl. Discov. 9, eâ‚¬1312 (2019).\n\n8. Markus, A. F., Kors, J. A. & Rijnbeek, P. R. The role of explainability in creating\ntrustworthy artificial intelligence for health care: a comprehensive survey of the\nterminology, design choices, and evaluation strategies. J. Biomed. Inf. 113,\n103655 (2021).\n\n9. Salahuddin, Z., Woodruff, H. C., Chatterjee, A. & Lambin, P. Transparency of deep\nneural networks for medical image analysis: A review of interpretability meth-\nods. Comput. Biology Med. 140, 105111 (2022).\n\n10. Banegas-Luna, A. J. et al. Towards the interpretability of machine learning\npredictions for medical applications targeting personalised therapies: A cancer\ncase survey. Int. J. Mol. Sci. 22, 4394 (2021).\n\n11. Ploug, T. & Holm, S. The four dimensions of contestable ai diagnostics-a patient-\ncentric approach to explainable ai. Artif. Intell. Med. 107, 101901 (2020).\n\n12. Amann, J., Blasimme, A., Vayena, E., Frey, D. & Madai, V. |. Explainability for\nartificial intelligence in healthcare: a multidisciplinary perspective. BMC Med. Inf.\nDec. Making 20, 1-9 (2020).\n\n13. Norman, D. A. Affordance, conventions, and design. /nteractions 6, 38-43 (1999).\n\n14. Cai, C. J. et al. Human-centered tools for coping with imperfect algorithms\nduring medical decision-making. In Proceedings of the 2019 CHI Conference on\nHuman Factors in Computing Systems, 1-14 (2019).\n\n15. Xie, Y., Chen, M., Kao, D., Gao, G. & Chen, X. Chexplain: Enabling physicians to\nexplore and understand data-driven, Al-enabled medical imaging analysis. In\nProceedings of the 2020 CHI Conference on Human Factors in Computing Systems,\n1-13 (2020).\n\n16. Jacobs, M. et al. Designing Al for trust and collaboration in time-constrained\nmedical decisions: A sociotechnical lens. In Proceedings of the 2021 CHI Con-\nference on Human Factors in Computing Systems, 1-14 (2021).\n\n17. Suresh, H., Gomez, S. R., Nam, K. K. & Satyanarayan, A. Beyond expertise and\nroles: A framework to characterize the stakeholders of interpretable machine\nlearning and their needs. In Proceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems, 1-16 (2021).\n\n18. Lai, V. & Tan, C. On human predictions with explanations and predictions of\nmachine learning models: A case study on deception detection. In Proceed-\nings of the conference on fairness, accountability, and transparency, 29-38\n(2019).\n\n19. Eiband, M. et al. Bringing transparency design into practice. In 23rd international\nconference on intelligent user interfaces, 211-223 (2018).\n\n20. Wang, X. & Yin, M. Are explanations helpful? a comparative study of the effects\nof explanations in Al-assisted decision-making. In 26th International Conference\non Intelligent User Interfaces, 318-328 (2021).\n\n21. Cheng, H.-F. et al. Explaining decision-making algorithms through ui: Strategies\nto help non-expert stakeholders. In Proceedings of the 2019 chi conference on\nhuman factors in computing systems, 1-12 (2019).\n\n22. Smith-Renner, A. et al. No explainability without accountability: An empirical\nstudy of explanations and feedback in interactive ml. In Proceedings of the 2020\nCHI Conference on Human Factors in Computing Systems, 1-13 (2020).\n\n23. Bansal, G. et al. Does the whole exceed its parts? the effect of ai explanations on\ncomplementary team performance. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems, 1-16 (2021).\n\n24. Bansal, G. et al. Beyond accuracy: The role of mental models in human-Al team\nperformance. Proceedings of the AAAI Conference on Human Computation and\nCrowdsourcing 7, 2-11 (2019).\n\n25. Nourani, M. et al. Anchoring bias affects mental model formation and user\nreliance in explainable ai systems. In 26th International Conference on Intelligent\nUser Interfaces, 340-350 (2021).\n\n26. McCoy, L. G., Brenna, C. T., Chen, S., Vold, K. & Das, S. Believing in black boxes:\nMachine learning for healthcare does not need explainability to be evidence-\nbased. J. Clin. Epidemiol 142, 252-257 (2021).\n"
        },
        {
            "idx": 2,
            "thing": "list",
            "score": 98.03,
            "box": [
                1314.4,
                247.8,
                2331.0,
                3047.8
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_12/region_2_list.png",
            "text": "27. Deeley, M. et al. Segmentation editing improves efficiency while reducing inter-\nexpert variation and maintaining accuracy for normal brain tissues in the pre-\nsence of space-occupying lesions. Phys. Med. Biol. 58, 4071 (2013).\n\n28. Banerjee, |. et al. Reading race: Ai recognises patient's racial identity in medical\nimages. preprint at https://arxiv.org/abs/2107.10356 (2021).\n\n29. Liu, T. A. et al. Gene expression profile prediction in uveal melanoma using deep\nlearning: A pilot study for the development of an alternative survival prediction\ntool. Ophthalmol. Retina 4, 1213-1215 (2020).\n\n30. Lu, M. Y. et al. Deep learning-based computational pathology predicts origins\nfor cancers of unknown primary. preprint at https://arxiv.org/abs/2006.13932\n(2020).\n\n31. Cai, C. J., Winter, S., Steiner, D., Wilcox, L. & Terry, M. \"hello Al\": Uncovering the\nonboarding needs of medical practitioners for human-ai collaborative decision-\nmaking. Proce. ACM Human-Comput. Interaction 3, 1-24 (2019).\n\n32. Wang, D., Yang, Q., Abdul, A. & Lim, B. Y. Designing theory-driven user-centric\nexplainable Al. In Proceedings of the 2019 CHI conference on human factors in\ncomputing systems, 1-15 (2019).\n\n33. Nourani, M., King, J. & Ragan, E. The role of domain expertise in user trust and\nthe impact of first impressions with intelligent systems. In Proceedings of the\nAAAI Conference on Human Computation and Crowdsourcing, vol. 8, 112-121\n(2020).\n\n34. Bucinca, Z., Malaya, M. B. & Gajos, K. Z. To trust or to think: Cognitive forcing\nfunctions can reduce overreliance on ai in ai-assisted decision-making. Pro-\nceedings of the ACM on Human-Computer Interaction 5, 1-21 (2021).\n\n35. Gaube, S. et al. Do as ai say: susceptibility in deployment of clinical decision-aids.\nNPJ Digital Medicine 4, 1-8 (2021).\n\n36. Amershi, S. et al. Guidelines for human-Al interaction. In Proceedings of the 2019\nchi conference on human factors in computing systems, 1-13 (2019).\n\n37. Liao, Q. V., Gruen, D. & Miller, S. Questioning the Al: informing design practices\nfor explainable ai user experiences. In Proceedings of the 2020 CHI Conference on\nHuman Factors in Computing Systems, 1-15 (2020).\n\n38. Mohseni, S., Zarei, N. & Ragan, E. D. A multidisciplinary survey and framework for\ndesign and evaluation of explainable Al systems. ACM Trans. Interactive Intell.\nSyst. 11, 1-45 (2021).\n\n39. Chen, J., Kallus, N., Mao, X., Svacha, G. & Udell, M. Fairness under unawareness:\nAssessing disparity when protected class is unobserved. In Proceedings of the\nconference on fairness, accountability, and transparency, 339-348 (2019).\n\n40. Datta, A, Tschantz, M. C. & Datta, A. Automated experiments on ad privacy\nsettings: A tale of opacity, choice, and discrimination. preprint at https://\narxiv.org/abs/1408.6491 (2014).\n\n41. Leslie, D. Understanding artificial intelligence ethics and safety: A guide for the\nresponsible design and implementation of ai systems in the public sector.\nAvailable at SSRN 3403301 (2019).\n\n42. Sokol, K. & Flach, P. Explainability fact sheets: a framework for systematic\nassessment of explainable approaches. In Proceedings of the 2020 Conference on\nFairness, Accountability, and Transparency, 56-67 (2020).\n\n43. Liu, X., Rivera, S. C., Moher, D., Calvert, M. J. & Denniston, A. K. Reporting\nguidelines for clinical trial reports forinterventions involving artificial intelli-\ngence: the consort-Al extension. BMJ 370, m3164, https://doi.org/10.1136/\nbmj.m3164 (2020).\n\n44. DECIDE-Al Steering Group. DECIDE-Al: new reporting guidelines to bridge the\ndevelopment-to-implementation gap in clinical artificial intelligence. Nat. Med.\n27, 186-187 (2021).\n\n45. Cabitza, F. & Campagner, A. The need to separate the wheat from the chaff in\nmedical informatics: Introducing a comprehensive checklist for the (self)-\nassessment of medical ai studies (2021).\n\n46. Hernandez-Boussard, T., Bozkurt, S., loannidis, J. P. & Shah, N. H. Minimar\n(minimum information for medical ai reporting): developing reporting stan-\ndards for artificial intelligence in health care. J. Am. Med. Inf. Assoc. 27,\n2011-2015 (2020).\n\n47. Scott, |, Carter, S. & Coiera, E. Clinician checklist for assessing suitability of\nmachine learning applications in healthcare. BMJ Health & Care Informatics 28\n(2021).\n\n48. Tjoa, E. & Guan, C. A survey on explainable artificial intelligence (xai): Toward\nmedical xai. EEE Trans. Neural Netw. Learn. Syst. 32, 4793-4813 (2020).\n\n49. van der Velden, B. H., Kuijf, H. J., Gilhuijs, K. G. & Viergever, M. A. Explainable\nartificial intelligence (xai) in deep learning-based medical image analysis. Med.\nImage Anal 79, 102470 (2022).\n\n50. Gulum, M. A., Trombley, C. M. & Kantardzic, M. A review of explainable deep\nlearning cancer detection models in medical imaging. Appl. Sci. 11, 4573 (2021).\n\n51. Krizhevsky, A., Sutskever, |. & Hinton, G. E. Imagenet classification with deep\nconvolutional neural networks. Commun. ACM 60, 84-90 (2017).\n\n52. Moher, D., Liberati, A., Tetzlaff, J., Altman, D. G. & Group, P. Preferred reporting\nitems for systematic reviews and meta-analyses: the prisma statement. PLoS\nMed. 6, â‚¬1000097 (2009).\n"
        }
    ]
}