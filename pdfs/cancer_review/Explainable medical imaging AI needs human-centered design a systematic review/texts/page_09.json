{
    "page": {
        "original_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages/page_09.png",
        "current_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/pages_ordered/page_09.png",
        "image_width": 2481,
        "image_height": 3296,
        "regions_num": 14,
        "page_idx": 9
    },
    "regions": [
        {
            "idx": 1,
            "thing": "text",
            "score": 99.97,
            "box": [
                149.0,
                240.8,
                1185.0,
                787.9
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_01_text.png",
            "text": "enabled the generation of pixel-attribution methods” to visualize\npixel-level importance for a specific class of interest?°-©°. In\nsegmentation tasks, where clinically relevant abnormalities and\norgans are usually of small sizes, features from different resolution\nlevels were aggregated to compute attention and generate more\naccurate outcomes, as demonstrated in multiple applications, e.g.,\nmulti-class segmentation in fetal Magnetic Resonance Imagings\n(MRIs)°® and multiple sclerosis segmentation in MRIs°'. Clinical\nprior knowledge was also inserted into the attention mechanism\nto make the whole system more transparent. For instance,°° split\nbrain MRIs into 96 clinically important regions and used a genetic\nalgorithm to calculate the importance of each region to evaluate\nAlzheimer’s Disease (AD).\n"
        },
        {
            "idx": 2,
            "thing": "text",
            "score": 99.97,
            "box": [
                149.0,
                790.0,
                1184.0,
                1294.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_02_text.png",
            "text": "Human-understandable features, e.g., hand-crafted low-dimen-\nsional features or clinical variables (age, gender, etc.) were\nfrequently used to establish transparent systems. There existed\ntwo main ways to use human-understandable features in medical\nimaging: (1) Extracting hand-crafted features, e.g., morphological\nand radiomic features, from predicted segmentation masks\ngenerated by a non-transparent model’°-’? followed by analysis\nof those hand-crafted features using a separate classification\nmodule; (2) Directly predicting human-understandable features\ntogether with the main classification and detection task®°**. In\nthese approaches, all tasks usually shared the same network\narchitecture and parameter weights.\n"
        },
        {
            "idx": 3,
            "thing": "text",
            "score": 99.96,
            "box": [
                149.9,
                1296.3,
                1183.7,
                1714.5
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_03_text.png",
            "text": "Instead of explicitly extracting or predicting human-\nunderstandable features, other articles further analyzed deep\nencoded features with human-understandable techniques by\nfollowing clinical knowledge. Techniques such as decision trees\nwere constructed based on clinical taxonomy for hierarchical\nlearning’*®°-°°. Rule-based algorithms?' and regression meth-\nods®*? were used to promote transparency of the prediction?\ncreated a Graphical Convolution Network (GCN) based on clinical\nknowledge to model the correlations among colposcopic images\ncaptured around five key time slots during a visual examination.\n"
        },
        {
            "idx": 4,
            "thing": "text",
            "score": 99.94,
            "box": [
                150.0,
                1716.6,
                1183.3,
                1925.6
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_04_text.png",
            "text": "We also identified various other methods to create transparent\nsystems. These methods can be categorized as visualization-\nbased, feature-based, region importance-based, and architecture\nmodification-based methods. Each approach is discussed in\ndetail below.\n"
        },
        {
            "idx": 5,
            "thing": "text",
            "score": 99.96,
            "box": [
                149.8,
                1926.8,
                1183.8,
                2388.8
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_05_text.png",
            "text": "Visualization-based methods provide easy-to-understand illus-\ntrations by overlaying the original images with additional visual\nlayouts generated from transparency techniques. There existed\ntwo main visualization-based methods: (1) Visualizing pixel-\nattribution maps: These maps may be generated using gradient-\nbased importance analysis?*°, pixel-level predicted probability”®,\nor a combination of different levels of feature maps?”’”®. (2) Latent\nfeature evolution: Encoded features were evolved according to\nthe gradient ascent direction so that the decoded image (e.g.,\ngenerated with an auto-encoder technique®’) gradually change\nfrom one class to another!°°'°!,\n"
        },
        {
            "idx": 6,
            "thing": "text",
            "score": 99.97,
            "box": [
                150.0,
                2391.0,
                1184.1,
                2810.4
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_06_text.png",
            "text": "Feature-based methods directly analyze encoded features in an\nattempt to make the models transparent. Various feature-based\ntransparency method were proposed for transparent learn-\ning'°2-'% first encoded images to deep features and then\nclustered samples based on these deep features for prediction\nor image grouping tasks. Feature importance was also well-\nstudied to identify features that are most relevant for a specific\nclass by feature perturbation'°*'°° and gradients'©”'°8 identified\nand removed features with less importance for final prediction\nthrough feature ranking.\n"
        },
        {
            "idx": 7,
            "thing": "text",
            "score": 99.97,
            "box": [
                150.2,
                2812.1,
                1183.9,
                3063.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_07_text.png",
            "text": "As an alternative to measure feature contribution, input region\nimportance was also analyzed to reveal sub-region relevance to\neach prediction class. Image occlusion with blank  sub-\nregions'°?-''' and healthy-looking sub-regions''* was used to\nfind the most informative and relevant sub-regions for classifica-\ntion and detection tasks.\n"
        },
        {
            "idx": 8,
            "thing": "text",
            "score": 99.97,
            "box": [
                1259.3,
                240.8,
                2293.0,
                822.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_08_text.png",
            "text": "Other approaches modified the network architecture according\nto relevant clinical knowledge to make the whole system\ntransparent?’ pruned the architecture according to the degree\nof scale invariance at each layer in the network''? created ten\nbranches with shared weights for ten ultrasound images to mimic\nthe clinical workflow of liver fibrosis stage prediction''* aggre-\ngated information from all three views of mammograms and used\ntraditional methods to detect nipple and muscle direction, which\nwas followed by a grid alignment according to the nipple and\nmuscle direction for left and right breasts''? proposed to learn\nrepresentations of the underlying anatomy with a convolutional\nauto-encoder by mapping the predicted and ground truth\nsegmentation maps to a low dimensional representation to\nregularize the training objective of the segmentation network.\n"
        },
        {
            "idx": 9,
            "thing": "text",
            "score": 99.98,
            "box": [
                1258.3,
                822.6,
                2292.4,
                1153.6
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_09_text.png",
            "text": "Some other methods used the training image distribution to\nachieve transparency in classification''® used _ similar-looking\nimages (nearest training images in feature space) to classify\ntesting images with majority votes. Causal inference with plug-in\nclinical prior knowledge also introduced transparency directly to\nautomatic systems''”-''?. Confidence calibration and uncertainty\nestimation methods were also used to generate additional\nconfidence information for end users!7°-'?,\n"
        },
        {
            "idx": 10,
            "thing": "title",
            "score": 97.48,
            "box": [
                1258.7,
                1208.9,
                1418.3,
                1250.3
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_10_title.png",
            "text": "T: targets\n"
        },
        {
            "idx": 11,
            "thing": "text",
            "score": 99.98,
            "box": [
                1259.1,
                1257.6,
                2293.4,
                1797.0
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_11_text.png",
            "text": "A striking observation was that none of the selected articles aimed\nat building transparent systems for users other than care\nproviders. Less than half of the articles explicitly specified\nclinicians as the intended end users of the system (n = 30). From\nthe remaining 38 articles, 17 articles implied that the envisioned\nend users would be clinicians, while the remaining 21 did not\nspecify the envisioned target users. Articles that were more\nexplicit about their end users were more likely to rely on clinical\nprior knowledge (Level 2 evidence) in model design. In total, 47%\nof articles that specified or implied clinicians as end users\nimplemented clinical prior knowledge in the transparent systems\nwhile only 18% of articles without end user information use\nclinical prior knowledge.\n"
        },
        {
            "idx": 12,
            "thing": "title",
            "score": 98.25,
            "box": [
                1257.7,
                1851.3,
                1460.3,
                1892.5
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_12_title.png",
            "text": "R: reporting\n"
        },
        {
            "idx": 13,
            "thing": "text",
            "score": 99.98,
            "box": [
                1259.1,
                1900.2,
                2293.2,
                2729.6
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_13_text.png",
            "text": "Evaluating different properties of a transparent algorithm besides\ntask-related metrics, especially its performance in regards to\nachieving the desired human factors engineering goals, comple-\nments the assessment of the ML model's intended purpose. We\nidentified that the quality of the transparency component is\ncurrently being evaluated through four main approaches. The first\none involves metrics based on human perception, such as the\nmean opinion score introduced in ref. ''> to capture two expert\nparticipants’ rating of the model’s outcome quality and similarity\nto the ground truth on a 5-point scale. Using two study\nparticipants, pathologists’ feedback was also requested in ref. '°”\nto assess their agreement with patch-based visualizations that\ndisplay features relevant for normal and abnormal tissue. The level\nof agreement was not formally quantified, but reported as a\nqualitative description. Similarly, one study participant was\ninvolved in a qualitative assessment of explanations quality in\nref. ®'1°8. These evaluations are different from empirical user\nstudies as they are limited to a few individuals and were mostly\nused to subjectively confirm the correctness of the transparent\ncomponent.\n"
        },
        {
            "idx": 14,
            "thing": "text",
            "score": 99.98,
            "box": [
                1258.5,
                2732.7,
                2292.1,
                3063.5
            ],
            "crop_image_path": "/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/pdfs/cancer_review/Explainable medical imaging AI needs human-centered design a systematic review/crops_ordered/page_09/region_14_text.png",
            "text": "The second approach attempted to quantify the quality of\nexplanations for a specific purpose (functionally-grounded evalua-\ntion'?). For instance, some articles evaluated the localization ability\nof post-hoc explanations by defining an auxiliary task, such as\ndetection®”** or segmentation®”®°*''? of anatomical structures\nrelated to the main task. They then contrasted relevant regions\nidentified by the model with ground truth annotations. These\nquantitative measures (dice score, precision, recall) allowed for\n"
        }
    ]
}