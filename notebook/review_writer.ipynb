{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bd0166d-e107-4dc3-a95d-f72a5357e3d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Repo path:   [E:\\_codes\\GPT-Paper]\n",
      "Working dir: [E:\\_codes\\GPT-Paper\\notebook]\n",
      "Now: [2023-09-07 01:25:40.191874]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%run -i \"startup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4cf78b-92a0-4d92-9584-9b64ec8d9d3f",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "outline"
    ]
   },
   "source": [
    "Title: Unraveling the “black-box” of artificial intelligence-based pathological analysis of liver cancer\n",
    "\n",
    "# 1. Current advances of AI-based approaches for clinical management of liver cancer\n",
    "\n",
    "## 1.1 AI-based prognostication of liver cancer\n",
    "\n",
    "## 1.2 Molecular profiling of liver cancer via AI\n",
    "\n",
    "## 1.3 Exploring predictive indicators for therapy response\n",
    "\n",
    "# 2. Current challenges limiting AI-based approaches in the management of liver cancer\n",
    "\n",
    "(One paragraph highlighting the urgent need to explain the “black box” of deeplearning)\n",
    "\n",
    "# 3. Strategies for unraveling the “black-box” of AI-based pathological analysis of liver cancer\n",
    "\n",
    "## 3.1 Model-based explanation\n",
    "### 3.1.1 Support vector machine or random forests vs. deep learning\n",
    "### 3.1.2 Supervised learning vs. weakly supervised learning vs. unsupervised learning\n",
    "### 3.1.3 Textual explanation\n",
    "#### 3.1.3.1 Image captioning\n",
    "#### 3.1.3.2 Image captioning with visual explanation\n",
    "### 3.1.4 Example-based explanation \n",
    "#### 3.1.4.1 Triplet network\n",
    "#### 3.1.4.2 Prototypes\n",
    "\n",
    "## 3.2 Post hoc explanation\n",
    "\n",
    "### 3.2.1 Visual explanation (saliency mapping, pathologist-in-the-loop)\n",
    "#### 3.2.1.1 Backpropagation-based approaches\n",
    "      Including class activation mapping (CAM) and gradient-weighted class activation mapping (Grad-CAM)\n",
    "#### 3.2.1.2 Perturbation-based approaches\n",
    "      Including Occlusion sensitivity map (OSM), local interpretable model-agnostic explannations (LIME)\n",
    "#### 3.2.1.3 Multiple instance learning-based approaches\n",
    "\n",
    "### 3.2.2 Textual explanation\n",
    "   Testing with concept activation vectors (TCAV)\n",
    "### 3.2.3 Example-based explanation\n",
    "\n",
    "# 4. Conclusion and future applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769fe72e-848f-4766-b89d-1aa91183f615",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipynb_name = \"review_writer.ipynb\"\n",
    "markdown_cells = get_notebook_cells(notebook_path=ipynb_name, cell_types=[\"markdown\"])\n",
    "outline_cells = [\n",
    "    cell for cell in markdown_cells if \"outline\" in cell[\"metadata\"][\"tags\"]\n",
    "]\n",
    "outline_content = \"\".join([cell[\"source\"] for cell in outline_cells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "193d8a37-cb0c-4096-be66-55539ae48bf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens count: [709]\n",
      "[\n",
      "    {\n",
      "        \"idx\": 0,\n",
      "        \"level\": \"0\",\n",
      "        \"title\": \"Unraveling the “black-box” of artificial intelligence-based pathological analysis of liver cancer\",\n",
      "        \"intro\": \"A comprehensive exploration of the current advances, challenges, and strategies in applying AI for pathological analysis of liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 1,\n",
      "        \"level\": \"1\",\n",
      "        \"title\": \"Current advances of AI-based approaches for clinical management of liver cancer\",\n",
      "        \"intro\": \"An overview of the latest developments in AI technologies and their applications in the clinical management of liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 2,\n",
      "        \"level\": \"1.1\",\n",
      "        \"title\": \"AI-based prognostication of liver cancer\",\n",
      "        \"intro\": \"Discussion on how AI is being used to predict the progression and outcome of liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 3,\n",
      "        \"level\": \"1.2\",\n",
      "        \"title\": \"Molecular profiling of liver cancer via AI\",\n",
      "        \"intro\": \"Insights into how AI is leveraged for molecular profiling of liver cancer for personalized treatment strategies.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 4,\n",
      "        \"level\": \"1.3\",\n",
      "        \"title\": \"Exploring predictive indicators for therapy response\",\n",
      "        \"intro\": \"Exploration of AI's role in identifying predictive indicators for therapy response in liver cancer treatment.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 5,\n",
      "        \"level\": \"2\",\n",
      "        \"title\": \"Current challenges limiting AI-based approaches in the management of liver cancer\",\n",
      "        \"intro\": \"A detailed analysis of the existing challenges and limitations in the use of AI for liver cancer management.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 6,\n",
      "        \"level\": \"3\",\n",
      "        \"title\": \"Strategies for unraveling the “black-box” of AI-based pathological analysis of liver cancer\",\n",
      "        \"intro\": \"A comprehensive discussion on various strategies to decode the complex workings of AI in pathological analysis of liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 7,\n",
      "        \"level\": \"3.1\",\n",
      "        \"title\": \"Model-based explanation\",\n",
      "        \"intro\": \"An in-depth look into model-based explanations for AI's decision-making process in liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 8,\n",
      "        \"level\": \"3.1.1\",\n",
      "        \"title\": \"Support vector machine or random forests vs. deep learning\",\n",
      "        \"intro\": \"A comparative study of different AI models like support vector machines, random forests and deep learning for liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 9,\n",
      "        \"level\": \"3.1.2\",\n",
      "        \"title\": \"Supervised learning vs. weakly supervised learning vs. unsupervised learning\",\n",
      "        \"intro\": \"A thorough comparison of supervised, weakly supervised, and unsupervised learning approaches in AI for liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 10,\n",
      "        \"level\": \"3.1.3\",\n",
      "        \"title\": \"Textual explanation\",\n",
      "        \"intro\": \"Understanding how textual explanations can be generated from AI models for liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 11,\n",
      "        \"level\": \"3.1.3.1\",\n",
      "        \"title\": \"Image captioning\",\n",
      "        \"intro\": \"Insight into how AI can generate textual descriptions for pathological images in liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 12,\n",
      "        \"level\": \"3.1.3.2\",\n",
      "        \"title\": \"Image captioning with visual explanation\",\n",
      "        \"intro\": \"Discussion on how AI combines textual descriptions with visual explanations for a comprehensive understanding of liver cancer pathology.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 13,\n",
      "        \"level\": \"3.1.4\",\n",
      "        \"title\": \"Example-based explanation\",\n",
      "        \"intro\": \"Exploring how AI uses examples to provide explanations for its decisions in liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 14,\n",
      "        \"level\": \"3.1.4.1\",\n",
      "        \"title\": \"Triplet network\",\n",
      "        \"intro\": \"Understanding how the triplet network, an example-based explanation model, works in the context of AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 15,\n",
      "        \"level\": \"3.1.4.2\",\n",
      "        \"title\": \"Prototypes\",\n",
      "        \"intro\": \"Exploration of how prototypes are used as examples to explain AI decisions in liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 16,\n",
      "        \"level\": \"3.2\",\n",
      "        \"title\": \"Post hoc explanation\",\n",
      "        \"intro\": \"Discussion on post hoc explanations, which provide insights into AI decisions after they have been\n",
      "Response Tokens count: [1000] [stop]\n",
      "Continue ...\n",
      "Prompt Tokens count: [1717]\n",
      "[\n",
      "    ...\n",
      "    {\n",
      "        \"idx\": 16,\n",
      "        \"level\": \"3.2\",\n",
      "        \"title\": \"Post hoc explanation\",\n",
      "        \"intro\": \"Discussion on post hoc explanations, which provide insights into AI decisions after they have been made in the context of liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 17,\n",
      "        \"level\": \"3.2.1\",\n",
      "        \"title\": \"Visual explanation (saliency mapping, pathologist-in-the-loop)\",\n",
      "        \"intro\": \"Exploration of visual explanation techniques like saliency mapping and pathologist-in-the-loop in AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 18,\n",
      "        \"level\": \"3.2.1.1\",\n",
      "        \"title\": \"Backpropagation-based approaches\",\n",
      "        \"intro\": \"Insight into backpropagation-based approaches for visual explanation in AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 19,\n",
      "        \"level\": \"3.2.1.2\",\n",
      "        \"title\": \"Perturbation-based approaches\",\n",
      "        \"intro\": \"Understanding of perturbation-based approaches for providing visual explanations in AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 20,\n",
      "        \"level\": \"3.2.1.3\",\n",
      "        \"title\": \"Multiple instance learning-based approaches\",\n",
      "        \"intro\": \"Discussion on how multiple instance learning-based approaches contribute to visual explanations in AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 21,\n",
      "        \"level\": \"3.2.2\",\n",
      "        \"title\": \"Textual explanation\",\n",
      "        \"intro\": \"Insight into how textual explanations can be used in post hoc analysis of AI's decisions in liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 22,\n",
      "        \"level\": \"3.2.3\",\n",
      "        \"title\": \"Example-based explanation\",\n",
      "        \"intro\": \"Exploring how example-based explanations can be used in post hoc analysis of AI's decisions in liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 23,\n",
      "        \"level\": \"4\",\n",
      "        \"title\": \"Conclusion and future applications\",\n",
      "        \"intro\": \"Final thoughts on the current state of AI in liver cancer analysis and potential future developments in the field.\"\n",
      "    }\n",
      "]\n",
      "Response Tokens count: [474] [stop]\n"
     ]
    }
   ],
   "source": [
    "outline_filler.clear_history_messages()\n",
    "outline_filler_res = outline_filler.chat(outline_content, continous=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "942bbdb9-1bb2-496a-9881-bc542f4e7779",
   "metadata": {},
   "source": [
    "A typical workflow of generate a detailed review of a section title:\n",
    "- [Agent] outline_filler: Fill in the outline with intros, and structuralize it to JSON with idx and levels.\n",
    "- [Agent] Synonymer: Generate similar texts for each sub-section\n",
    "- [Program] Retrieve and Re-rank from local embbedings of docs\n",
    "- [Agent] Summarizer: Summarize the above content into a section\n",
    "- [Agent] Polisher: Polish the content of the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "725ecc57-3a03-4931-b56d-16297893b51f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idx': 0,\n",
       "  'level': '0',\n",
       "  'title': 'Unraveling the “black-box” of artificial intelligence-based pathological analysis of liver cancer',\n",
       "  'intro': 'A comprehensive exploration of the current advances, challenges, and strategies in applying AI for pathological analysis of liver cancer.'},\n",
       " {'idx': 1,\n",
       "  'level': '1',\n",
       "  'title': 'Current advances of AI-based approaches for clinical management of liver cancer',\n",
       "  'intro': 'An overview of the latest developments in AI technologies and their applications in the clinical management of liver cancer.'},\n",
       " {'idx': 2,\n",
       "  'level': '1.1',\n",
       "  'title': 'AI-based prognostication of liver cancer',\n",
       "  'intro': 'Discussion on how AI is being used to predict the progression and outcome of liver cancer.'},\n",
       " {'idx': 3,\n",
       "  'level': '1.2',\n",
       "  'title': 'Molecular profiling of liver cancer via AI',\n",
       "  'intro': 'Insights into how AI is leveraged for molecular profiling of liver cancer for personalized treatment strategies.'},\n",
       " {'idx': 4,\n",
       "  'level': '1.3',\n",
       "  'title': 'Exploring predictive indicators for therapy response',\n",
       "  'intro': \"Exploration of AI's role in identifying predictive indicators for therapy response in liver cancer treatment.\"},\n",
       " {'idx': 5,\n",
       "  'level': '2',\n",
       "  'title': 'Current challenges limiting AI-based approaches in the management of liver cancer',\n",
       "  'intro': 'A detailed analysis of the existing challenges and limitations in the use of AI for liver cancer management.'},\n",
       " {'idx': 6,\n",
       "  'level': '3',\n",
       "  'title': 'Strategies for unraveling the “black-box” of AI-based pathological analysis of liver cancer',\n",
       "  'intro': 'A comprehensive discussion on various strategies to decode the complex workings of AI in pathological analysis of liver cancer.'},\n",
       " {'idx': 7,\n",
       "  'level': '3.1',\n",
       "  'title': 'Model-based explanation',\n",
       "  'intro': \"An in-depth look into model-based explanations for AI's decision-making process in liver cancer analysis.\"},\n",
       " {'idx': 8,\n",
       "  'level': '3.1.1',\n",
       "  'title': 'Support vector machine or random forests vs. deep learning',\n",
       "  'intro': 'A comparative study of different AI models like support vector machines, random forests and deep learning for liver cancer analysis.'},\n",
       " {'idx': 9,\n",
       "  'level': '3.1.2',\n",
       "  'title': 'Supervised learning vs. weakly supervised learning vs. unsupervised learning',\n",
       "  'intro': 'A thorough comparison of supervised, weakly supervised, and unsupervised learning approaches in AI for liver cancer analysis.'},\n",
       " {'idx': 10,\n",
       "  'level': '3.1.3',\n",
       "  'title': 'Textual explanation',\n",
       "  'intro': 'Understanding how textual explanations can be generated from AI models for liver cancer analysis.'},\n",
       " {'idx': 11,\n",
       "  'level': '3.1.3.1',\n",
       "  'title': 'Image captioning',\n",
       "  'intro': 'Insight into how AI can generate textual descriptions for pathological images in liver cancer analysis.'},\n",
       " {'idx': 12,\n",
       "  'level': '3.1.3.2',\n",
       "  'title': 'Image captioning with visual explanation',\n",
       "  'intro': 'Discussion on how AI combines textual descriptions with visual explanations for a comprehensive understanding of liver cancer pathology.'},\n",
       " {'idx': 13,\n",
       "  'level': '3.1.4',\n",
       "  'title': 'Example-based explanation',\n",
       "  'intro': 'Exploring how AI uses examples to provide explanations for its decisions in liver cancer analysis.'},\n",
       " {'idx': 14,\n",
       "  'level': '3.1.4.1',\n",
       "  'title': 'Triplet network',\n",
       "  'intro': 'Understanding how the triplet network, an example-based explanation model, works in the context of AI-based liver cancer analysis.'},\n",
       " {'idx': 15,\n",
       "  'level': '3.1.4.2',\n",
       "  'title': 'Prototypes',\n",
       "  'intro': 'Exploration of how prototypes are used as examples to explain AI decisions in liver cancer analysis.'},\n",
       " {'idx': 16,\n",
       "  'level': '3.2',\n",
       "  'title': 'Post hoc explanation',\n",
       "  'intro': 'Discussion on post hoc explanations, which provide insights into AI decisions after they have been made in the context of liver cancer analysis.'},\n",
       " {'idx': 17,\n",
       "  'level': '3.2.1',\n",
       "  'title': 'Visual explanation (saliency mapping, pathologist-in-the-loop)',\n",
       "  'intro': 'Exploration of visual explanation techniques like saliency mapping and pathologist-in-the-loop in AI-based liver cancer analysis.'},\n",
       " {'idx': 18,\n",
       "  'level': '3.2.1.1',\n",
       "  'title': 'Backpropagation-based approaches',\n",
       "  'intro': 'Insight into backpropagation-based approaches for visual explanation in AI-based liver cancer analysis.'},\n",
       " {'idx': 19,\n",
       "  'level': '3.2.1.2',\n",
       "  'title': 'Perturbation-based approaches',\n",
       "  'intro': 'Understanding of perturbation-based approaches for providing visual explanations in AI-based liver cancer analysis.'},\n",
       " {'idx': 20,\n",
       "  'level': '3.2.1.3',\n",
       "  'title': 'Multiple instance learning-based approaches',\n",
       "  'intro': 'Discussion on how multiple instance learning-based approaches contribute to visual explanations in AI-based liver cancer analysis.'},\n",
       " {'idx': 21,\n",
       "  'level': '3.2.2',\n",
       "  'title': 'Textual explanation',\n",
       "  'intro': \"Insight into how textual explanations can be used in post hoc analysis of AI's decisions in liver cancer.\"},\n",
       " {'idx': 22,\n",
       "  'level': '3.2.3',\n",
       "  'title': 'Example-based explanation',\n",
       "  'intro': \"Exploring how example-based explanations can be used in post hoc analysis of AI's decisions in liver cancer.\"},\n",
       " {'idx': 23,\n",
       "  'level': '4',\n",
       "  'title': 'Conclusion and future applications',\n",
       "  'intro': 'Final thoughts on the current state of AI in liver cancer analysis and potential future developments in the field.'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outline_details_json_path = \"review_outline_details.json\"\n",
    "with open(outline_details_json_path, \"r\", encoding=\"utf-8\") as rf:\n",
    "    outline_details_data = json.load(rf)\n",
    "display(outline_details_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af25b87c-e0e6-4fb0-b2dd-2d61ec2e3dbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d522a7aa22648ce917ace548e767221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='Generate Details', icon='radiation', style=ButtonStyle(), tooltip='Click an…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "section_widget = SectionWidget()\n",
    "section_widget.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167fc03e-58c6-4ba6-aa97-ae2cdd53a99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 queries after combined.\n"
     ]
    }
   ],
   "source": [
    "section_1 = (\n",
    "    \"Current advances of AI-based approaches for clinical management of liver cancer\"\n",
    ")\n",
    "section_1_intro = \"latest advancements in the use of AI for managing liver cancer.\"\n",
    "section_1_queries = retriever.query_multi([section_1, section_1_intro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db03713d-55bb-4a58-b7fc-4d57b7c6c57f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pdf_name': 'Artificial Intelligence in Hepatology Ready for the Primetime',\n",
       "  'regions': [{'text': \"Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence.\\nAI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes.\\nThe field of AI is in a very early phase with limited clinical use.\\nAI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research.\\nIn this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications.\\nDifferent AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions.\\nAI has helped to reduce human errors and improve treatment protocols.\\nFurther research and validation are required for future use of AI in hepatology.\\n(J Ciin Exp Hepator 2023;13:149-161)\",\n",
       "    'page_idx': 1,\n",
       "    'region_idx': 4},\n",
       "   {'text': \"n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress.\\nThe use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed' (Table 1).\\nWith the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated.\\nAI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD)\",\n",
       "    'page_idx': 1,\n",
       "    'region_idx': 5},\n",
       "   {'text': 'AI is an upcoming promising technology that is rapidly becoming an essential part of patient management.\\nApplications of AI have expanded in all branches of medicines, especially endoscopy and hepatology.\\nThe conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes.” In this review the majority of studies mentioned focussed on diagnosis part.\\nThere are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed.\\nAI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients.\\nWith the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available.\\nHowever, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research.\\nIt is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging.\\nHence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine.',\n",
       "    'page_idx': 11,\n",
       "    'region_idx': 6}]},\n",
       " {'pdf_name': 'Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction',\n",
       "  'regions': [{'text': 'To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion.\\nTwo categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig.\\n2A).\\nRadiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°?\\nBecause both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches.\\nHerein, we review Al tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction.',\n",
       "    'page_idx': 5,\n",
       "    'region_idx': 2}]},\n",
       " {'pdf_name': 'Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation',\n",
       "  'regions': [{'text': 'Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18].\\nWith the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21].\\nFor HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23].\\nWith largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AJ algorithms in transplant patients suffering from HCC deserve to be explored.\\nMoreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed.',\n",
       "    'page_idx': 3,\n",
       "    'region_idx': 7}]},\n",
       " {'pdf_name': 'Exploring pathological signatures for predicting the recurrence of early-stage hepatocellular carcinoma based on deep learning',\n",
       "  'regions': [{'text': 'The emergence of AI has reformed multiple aspects of cancer management.\\nThe combination of deep learning and',\n",
       "    'page_idx': 9,\n",
       "    'region_idx': 6}]},\n",
       " {'pdf_name': 'JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma',\n",
       "  'regions': [{'text': 'Hepatocellular carcinoma (HCC) currently represents the fifth most common malignancy and the thirdleading cause of cancer-related death worldwide, with incidence and mortality rates that are increasing.\\nRecently, artificial intelligence (AI) has emerged as a unique opportunity to improve the full spectrum of HCC clinical care, by improving HCC risk prediction, diagnosis, and prognostication.\\nAl approaches include computational search algorithms, machine learning (ML) and deep learning (DL) models.\\nML consists of a computer running repeated iterations of models, in order to progressively improve performance of a specific task, such as classifying an outcome.\\nDL models are a subtype of ML, based on neural network structures that are inspired by the neuroanatomy of the human brain.\\nA growing body of recent data now apply DL models to diverse data sources — including electronic health record data, imaging modalities, histopathology and molecular biomarkers - to improve the accuracy of HCC risk prediction, detection and prediction of treatment response.\\nDespite the promise of these early results, future research is still needed to standardise AI data, and to improve both the generalisability and interpretability of results.\\nIf such challenges can be overcome, Al has the potential to profoundly change the way in which care is provided to patients with or at risk of HCC.',\n",
       "    'page_idx': 1,\n",
       "    'region_idx': 8},\n",
       "   {'text': 'Owing to the broad heterogeneity in HCC risk factors and pathogenesis, established strategies for prediction and prognostication are still limited.\\nRecently, artificial intelligence (AI) has emerged as a unique opportunity to improve the full spectrum of HCC clinical care, by: i) improving the prediction of future HCC risk in patients with established liver disease; ii) improving the accuracy of HCC',\n",
       "    'page_idx': 1,\n",
       "    'region_idx': 12},\n",
       "   {'text': 'Another rapidly growing area of research is focused on improved characterisation of indeterminate liver lesions.\\nIn clinical practice, when an abdominal ultrasound shows a new liver lesion, a patient is typically referred for further imaging, with contrast-enhanced CT or MRI.\\nBased on the fulfilment of specific radiologic criteria, certain liver lesions may be considered as having pathognomonic features of HCC, and thus do not require liver biopsy for further histological confirmation.\\nHowever, liver nodules imaged by CT or MRI often demonstrate indeterminate features, for which current recommendations include either liver biopsy or close interval follow-up with serial imaging.”° This practice is sub-optimal, resulting in numerous imaging studies, patient stress, and the potential for delayed diagnoses of liver cancer.\\nFor this reason, a growing body of recent literature has explored AI approaches to improve risk stratification of indeterminate liver lesions, to facilitate earlier and more accurate detection of HCC.',\n",
       "    'page_idx': 4,\n",
       "    'region_idx': 4},\n",
       "   {'text': 'A growing body of research has applied AI approaches to improve HCC risk prediction, and to more accurately detect and risk stratify existing HCC tumours, based on EHR data, radiomics approaches, and molecular or histopathological biomarkers.',\n",
       "    'page_idx': 6,\n",
       "    'region_idx': 13},\n",
       "   {'text': \"Prospective studies are needed to fully demonstrate the potential of AI to improve the clinical care of patients with HCC.\\nIn other medical areas, several Al-based randomised clinical trials have already been conducted.\\nAs such, in endoscopy, numerous randomised clinical trials have evaluated the impact of computer-aided systems on physicians’ performance in diagnosing intestinal adenoma or indicating blind spots of colonoscopy.’*°° The need to incorporate these new developments prompted the research community to extend the widely used SPIRIT and CONSORT guidelines for the use of Al methods in 2020.°'°?\\nAccording to ClinicalTrials.gov (https:// clinicaltrials.gov/), there are currently 6 ongoing trials involving AI for the management of HCC.\\nA research group at the University of Hong Kong is comparing an algorithm designed to diagnose HCC from CT images against the standard diagnostic procedure that relies on the LI-RADS criteria (NCT04843176).°?\\nA multicentre study from France is prospectively developing an AI algorithm in a non-randomised clinical trial.\\nThe research group uses clinical, biological and ultrasound data to\",\n",
       "    'page_idx': 11,\n",
       "    'region_idx': 6}]},\n",
       " {'pdf_name': 'Quantitative analysis of artificial intelligence on liver cancer',\n",
       "  'regions': [{'text': 'Objective: To provide the current research progress, hotspots, and emerging trends for Al in liver cancer, we have compiled a relative comprehensive and quantitative report on the research of liver disease using artificial intelligence by employing bibliometrics in this study.',\n",
       "    'page_idx': 1,\n",
       "    'region_idx': 11},\n",
       "   {'text': 'With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6).\\nThrough self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7).\\nOriginal AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary.\\nAs early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8).\\nDuring this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11).\\nWith the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13).\\nIn particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19).',\n",
       "    'page_idx': 2,\n",
       "    'region_idx': 5},\n",
       "   {'text': 'As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published.\\nFor example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published.\\nHowever, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications.\\nMeanwhile, there is a lack of quantitative analysis of all',\n",
       "    'page_idx': 2,\n",
       "    'region_idx': 6},\n",
       "   {'text': 'Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest.\\nInformation was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords.',\n",
       "    'page_idx': 2,\n",
       "    'region_idx': 8},\n",
       "   {'text': 'The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end.\\nThe analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32).\\nAdditionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32).\\nIn particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field.',\n",
       "    'page_idx': 3,\n",
       "    'region_idx': 5},\n",
       "   {'text': 'A total of 1724 papers were collected from WoSCC database inception according to our data searching strategy, including 1547 original articles and 177 reviews (Figure 1).\\nResearch on AI in liver cancer started in 2003 and has increased every year (Figure 2).\\nResearch has advanced especially rapidly from 2017, accounting for almost 70% of all publications.\\nAs of the search date, all papers have been cited 27049 times, and the H-index and average citations per item are 67 and 15.69, respectively.\\nThe H-index (36) is a mixed index which could be used as a significant indicator of appraising both the number and level of academic output of a scientific researcher, country, journal, or institution.',\n",
       "    'page_idx': 3,\n",
       "    'region_idx': 11},\n",
       "   {'text': 'More than 2000 institutions have participated in research on AI in liver cancer, and the top 10 institutions with the highest contribution are shown in Table 2.\\nThe top three institutions were the League Of European Research Universities, Sun Yat Sen University, and Zhejiang University with a total of 109, 62, and 58 articles.\\nFigure 4',\n",
       "    'page_idx': 3,\n",
       "    'region_idx': 15},\n",
       "   {'text': 'FIGURE 2 Global trend of publications and citations on artificial intelligence research in liver cancer from 2003 to 2022.',\n",
       "    'page_idx': 4,\n",
       "    'region_idx': 2},\n",
       "   {'text': 'TABLE 2 Top 10 institutes with publications researching the use of artificial intelligence in liver cancer.',\n",
       "    'page_idx': 5,\n",
       "    'region_idx': 8},\n",
       "   {'text': 'TABLE 3.\\nThe 10 most productive authors of publications researching the use of artificial intelligence in liver cancer.',\n",
       "    'page_idx': 6,\n",
       "    'region_idx': 3},\n",
       "   {'text': 'In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords.\\nUltimately, 1724 articles focusing on AI in liver cancer were collected from the WoSCC database and analyzed.',\n",
       "    'page_idx': 7,\n",
       "    'region_idx': 5},\n",
       "   {'text': 'Research on AI in liver cancer mainly started in 2003 and entered a stage of rapid development in 2017.\\nChina is the most productive country with 35.33% of total publications; however, the USA ranked first according to the H-index, citations, and average citations per paper.\\nIt is notable that China, as a country with a high incidence of liver cancer, has a high number of studies on AI in liver cancer.\\nHowever, most studies in China have limited impact, which may need further improvement from topic selection and research implementation.\\nThe League Of European Research Universities is the most productive institution, followed by Sun Yat Sen University and Zhejiang University.\\nThis is consistent with the conclusion of the most productive country above.\\nWe also found that cooperation',\n",
       "    'page_idx': 7,\n",
       "    'region_idx': 6},\n",
       "   {'text': 'Regarding data type, studies of Al in liver cancer started from the simple data modeling of genetic or molecular data (9-11).\\nWith the development of medical imaging, research on medical imaging has been gradually increasing.\\nCT, ultrasound, and MRI are the top three most used data types.\\nFirst, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis.\\nMoreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months.\\nTherefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39).\\nSecond, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer.\\nFinally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult.\\nTherefore, it is used less often than CT.\\nHowever, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation.\\nThis suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research.\\nAt the same time, few studies used pathological, genetic, and other clinical data (42-44).\\nThe main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult.',\n",
       "    'page_idx': 8,\n",
       "    'region_idx': 4},\n",
       "   {'text': 'Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc.\\nReports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67).\\nBut the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen.',\n",
       "    'page_idx': 8,\n",
       "    'region_idx': 8},\n",
       "   {'text': 'Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a',\n",
       "    'page_idx': 9,\n",
       "    'region_idx': 5},\n",
       "   {'text': 'This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer.\\nThe results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world.\\nIn addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field.\\nHowever, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend.',\n",
       "    'page_idx': 10,\n",
       "    'region_idx': 2}]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_1_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584f796-4bb1-4814-af20-6ad5fff56b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section_1_sum_res, section_1_trans_res = summarize_and_translate_section(\n",
    "    section_1, section_1_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28249706-7a66-4c31-917b-80e6f1c620ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 queries after combined.\n"
     ]
    }
   ],
   "source": [
    "section_2 = (\n",
    "    \"Current challenges limiting AI-based approaches in the management of liver cancer\"\n",
    ")\n",
    "section_2_intro = \"the challenges and limitations of using AI in managing liver cancer\"\n",
    "section_2_queries = retriever.query_multi([section_2, section_2_intro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056ff0a8-e277-46c7-8e9c-eb20946d122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens count: [3968]\n",
      "# Topic: Current challenges limiting AI-based approaches in the management of liver cancer\n",
      "\n",
      "# Statement:\n",
      "Artificial Intelligence (AI) has shown significant promise in the management of liver cancer, however, several challenges still exist. One of the main issues is the need for structured data collection, sharing, and storage [12]. Another challenge is the high medical cost of genetic examination and the difficulty of realizing AI in multiomics research [8]. Additionally, the lack of research on AI-assisted decision-making for precision treatment methods adapted to specific patients is a limiting factor [7]. Furthermore, while AI has been beneficial in the diagnosis and management of liver diseases, further research and validation are required for future use [13]. Lastly, the adoption of coordinated research opportunities is essential to facilitate the development of many clinically useful tools, as working in isolation from AI and data scientists can hinder the growth of clinical medicine [5].\n",
      "\n",
      "# References:\n",
      "[1] 'JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma': Page 12, Region 4\n",
      "[2] 'Quantitative analysis of artificial intelligence on liver cancer': Page 8, Region 4\n",
      "[3] 'Quantitative analysis of artificial intelligence on liver cancer': Page 8, Region 8\n",
      "[4] 'Artificial Intelligence in Hepatology Ready for the Primetime': Page 1, Region 4\n",
      "[5] 'Artificial Intelligence in Hepatology Ready for the Primetime': Page 11, Region 2\n",
      "Response Tokens count: [305] [stop]\n",
      "Prompt Tokens count: [305]\n",
      "# 主题：限制肝癌管理中基于人工智能方法的当前挑战\n",
      "\n",
      "# 声明：\n",
      "人工智能（AI）在肝癌管理方面表现出显著的潜力，然而，仍然存在一些挑战。其中一个主要问题是需要进行结构化数据的收集、共享和存储[12]。另一个挑战是基因检查的高医疗成本和在多组学研究中实现AI的困难[8]。此外，缺乏针对特定患者适应的精准治疗方法的AI辅助决策的研究是一个限制因素[7]。此外，虽然AI在肝疾病的诊断和管理方面已经有益，但未来需要进一步的研究和验证[13]。最后，采用协同研究机会对于促进许多临床有用工具的发展至关重要，因为与AI和数据科学家孤立工作可能会阻碍临床医学的发展[5]。\n",
      "\n",
      "# 参考文献：\n",
      "[1] 《JOH 2022年肝细胞癌预防和临床管理的人工智能》：第12页，区域4\n",
      "[2] 《肝癌人工智能定量分析》：第8页，区域4\n",
      "[3] 《肝癌人工智能定量分析》：第8页，区域8\n",
      "[4] 《肝病学中的人工智能已经准备好迎接黄金时代》：第1页，区域4\n",
      "[5] 《肝病学中的人工智能已经准备好迎接黄金时代》：第11页，区域2\n",
      "Response Tokens count: [522] [stop]\n"
     ]
    }
   ],
   "source": [
    "section_2_sum_res, section_2_trans_res = summarize_and_translate_section(\n",
    "    section_2, section_2_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7e0c079-e5f4-414f-8f60-65dff0d7ed6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "<style>\n",
       "    .tooltip {\n",
       "        position: relative;\n",
       "        display: inline-block;\n",
       "    }\n",
       "    .tooltip .tooltiptext {\n",
       "        visibility: hidden;\n",
       "        white-space:pre;\n",
       "        background-color: rgba(100,100,0,0.8);\n",
       "        color: white;\n",
       "        text-align: center;\n",
       "        border-radius: 2px;\n",
       "        padding: 5px 0;\n",
       "        \n",
       "        position: absolute;\n",
       "        z-index: 400;\n",
       "    }\n",
       "    .tooltip:hover .tooltiptext {\n",
       "        visibility: visible;\n",
       "    }\n",
       "    .tooltip:hover {\n",
       "        background-color: rgba(0,100,100,0.8);\n",
       "    }\n",
       "</style>\n",
       "</head>\n",
       "<body>\n",
       "\n",
       "<p>This is a paragraph with numbered references. Here is reference number <span class=\"tooltip\">1<span class=\"tooltiptext\">This is the referred text for reference number 1.</span></span>\n",
       "\n",
       "<br />\n",
       "and here is reference number <span class=\"tooltip\">2<span class=\"tooltiptext\">This is the referred text for reference number 2.</span></span>.</p>\n",
       "\n",
       "</body>\n",
       "</html>\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    HTML(\n",
    "        \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    .tooltip {\n",
    "        position: relative;\n",
    "        display: inline-block;\n",
    "    }\n",
    "    .tooltip .tooltiptext {\n",
    "        visibility: hidden;\n",
    "        white-space:pre;\n",
    "        background-color: rgba(100,100,0,0.8);\n",
    "        color: white;\n",
    "        text-align: center;\n",
    "        border-radius: 2px;\n",
    "        padding: 5px 0;\n",
    "        \n",
    "        position: absolute;\n",
    "        z-index: 400;\n",
    "    }\n",
    "    .tooltip:hover .tooltiptext {\n",
    "        visibility: visible;\n",
    "    }\n",
    "    .tooltip:hover {\n",
    "        background-color: rgba(0,100,100,0.8);\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<p>This is a paragraph with numbered references. Here is reference number <span class=\"tooltip\">1<span class=\"tooltiptext\">This is the referred text for reference number 1.</span></span>\n",
    "\n",
    "<br />\n",
    "and here is reference number <span class=\"tooltip\">2<span class=\"tooltiptext\">This is the referred text for reference number 2.</span></span>.</p>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00a688651a634c88b24b28aab12707e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 1",
       "layout": "IPY_MODEL_ebc5d1044f66414f8202845009b9be00",
       "style": "IPY_MODEL_f26bc58d2f644d4686b0ae4ee94b4bc2"
      }
     },
     "02c4c01954334678949b708fdc734995": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 3",
       "layout": "IPY_MODEL_eb37c519c0474d188ee61045e0ba482c",
       "style": "IPY_MODEL_b3ed309c168e4fffabb529ea307a8a2a"
      }
     },
     "03472cf13a484b4a888586a77a0a1073": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "04a0b4b735d949d7939448d1accf9adf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "05137bca775948eeaede59d67b81d778": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TabModel",
      "state": {
       "children": [
        "IPY_MODEL_9faff9bddc2a4176883e2f21b912a3a9",
        "IPY_MODEL_6bd83709756d42178961b13b4c6996e3",
        "IPY_MODEL_26de0afb53604c1bb55f599d8298ca5a",
        "IPY_MODEL_81505f2b827c4e87ad2ef111125f8bc9"
       ],
       "layout": "IPY_MODEL_9c73f0cb0d804e1a8c7bfc394242530c",
       "selected_index": 0,
       "titles": [
        "Section 1",
        "Section 2",
        "Section 3",
        "Section 4"
       ]
      }
     },
     "0545a06de2954c80848c0a95a8cfd6e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "096696544e1e492b83abee2a03b6fa35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0ce17e0b1fd54f8fb072285f7a1626a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TabModel",
      "state": {
       "children": [
        "IPY_MODEL_f62c552c63bf40ba93a9b82ab492dac2",
        "IPY_MODEL_a686db340a9e40ca8b5f495803c95ea4",
        "IPY_MODEL_2bb558afab2b405bb4dc4f4802202f5c",
        "IPY_MODEL_c4ad83be5a77495bb00ff1283a378afe"
       ],
       "layout": "IPY_MODEL_ee56a073d02a420396edf634889e07a7",
       "selected_index": 0,
       "titles": [
        "Section 1",
        "Section 2",
        "Section 3",
        "Section 4"
       ]
      }
     },
     "105d588459934132a8d5e52a1a350d86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "15cb487afa5942d2b9c340d21e3fbf0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "15ed8f0dec454306a52d022654e49e34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Generate Details",
       "icon": "radiation",
       "layout": "IPY_MODEL_15cb487afa5942d2b9c340d21e3fbf0b",
       "style": "IPY_MODEL_65b4b6a48acc49b78f5915273fc5f210",
       "tooltip": "Click and generate details for this section"
      }
     },
     "1654ac424ca6466bab805c243d2f8f42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "166d7a9759d34637b402613e769f73e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1fc6e9de5fc04afebad663d08d57771c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 4",
       "layout": "IPY_MODEL_6f5d79b690564ae189ff15a9edcbb4ee",
       "style": "IPY_MODEL_f26de1f860b94112ac856dde5edf6bc4"
      }
     },
     "231db4cea8a1404bb242348f401775d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2424cf052a7347b0b8cde069d1fd326e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2471e17e41e84f7da24fffe733347784": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "layout": "IPY_MODEL_a031fa9b2eb749028779e0a4fdf82269"
      }
     },
     "26de0afb53604c1bb55f599d8298ca5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 3",
       "layout": "IPY_MODEL_d3ceed68083242dfb933b390eb3a73a3",
       "style": "IPY_MODEL_6475cc16230a4fa895da7c656c288fb3"
      }
     },
     "2a049a4d8e3e4a3cb84f4d3da597f3c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "2bb558afab2b405bb4dc4f4802202f5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 3",
       "layout": "IPY_MODEL_7378b48b0f9347ff877cec370b4460b3",
       "style": "IPY_MODEL_5132b610767b44f0b99772519e5aa251"
      }
     },
     "2c5580f9afb6462ca426106cf9dbb58e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Section 1",
       "layout": "IPY_MODEL_e2aa0eafef0348b8a4c4c23415aedc1f",
       "style": "IPY_MODEL_428c1180961b4bc886ab44f9f56b27dc"
      }
     },
     "2cf13c821e394974a7ccd400b79e1357": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2edf5907dead49a2911d76a2ead613ff": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_b5862c2549ef41c682bcb0aaeb9d443e"
      }
     },
     "2f4c300fd1ac4d80a8919b23e067bcf0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 1",
       "layout": "IPY_MODEL_5fec8a4790034e13bce619aa056d669a",
       "style": "IPY_MODEL_c42fa7a52c094857a6c5bb1242eb2ff6"
      }
     },
     "31202a7e6a73499683700a84b303814b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "31dc331067ce407186810339a3ee13ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 3",
       "layout": "IPY_MODEL_aea4d2cb6c594919a7395c7b3d3339e8",
       "style": "IPY_MODEL_83ca27ac027d4f0dad8bddf0b1853973"
      }
     },
     "367f631472db4603a7d0c24fc2379cdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3b7a2847b0e648d0b9332043751923c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e0b9911125d245c992be87fdfc0595d6",
        "IPY_MODEL_aa366ba732c54219a40454ef20a2d0ab",
        "IPY_MODEL_2edf5907dead49a2911d76a2ead613ff"
       ],
       "layout": "IPY_MODEL_cd2780eab7464783a3e13a4880410d78"
      }
     },
     "3e0a3afcc1894cfdbc2e305864933462": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41167cb24de44f1ea245389f442efebe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4243a1b825eb4574a9003ae2f3147d4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 4",
       "layout": "IPY_MODEL_cacfa1b956694346a58cd6ecb9da326b",
       "style": "IPY_MODEL_6e44ccf681ee46bf90873d53f8fff8ec"
      }
     },
     "428c1180961b4bc886ab44f9f56b27dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "44b30c68f1014b9188a0ef9ee37a4a18": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "49ba377912f046428e23432ef6e65363": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Generate Details",
       "icon": "radiation",
       "layout": "IPY_MODEL_a877a426b999420492070d256902669c",
       "style": "IPY_MODEL_b9fd0e06373546d4857ba8b3469d097a",
       "tooltip": "Click and generate details for this section"
      }
     },
     "4a0365c46a114a669f3b8d988a026d89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Section 2",
       "layout": "IPY_MODEL_a7446e80b4ca40b28a5c90e6aebc4bc2",
       "style": "IPY_MODEL_0545a06de2954c80848c0a95a8cfd6e2"
      }
     },
     "4bbcb54091da43a982e150164ed79cb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 4",
       "layout": "IPY_MODEL_b261c99f6248497397f2cd62a6a0d569",
       "style": "IPY_MODEL_dadfbdd70ebd43439688cc1f983ced64"
      }
     },
     "4c20d47609ec44528b2838cc6cc9c725": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4f2cc5627f794fd79ccfa601f1779e34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_49ba377912f046428e23432ef6e65363",
        "IPY_MODEL_0ce17e0b1fd54f8fb072285f7a1626a4"
       ],
       "layout": "IPY_MODEL_096696544e1e492b83abee2a03b6fa35"
      }
     },
     "5132b610767b44f0b99772519e5aa251": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "51a5637a06a1416a984c15e7becbc9d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "51afd3ae52f54a948bf026a40c9c47e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "55b37ed9e4894e36b9a48a6da332f528": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "56b7f4d9aa6a4a6eb033c3523d841d3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Generate Details",
       "icon": "radiation",
       "layout": "IPY_MODEL_166d7a9759d34637b402613e769f73e0",
       "style": "IPY_MODEL_51afd3ae52f54a948bf026a40c9c47e9",
       "tooltip": "Click and generate details for this section"
      }
     },
     "586179f18fd44571901d1645d859c8e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5d4f1fbf2393458b9d5e6158acafa6a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 1",
       "layout": "IPY_MODEL_f9a1744f27d94ee081611e3dea43526f",
       "style": "IPY_MODEL_04a0b4b735d949d7939448d1accf9adf"
      }
     },
     "5ef9a85b40a04bc89ff95609fe16ac73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5fec8a4790034e13bce619aa056d669a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "600a4faa40634701ad3b2e05b7837dce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "623d55fac869405eb41fe9bebaa1b060": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6475cc16230a4fa895da7c656c288fb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "649cbec77de74029996de046587fc1b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6511962922ec48baa21c388483a82636": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "65b4b6a48acc49b78f5915273fc5f210": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "664787fe10694a049980c5363a10e667": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 2",
       "layout": "IPY_MODEL_a52298789f844559b94d91a64c8fe651",
       "style": "IPY_MODEL_105d588459934132a8d5e52a1a350d86"
      }
     },
     "6a7a575bfddb4eeb8716bd89426061d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6b5f2eaa17d44d958c30452235e47757": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 2",
       "layout": "IPY_MODEL_649cbec77de74029996de046587fc1b0",
       "style": "IPY_MODEL_1654ac424ca6466bab805c243d2f8f42"
      }
     },
     "6bd83709756d42178961b13b4c6996e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 2",
       "layout": "IPY_MODEL_f5bd356641bb4b20ae6878b287fab770",
       "style": "IPY_MODEL_f1986e35fe1542adb407f926df237a62"
      }
     },
     "6d26fa943d6b4c62899274addf8769ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "6d63b8412eb441d4a7a7ceb39f2e7fb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6e2072decc3241a99745b20e192417c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 1",
       "layout": "IPY_MODEL_5ef9a85b40a04bc89ff95609fe16ac73",
       "style": "IPY_MODEL_6d63b8412eb441d4a7a7ceb39f2e7fb6"
      }
     },
     "6e44ccf681ee46bf90873d53f8fff8ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6f5d79b690564ae189ff15a9edcbb4ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7378b48b0f9347ff877cec370b4460b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "789ece937ddf48f595d4d9ce318713ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "78e7a2cf0c0e4a288528a4ccc75b5359": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Section 4",
       "layout": "IPY_MODEL_4c20d47609ec44528b2838cc6cc9c725",
       "style": "IPY_MODEL_8a865dd8c0c444e2b302b8bd27fa71b7"
      }
     },
     "78f009f5eaa6459db66411b242d524b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Generate Details",
       "icon": "radiation",
       "layout": "IPY_MODEL_b1b4df0610324766b546c8489cc5393b",
       "style": "IPY_MODEL_6d26fa943d6b4c62899274addf8769ee",
       "tooltip": "Click and generate details for this section"
      }
     },
     "7cb5a2023a69403085171ffb7dfa74ec": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_bcdd15826d364025ac9b7d63e134b390"
      }
     },
     "7d6ffbdcbc6a4c3cbd2bff263adc12d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TabModel",
      "state": {
       "children": [
        "IPY_MODEL_5d4f1fbf2393458b9d5e6158acafa6a2",
        "IPY_MODEL_b96aa13325a343c293b392c3a6f055a6",
        "IPY_MODEL_90fe712fb6c44ef28d4fc35a133cbb7a",
        "IPY_MODEL_bfbe92864a794e5099ad01ba45918539"
       ],
       "layout": "IPY_MODEL_31202a7e6a73499683700a84b303814b",
       "selected_index": 3,
       "titles": [
        "Section 1",
        "Section 2",
        "Section 3",
        "Section 4"
       ]
      }
     },
     "7e8bf379e66e48ae84be1bf7e5339a34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "81505f2b827c4e87ad2ef111125f8bc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 4",
       "layout": "IPY_MODEL_9a26deb23abf4390b1399f5314bf125c",
       "style": "IPY_MODEL_367f631472db4603a7d0c24fc2379cdc"
      }
     },
     "82b7e214e2e0401294d3e0a30f68a1e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 3",
       "layout": "IPY_MODEL_9aa113c1785043dfae2ee676b2759fed",
       "style": "IPY_MODEL_e395293e07194feb8e7c76fc6247d3aa"
      }
     },
     "83ca27ac027d4f0dad8bddf0b1853973": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "86f82ed7d0cd491399768154b56a7d71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 2",
       "layout": "IPY_MODEL_b164fe0534b44d349a45c51cfcd506b9",
       "style": "IPY_MODEL_41167cb24de44f1ea245389f442efebe"
      }
     },
     "8a865dd8c0c444e2b302b8bd27fa71b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8cadb62dd07c406197663a06e40d8b58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8d1418f9bc4f4f0cb2284cd4da97ef43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8d522a7aa22648ce917ace548e767221": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_78f009f5eaa6459db66411b242d524b2",
        "IPY_MODEL_a76840f75fa946a5badce317d33182f9",
        "IPY_MODEL_ab8f2c668c7d476d9a2220466ed55e51"
       ],
       "layout": "IPY_MODEL_586179f18fd44571901d1645d859c8e6"
      }
     },
     "90fe712fb6c44ef28d4fc35a133cbb7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 3",
       "layout": "IPY_MODEL_55b37ed9e4894e36b9a48a6da332f528",
       "style": "IPY_MODEL_cbb1f2e3c2d141aa963ca0b81203e5eb"
      }
     },
     "9a26deb23abf4390b1399f5314bf125c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9a6acad93fef4d9bb0a1cd4dec76b650": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9aa113c1785043dfae2ee676b2759fed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9c73f0cb0d804e1a8c7bfc394242530c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9df130e9c6ca4947ae70691b162d1f9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9e7fb27c5323430f980f8d21b902e345": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TabModel",
      "state": {
       "children": [
        "IPY_MODEL_2f4c300fd1ac4d80a8919b23e067bcf0",
        "IPY_MODEL_86f82ed7d0cd491399768154b56a7d71",
        "IPY_MODEL_82b7e214e2e0401294d3e0a30f68a1e9",
        "IPY_MODEL_d677ca2cb0184fcfbec272a0f221f578"
       ],
       "layout": "IPY_MODEL_7e8bf379e66e48ae84be1bf7e5339a34",
       "selected_index": 0,
       "titles": [
        "Section 1",
        "Section 2",
        "Section 3",
        "Section 4"
       ]
      }
     },
     "9faff9bddc2a4176883e2f21b912a3a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 1",
       "layout": "IPY_MODEL_9a6acad93fef4d9bb0a1cd4dec76b650",
       "style": "IPY_MODEL_fbfa91f279134f1e997159a62871547d"
      }
     },
     "a031fa9b2eb749028779e0a4fdf82269": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a52298789f844559b94d91a64c8fe651": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a5fbd171a989411e9ae5afd223d41f66": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a686db340a9e40ca8b5f495803c95ea4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 2",
       "layout": "IPY_MODEL_8cadb62dd07c406197663a06e40d8b58",
       "style": "IPY_MODEL_f1a4e75d04eb4e6ca80f30bb806e3e12"
      }
     },
     "a7446e80b4ca40b28a5c90e6aebc4bc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a76840f75fa946a5badce317d33182f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TabModel",
      "state": {
       "children": [
        "IPY_MODEL_00a688651a634c88b24b28aab12707e7",
        "IPY_MODEL_664787fe10694a049980c5363a10e667",
        "IPY_MODEL_ecdfa80a089b4d49be8c7cff0079afeb",
        "IPY_MODEL_4bbcb54091da43a982e150164ed79cb5"
       ],
       "layout": "IPY_MODEL_a5fbd171a989411e9ae5afd223d41f66",
       "selected_index": 2,
       "titles": [
        "Section 1",
        "Section 2",
        "Section 3",
        "Section 4"
       ]
      }
     },
     "a877a426b999420492070d256902669c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aa366ba732c54219a40454ef20a2d0ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TabModel",
      "state": {
       "children": [
        "IPY_MODEL_e78b39996a684975a35d351373903bf2",
        "IPY_MODEL_6b5f2eaa17d44d958c30452235e47757",
        "IPY_MODEL_31dc331067ce407186810339a3ee13ef",
        "IPY_MODEL_1fc6e9de5fc04afebad663d08d57771c"
       ],
       "layout": "IPY_MODEL_6511962922ec48baa21c388483a82636",
       "selected_index": 2,
       "titles": [
        "Section 1",
        "Section 2",
        "Section 3",
        "Section 4"
       ]
      }
     },
     "ab8f2c668c7d476d9a2220466ed55e51": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_03472cf13a484b4a888586a77a0a1073",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "Button clicked at 2023-09-07 01:26:04.788644\n"
        }
       ]
      }
     },
     "ae9f70a87eac4b63966ce59af293816e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aea4d2cb6c594919a7395c7b3d3339e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aec71e48c4d44817be2d504caf30e9e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "af4791ed3ea34134b957fa0b20c30e64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b164fe0534b44d349a45c51cfcd506b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b1b4df0610324766b546c8489cc5393b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b2196a0e229342e09d974c4582523d20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b261c99f6248497397f2cd62a6a0d569": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b3ed309c168e4fffabb529ea307a8a2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b57a9aed2e274503b93d2c4d510ded4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b5862c2549ef41c682bcb0aaeb9d443e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b96aa13325a343c293b392c3a6f055a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 2",
       "layout": "IPY_MODEL_eb7eaab83af8484693f12005f07219c4",
       "style": "IPY_MODEL_9df130e9c6ca4947ae70691b162d1f9e"
      }
     },
     "b9fd0e06373546d4857ba8b3469d097a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "bba3795ca0a141b18e415c9a4aaa8cd0": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_789ece937ddf48f595d4d9ce318713ce"
      }
     },
     "bcdd15826d364025ac9b7d63e134b390": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bdc5546679524ef5aba9d7b952c4c201": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bfbe92864a794e5099ad01ba45918539": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 4",
       "layout": "IPY_MODEL_bdc5546679524ef5aba9d7b952c4c201",
       "style": "IPY_MODEL_8d1418f9bc4f4f0cb2284cd4da97ef43"
      }
     },
     "c3c226bf228f41ecbebcd5a419b59381": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d889c8d550c14d449f47032aae8cc534",
        "IPY_MODEL_9e7fb27c5323430f980f8d21b902e345",
        "IPY_MODEL_7cb5a2023a69403085171ffb7dfa74ec"
       ],
       "layout": "IPY_MODEL_623d55fac869405eb41fe9bebaa1b060"
      }
     },
     "c404329354274ae49720c6e502e8c68e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c42fa7a52c094857a6c5bb1242eb2ff6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c444cff7418f4eb7b0b22594b8ac9e03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c4ad83be5a77495bb00ff1283a378afe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 4",
       "layout": "IPY_MODEL_c8515069c75c4686befffefa4c77ffdb",
       "style": "IPY_MODEL_c444cff7418f4eb7b0b22594b8ac9e03"
      }
     },
     "c4d89b3e966946cd8403c1f24eb75fdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c8515069c75c4686befffefa4c77ffdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cacfa1b956694346a58cd6ecb9da326b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cbb1f2e3c2d141aa963ca0b81203e5eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cc35d69f750546ceac072f1afe201e95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 2",
       "layout": "IPY_MODEL_3e0a3afcc1894cfdbc2e305864933462",
       "style": "IPY_MODEL_b57a9aed2e274503b93d2c4d510ded4f"
      }
     },
     "cd2780eab7464783a3e13a4880410d78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d3ceed68083242dfb933b390eb3a73a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d4533ff9a0f942bf815cd09075a1ed96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TabModel",
      "state": {
       "children": [
        "IPY_MODEL_6e2072decc3241a99745b20e192417c0",
        "IPY_MODEL_cc35d69f750546ceac072f1afe201e95",
        "IPY_MODEL_02c4c01954334678949b708fdc734995",
        "IPY_MODEL_4243a1b825eb4574a9003ae2f3147d4d"
       ],
       "layout": "IPY_MODEL_ae9f70a87eac4b63966ce59af293816e",
       "selected_index": 0,
       "titles": [
        "Section 1",
        "Section 2",
        "Section 3",
        "Section 4"
       ]
      }
     },
     "d677ca2cb0184fcfbec272a0f221f578": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 4",
       "layout": "IPY_MODEL_c404329354274ae49720c6e502e8c68e",
       "style": "IPY_MODEL_aec71e48c4d44817be2d504caf30e9e9"
      }
     },
     "d889c8d550c14d449f47032aae8cc534": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Generate Details",
       "icon": "radiation",
       "layout": "IPY_MODEL_ee64755b1634462db01c5fc6245b1f0c",
       "style": "IPY_MODEL_da9febd2354549069de53471f8d50c02",
       "tooltip": "Click and generate details for this section"
      }
     },
     "da9febd2354549069de53471f8d50c02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "dadfbdd70ebd43439688cc1f983ced64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "db9edefb128e42768cc92ed252da8015": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_15ed8f0dec454306a52d022654e49e34",
        "IPY_MODEL_05137bca775948eeaede59d67b81d778",
        "IPY_MODEL_bba3795ca0a141b18e415c9a4aaa8cd0"
       ],
       "layout": "IPY_MODEL_ebaa2ca736c74401a18e382f18b4a2e3"
      }
     },
     "e046514cdfbf47679bd4df883f7bd61d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TabModel",
      "state": {
       "children": [
        "IPY_MODEL_2c5580f9afb6462ca426106cf9dbb58e",
        "IPY_MODEL_4a0365c46a114a669f3b8d988a026d89",
        "IPY_MODEL_e9e8e99815d84fdbbb9360389c449890",
        "IPY_MODEL_78e7a2cf0c0e4a288528a4ccc75b5359"
       ],
       "layout": "IPY_MODEL_2cf13c821e394974a7ccd400b79e1357",
       "selected_index": 0,
       "titles": [
        "Section 1",
        "Section 2",
        "Section 3",
        "Section 4"
       ]
      }
     },
     "e0b9911125d245c992be87fdfc0595d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Generate Details",
       "icon": "radiation",
       "layout": "IPY_MODEL_6a7a575bfddb4eeb8716bd89426061d9",
       "style": "IPY_MODEL_2a049a4d8e3e4a3cb84f4d3da597f3c5",
       "tooltip": "Click and generate details for this section"
      }
     },
     "e2aa0eafef0348b8a4c4c23415aedc1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e395293e07194feb8e7c76fc6247d3aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e78b39996a684975a35d351373903bf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 1",
       "layout": "IPY_MODEL_b2196a0e229342e09d974c4582523d20",
       "style": "IPY_MODEL_51a5637a06a1416a984c15e7becbc9d5"
      }
     },
     "e9e8e99815d84fdbbb9360389c449890": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Section 3",
       "layout": "IPY_MODEL_231db4cea8a1404bb242348f401775d0",
       "style": "IPY_MODEL_c4d89b3e966946cd8403c1f24eb75fdd"
      }
     },
     "eb37c519c0474d188ee61045e0ba482c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eb7eaab83af8484693f12005f07219c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ebaa2ca736c74401a18e382f18b4a2e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ebc5d1044f66414f8202845009b9be00": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ecdfa80a089b4d49be8c7cff0079afeb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 3",
       "layout": "IPY_MODEL_44b30c68f1014b9188a0ef9ee37a4a18",
       "style": "IPY_MODEL_af4791ed3ea34134b957fa0b20c30e64"
      }
     },
     "ee56a073d02a420396edf634889e07a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ee64755b1634462db01c5fc6245b1f0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f1986e35fe1542adb407f926df237a62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f1a4e75d04eb4e6ca80f30bb806e3e12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f26bc58d2f644d4686b0ae4ee94b4bc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f26de1f860b94112ac856dde5edf6bc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f5bd356641bb4b20ae6878b287fab770": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f62c552c63bf40ba93a9b82ab492dac2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "description": "Content 1",
       "layout": "IPY_MODEL_600a4faa40634701ad3b2e05b7837dce",
       "style": "IPY_MODEL_2424cf052a7347b0b8cde069d1fd326e"
      }
     },
     "f9a1744f27d94ee081611e3dea43526f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fbfa91f279134f1e997159a62871547d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
