{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd0166d-e107-4dc3-a95d-f72a5357e3d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo path:   [/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper]\n",
      "Working dir: [/mnt/sh_flex_storage/home/zehanyu/repos/GPT-Paper/notebook]\n",
      "Now: [2023-09-12 22:13:27.905199]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262f866383af459aa05b16f7e9fa4cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='Retrieve All', icon='wikipedia-w', layout=Layout(width='auto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc07612f60bd4f53a6a7f07484659b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='0: Unraveling the “black-box” of artificial intelligence-based patho…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6d68ace59145ecb2fa3135e2d02675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='1: Current advances of AI-based approaches for clinical management o…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b966b89f113474481598638a0cdf95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='1.1: AI-based prognostication of liver cancer', layout=Layout(border…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ccd9a979ed4f79b2154a2ac2859163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='1.2: Molecular profiling of liver cancer via AI', layout=Layout(bord…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe275e4e0f7d49fa86980a7acdb19e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='1.3: Exploring predictive indicators for therapy response', layout=L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee2f53f30e14a2bb2c6e839302a7d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='2: Current challenges limiting AI-based approaches in the management…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31291063fea34bad8f59aba0e2780671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3: Strategies for unraveling the “black-box” of AI-based pathologica…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21113547ba354201ad73aea3b1875fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.1: Model-based explanation', layout=Layout(border_bottom='1px soli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ca52328d334805adb20f841322eb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.1.1: Support vector machine or random forests vs. deep learning', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128290c84f3f4471b19d0c42bf63a693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.1.2: Supervised learning vs. weakly supervised learning vs. unsupe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bae548c0214a329c27f2dc0c820389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.1.3: Textual explanation', layout=Layout(border_bottom='1px solid …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125231da709c441a8ff952665e4232ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.1.3.1: Image captioning', layout=Layout(border_bottom='1px solid p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959f43bc782945d4b68b6208f43ab229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.1.3.2: Image captioning with visual explanation', layout=Layout(bo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5077f675a22940eb91cf465b4a8e1718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.1.4: Example-based explanation', layout=Layout(border_bottom='1px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b925d97facb745e9a82120a0f5ffb772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.1.4.1: Triplet network', layout=Layout(border_bottom='1px solid pu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856041e439c844df91dcbdda4d65c992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.1.4.2: Prototypes', layout=Layout(border_bottom='1px solid purple'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5639ce12d6e94d0faaed5ad90ff8bdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.2: Post hoc explanation', layout=Layout(border_bottom='1px solid p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928e80be193e43779b562ea68b8e9aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.2.1: Visual explanation (saliency mapping, pathologist-in-the-loop…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa34962f1e85432380922a004530595c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.2.1.1: Backpropagation-based approaches', layout=Layout(border_bot…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186750046f19408ba0577c9d9468c2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.2.1.2: Perturbation-based approaches', layout=Layout(border_bottom…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b34bb1f9f8e426ca41cb8c83188c7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.2.1.3: Multiple instance learning-based approaches', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b992ff33234390a754425d71f8a769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.2.2: Textual explanation', layout=Layout(border_bottom='1px solid …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0294cf7333b424da9c416ea8707b261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='3.2.3: Example-based explanation', layout=Layout(border_bottom='1px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbebef3b6ac34d4ab5df0d6dcf3dc580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Text(value='4: Conclusion and future applications', layout=Layout(border_bottom=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%run -i \"startup.py\"\n",
    "\n",
    "section_viewer_tree = SectionViewerTree(\"cancer_review\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cf1b7d4-6892-42c7-b6b7-9a74b6edbd79",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "outline"
    ]
   },
   "source": [
    "Title: Unraveling the “black-box” of artificial intelligence-based pathological analysis of liver cancer\n",
    "\n",
    "# 1. Current advances of AI-based approaches for clinical management of liver cancer\n",
    "\n",
    "## 1.1 AI-based prognostication of liver cancer\n",
    "\n",
    "## 1.2 Molecular profiling of liver cancer via AI\n",
    "\n",
    "## 1.3 Exploring predictive indicators for therapy response\n",
    "\n",
    "# 2. Current challenges limiting AI-based approaches in the management of liver cancer\n",
    "\n",
    "(One paragraph highlighting the urgent need to explain the “black box” of deeplearning)\n",
    "\n",
    "# 3. Strategies for unraveling the “black-box” of AI-based pathological analysis of liver cancer\n",
    "\n",
    "## 3.1 Model-based explanation\n",
    "### 3.1.1 Support vector machine or random forests vs. deep learning\n",
    "### 3.1.2 Supervised learning vs. weakly supervised learning vs. unsupervised learning\n",
    "### 3.1.3 Textual explanation\n",
    "#### 3.1.3.1 Image captioning\n",
    "#### 3.1.3.2 Image captioning with visual explanation\n",
    "### 3.1.4 Example-based explanation \n",
    "#### 3.1.4.1 Triplet network\n",
    "#### 3.1.4.2 Prototypes\n",
    "\n",
    "## 3.2 Post hoc explanation\n",
    "\n",
    "### 3.2.1 Visual explanation (saliency mapping, pathologist-in-the-loop)\n",
    "#### 3.2.1.1 Backpropagation-based approaches\n",
    "      Including class activation mapping (CAM) and gradient-weighted class activation mapping (Grad-CAM)\n",
    "#### 3.2.1.2 Perturbation-based approaches\n",
    "      Including Occlusion sensitivity map (OSM), local interpretable model-agnostic explannations (LIME)\n",
    "#### 3.2.1.3 Multiple instance learning-based approaches\n",
    "\n",
    "### 3.2.2 Textual explanation\n",
    "   Testing with concept activation vectors (TCAV)\n",
    "### 3.2.3 Example-based explanation\n",
    "\n",
    "# 4. Conclusion and future applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769fe72e-848f-4766-b89d-1aa91183f615",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipynb_name = \"review_writer.ipynb\"\n",
    "markdown_cells = get_notebook_cells(notebook_path=ipynb_name, cell_types=[\"markdown\"])\n",
    "outline_cells = [\n",
    "    cell for cell in markdown_cells if \"outline\" in cell[\"metadata\"][\"tags\"]\n",
    "]\n",
    "outline_content = \"\".join([cell[\"source\"] for cell in outline_cells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "193d8a37-cb0c-4096-be66-55539ae48bf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens count: [709]\n",
      "[\n",
      "    {\n",
      "        \"idx\": 0,\n",
      "        \"level\": \"0\",\n",
      "        \"title\": \"Unraveling the “black-box” of artificial intelligence-based pathological analysis of liver cancer\",\n",
      "        \"intro\": \"A comprehensive exploration of the current advances, challenges, and strategies in applying AI for pathological analysis of liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 1,\n",
      "        \"level\": \"1\",\n",
      "        \"title\": \"Current advances of AI-based approaches for clinical management of liver cancer\",\n",
      "        \"intro\": \"An overview of the latest developments in AI technologies and their applications in the clinical management of liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 2,\n",
      "        \"level\": \"1.1\",\n",
      "        \"title\": \"AI-based prognostication of liver cancer\",\n",
      "        \"intro\": \"Discussion on how AI is being used to predict the progression and outcome of liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 3,\n",
      "        \"level\": \"1.2\",\n",
      "        \"title\": \"Molecular profiling of liver cancer via AI\",\n",
      "        \"intro\": \"Insights into how AI is leveraged for molecular profiling of liver cancer for personalized treatment strategies.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 4,\n",
      "        \"level\": \"1.3\",\n",
      "        \"title\": \"Exploring predictive indicators for therapy response\",\n",
      "        \"intro\": \"Exploration of AI's role in identifying predictive indicators for therapy response in liver cancer treatment.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 5,\n",
      "        \"level\": \"2\",\n",
      "        \"title\": \"Current challenges limiting AI-based approaches in the management of liver cancer\",\n",
      "        \"intro\": \"A detailed analysis of the existing challenges and limitations in the use of AI for liver cancer management.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 6,\n",
      "        \"level\": \"3\",\n",
      "        \"title\": \"Strategies for unraveling the “black-box” of AI-based pathological analysis of liver cancer\",\n",
      "        \"intro\": \"A comprehensive discussion on various strategies to decode the complex workings of AI in pathological analysis of liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 7,\n",
      "        \"level\": \"3.1\",\n",
      "        \"title\": \"Model-based explanation\",\n",
      "        \"intro\": \"An in-depth look into model-based explanations for AI's decision-making process in liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 8,\n",
      "        \"level\": \"3.1.1\",\n",
      "        \"title\": \"Support vector machine or random forests vs. deep learning\",\n",
      "        \"intro\": \"A comparative study of different AI models like support vector machines, random forests and deep learning for liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 9,\n",
      "        \"level\": \"3.1.2\",\n",
      "        \"title\": \"Supervised learning vs. weakly supervised learning vs. unsupervised learning\",\n",
      "        \"intro\": \"A thorough comparison of supervised, weakly supervised, and unsupervised learning approaches in AI for liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 10,\n",
      "        \"level\": \"3.1.3\",\n",
      "        \"title\": \"Textual explanation\",\n",
      "        \"intro\": \"Understanding how textual explanations can be generated from AI models for liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 11,\n",
      "        \"level\": \"3.1.3.1\",\n",
      "        \"title\": \"Image captioning\",\n",
      "        \"intro\": \"Insight into how AI can generate textual descriptions for pathological images in liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 12,\n",
      "        \"level\": \"3.1.3.2\",\n",
      "        \"title\": \"Image captioning with visual explanation\",\n",
      "        \"intro\": \"Discussion on how AI combines textual descriptions with visual explanations for a comprehensive understanding of liver cancer pathology.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 13,\n",
      "        \"level\": \"3.1.4\",\n",
      "        \"title\": \"Example-based explanation\",\n",
      "        \"intro\": \"Exploring how AI uses examples to provide explanations for its decisions in liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 14,\n",
      "        \"level\": \"3.1.4.1\",\n",
      "        \"title\": \"Triplet network\",\n",
      "        \"intro\": \"Understanding how the triplet network, an example-based explanation model, works in the context of AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 15,\n",
      "        \"level\": \"3.1.4.2\",\n",
      "        \"title\": \"Prototypes\",\n",
      "        \"intro\": \"Exploration of how prototypes are used as examples to explain AI decisions in liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 16,\n",
      "        \"level\": \"3.2\",\n",
      "        \"title\": \"Post hoc explanation\",\n",
      "        \"intro\": \"Discussion on post hoc explanations, which provide insights into AI decisions after they have been\n",
      "Response Tokens count: [1000] [stop]\n",
      "Continue ...\n",
      "Prompt Tokens count: [1717]\n",
      "[\n",
      "    ...\n",
      "    {\n",
      "        \"idx\": 16,\n",
      "        \"level\": \"3.2\",\n",
      "        \"title\": \"Post hoc explanation\",\n",
      "        \"intro\": \"Discussion on post hoc explanations, which provide insights into AI decisions after they have been made in the context of liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 17,\n",
      "        \"level\": \"3.2.1\",\n",
      "        \"title\": \"Visual explanation (saliency mapping, pathologist-in-the-loop)\",\n",
      "        \"intro\": \"Exploration of visual explanation techniques like saliency mapping and pathologist-in-the-loop in AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 18,\n",
      "        \"level\": \"3.2.1.1\",\n",
      "        \"title\": \"Backpropagation-based approaches\",\n",
      "        \"intro\": \"Insight into backpropagation-based approaches for visual explanation in AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 19,\n",
      "        \"level\": \"3.2.1.2\",\n",
      "        \"title\": \"Perturbation-based approaches\",\n",
      "        \"intro\": \"Understanding of perturbation-based approaches for providing visual explanations in AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 20,\n",
      "        \"level\": \"3.2.1.3\",\n",
      "        \"title\": \"Multiple instance learning-based approaches\",\n",
      "        \"intro\": \"Discussion on how multiple instance learning-based approaches contribute to visual explanations in AI-based liver cancer analysis.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 21,\n",
      "        \"level\": \"3.2.2\",\n",
      "        \"title\": \"Textual explanation\",\n",
      "        \"intro\": \"Insight into how textual explanations can be used in post hoc analysis of AI's decisions in liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 22,\n",
      "        \"level\": \"3.2.3\",\n",
      "        \"title\": \"Example-based explanation\",\n",
      "        \"intro\": \"Exploring how example-based explanations can be used in post hoc analysis of AI's decisions in liver cancer.\"\n",
      "    },\n",
      "    {\n",
      "        \"idx\": 23,\n",
      "        \"level\": \"4\",\n",
      "        \"title\": \"Conclusion and future applications\",\n",
      "        \"intro\": \"Final thoughts on the current state of AI in liver cancer analysis and potential future developments in the field.\"\n",
      "    }\n",
      "]\n",
      "Response Tokens count: [474] [stop]\n"
     ]
    }
   ],
   "source": [
    "outline_filler.clear_history_messages()\n",
    "outline_filler_res = outline_filler.chat(outline_content, continous=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "942bbdb9-1bb2-496a-9881-bc542f4e7779",
   "metadata": {},
   "source": [
    "A typical workflow of generate a detailed review of a section title:\n",
    "- [Agent] outline_filler: Fill in the outline with intros, and structuralize it to JSON with idx and levels.\n",
    "- [Agent] Synonymer: Generate similar texts for each sub-section\n",
    "- [Program] Retrieve and Re-rank from local embbedings of docs\n",
    "- [Agent] Summarizer: Summarize the above content into a section\n",
    "- [Agent] Polisher: Polish the content of the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ecc57-3a03-4931-b56d-16297893b51f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outline_details_json_path = \"data/review_outline_details.json\"\n",
    "with open(outline_details_json_path, \"r\", encoding=\"utf-8\") as rf:\n",
    "    outline_details_data = json.load(rf)\n",
    "display(outline_details_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0031753d6f644f52ad463a70cd991483": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "0068b18936ac4914845d0c2602d6d8c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "009fe58f8f6a4365afde11a67638566c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "0143c3e3964d47a0982781bae7c47383": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_14e04506bd274353a6d18c56d2a21880",
       "style": "IPY_MODEL_b4f28fa7ccd64c4e901a08f5eacdec13",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "017b6bc779b340c680c852779581e949": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "0180cb96e13a4e3a9efab2828f1925a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "01a7b4b31eb143228b6ba5c097e04d31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "01d9ba735e0d473ea1b06161a28a367f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "01ebc972134c4fa090df26a63b239dae": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_8738c53c57d04c25ba4a080483037bfd"
      }
     },
     "01ee66589d6d4ed4881446b86156d9c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_08e7eb38668946cfac68542568f45eec",
        "IPY_MODEL_7374e220effb4f988a57fba2aebcd956"
       ],
       "layout": "IPY_MODEL_fb21a67e33aa40cb83c468fc22432767"
      }
     },
     "0229b0880554442fbbeb750c4882e65d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "0297760bd9cf46f3bf972abb40c498d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "02d9fe82014f43baadd1b3646e13b3bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cd7257f8b0f24b5db34b0453d68ec418",
       "style": "IPY_MODEL_8066150bb1c9435e9c264443e2f468c0",
       "value": "words:"
      }
     },
     "02ecc3c69fdc4603b246191133186f54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "02faa3f36fab4537ac7f84f21e633f2b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "032c5cfd840947039785186dc90d78d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "032de5e8923e46108f2ca8622bda8a9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ccf235b2478341dbaa58250cf59278c1",
        "IPY_MODEL_5514f030f81049d6beaeba8c8e3836a9"
       ],
       "layout": "IPY_MODEL_e547c4f8555540f1a0c072c6389a71a3"
      }
     },
     "0375240e2eaf40b5b7e59ad6c56d3fab": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_27b05885388845b9935640726138a08c"
      }
     },
     "03aaa7d656bf44de8717b52f7bda2636": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_cf8c8eb79aa94ee1a114eb82e9d30e66",
       "placeholder": "",
       "style": "IPY_MODEL_2a9a17910bea46f2a36148a6b479886a",
       "value": "500"
      }
     },
     "03b7d0d6fd9145a5877f123464043e2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_da648c9f646c4a02a5345e33490c6922",
       "style": "IPY_MODEL_9951360b74d54e6b9c88f2d2c2753130",
       "value": "queries,"
      }
     },
     "03cf1792dae343d78877ab86fd849920": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "display": "none",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "03dd8e1d77ef4f04aa594376bec45a34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "042a51824b25420d84cdec561ab5bc91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0455024034e54eb7a2f242c28eed308f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_12affdc9e34d4acb91d6f002a7ee7260",
       "style": "IPY_MODEL_bf927469e1d04715bafcf045a369127f",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "04d285352d8f4f5bafaf07a0379eb7f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_aa153e1d637e4d9fa6455bbaba3b1320",
       "style": "IPY_MODEL_3ef951f4a19d48c3ac9f2c1e3de490cb",
       "value": "words:"
      }
     },
     "04f83343973b469483e5d1169c75ffa9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_4086897d8a80434fa88d05f08fdaeaad",
       "style": "IPY_MODEL_1c75599849104efa8cbe6b67ef2eac51",
       "value": "Understanding of perturbation-based approaches for providing visual explanations in AI-based liver cancer analysis."
      }
     },
     "0546aedaf5b545799d176f5a3b9030b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "059d17fcfaae47609ec578626d4838d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0dc8b995044944668237f210b913b1e6",
        "IPY_MODEL_dfed4df6f441483b96006443fffc5f32"
       ],
       "layout": "IPY_MODEL_d13edf35e53f42a09484242611a9cbee"
      }
     },
     "05c82618049543a48914ca149f3c151b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_94614e7598c3424dab56113747a9b9f5",
       "style": "IPY_MODEL_e87314c58adf489e92373607805ddf37",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "063f610666594d21a937314ab58015a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "0663457f2f0648c599421f94fe891612": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "06b56d27fa63436dbdadb82445518108": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "073e5d15308042e98bee27401616a533": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_3d0c35a14cf74cdaa49f1cbad7af60fb",
       "style": "IPY_MODEL_7eeffdb08c7346248f4038587a18f7c2",
       "value": "3.1.4: Example-based explanation"
      }
     },
     "07456fb1d30241c88dc1a26fd76cb786": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "079cb1bc90ba4602b2792b4a330e272d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "0819539caa4d4b50a85c7d133512bfec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "083b6876571448fe94dae02c803215a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_94614e7598c3424dab56113747a9b9f5",
       "style": "IPY_MODEL_53db944396864cb3a4f1efeb24505a15",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "0853aacb77d94959a5e1f2fb88237501": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_d375c9d62415434f9f306d5a38427813"
      }
     },
     "087ed26a3c2142bc94fa41070cf19fee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_af68b67ec7e9487190905798b039a057",
        "IPY_MODEL_7759cab5c396493ca1aac5af20e0199f"
       ],
       "layout": "IPY_MODEL_ee16a713233a4ae7bc3ba1de5d4b8c2b"
      }
     },
     "0881152d236c40699785d38c23808917": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "08c2917582184039a7c9d16b558c1092": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e5c7bca9f5424f18a50e8166f0eca60a",
       "style": "IPY_MODEL_26bd11907b6c4238a9966c77aea637fe",
       "value": "words:"
      }
     },
     "08d4203db7684d5b88802687ab94f949": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "08e7eb38668946cfac68542568f45eec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_e57c1327f3aa4eaabd1aa47142027d8c",
       "style": "IPY_MODEL_0fffa52983f1498cb9fab9fb9e905ffa",
       "tooltip": "Retrieve related references"
      }
     },
     "092488cf3d0041768dd8de85fd21ae93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3b6c1d9c3a4c48c68d86187847701042",
        "IPY_MODEL_677c5f927cbd4487a26c6d186d76166e"
       ],
       "layout": "IPY_MODEL_34a9736c7d784afda81f6556c854004f"
      }
     },
     "0976635289fd4571b03bc80cc5b1a22f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "099716819ef048739cdecaebbae3c6ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b6dbcae4cb8b434f8a68aded30f1734a"
       ],
       "layout": "IPY_MODEL_901a05af035f44c3aa5c523af652d2fa"
      }
     },
     "099a20e196eb49a4912a638f45b06379": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "09a30498eba542269fdc3cea963d8c7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c282d596d59e4cd095287cfbaf098519",
        "IPY_MODEL_d543a9cb32714f0d804f1aaa59fffde0"
       ],
       "layout": "IPY_MODEL_68acd03ded0e4ee0a492bf9bd5346b5b"
      }
     },
     "09d234d5778a4208b2e0410deeb0e129": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "0a1305b666b84c52b09e19b74a534717": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "0a6ead8eded74242a2301b10d30f7432": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "0ab79db87b3d4982a90bce5a0312d3ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_b531cb8981884cb6b1991d2251e7fd71",
       "placeholder": "",
       "style": "IPY_MODEL_62e3f15827c241bb96f85c3433a2efb3"
      }
     },
     "0b0c2ea3863549e8a1beda77d59e3e1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0b41f4586a06404896ab6a19638ae578": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ae9514d4cbd347338b1bddd4df042c67",
        "IPY_MODEL_db477aba29654c2e800ff389cd17b6c5"
       ],
       "layout": "IPY_MODEL_69c67ff295384556a856a5719c3492f5"
      }
     },
     "0b4540fcbd37436c998a7b16102e7190": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_95814dfbcc244121bacd229133995c35",
        "IPY_MODEL_448e163e1621471f8886992083c8ecf1"
       ],
       "layout": "IPY_MODEL_e5c58669b4a0409e976de0470c749ec7"
      }
     },
     "0b5560022fa24917a5c2fbf00d69002b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "0baa2b74a2054f50811bb2a39576ec70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9e445ab99e5c4d48ae724f303e512d02",
       "style": "IPY_MODEL_7850706c5f50404db8224ff3b7ec842a",
       "value": "queries,"
      }
     },
     "0bed9d0c862840e88653256405ddbea3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0c258597214043a5a55d8f97cb955c9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_56fa6ef96dca47248fcbb5a2885b4ec0",
       "style": "IPY_MODEL_c54aeee8ed4a4bc3b9b533b797f74cfe",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "0c3d99ce97734e61ae987c231c7b1152": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0c474a41ba9848ee8c7971d34ddbd794": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_502355bfdcc5476ab76e55ed7e4eb93d",
        "IPY_MODEL_d4f999481b66475dbc771c7f3f57224b"
       ],
       "layout": "IPY_MODEL_c85f18ac090b47dc91d3f2a618e523b5"
      }
     },
     "0c9eee9e0eb44218a7bd87e7a3ed5c3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0cf5a5bb3b5e4c758284dccf3a15bc80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d18ed878510241d7820448dc19b86049",
        "IPY_MODEL_daf86b1d122e4b4094cad4fd143210f6"
       ],
       "layout": "IPY_MODEL_2ae26bad3e1b400cb2690dc130cdfcb6"
      }
     },
     "0cf70f6f3a99463bbe8ad5da952744a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "0d0533f476094a81babf12116d2b5c9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_30228f53b94e4dcf958cbc5da158d091",
       "placeholder": "",
       "style": "IPY_MODEL_b22e9f20041745eead70fe525d9d48ba"
      }
     },
     "0d73a3d0a6b343c48902af7d7cb353c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "0d84164856fd41359e810ad40a0a50db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "0db611051adb448ba974bc65da1b80a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0dc8b995044944668237f210b913b1e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_9c81866b1888464d899d8cf8abeaac08",
       "style": "IPY_MODEL_226057af62724dafb0706c79ba4e78aa",
       "value": "Insights into how AI is leveraged for molecular profiling of liver cancer for personalized treatment strategies."
      }
     },
     "0de2632dd52c4ba9b46af10c2a987ebc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0e1847704d93448b86dadfa54e07b9f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "0e5527f389b34c9a9d152a140d05e62a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "0ec7cfd608344b4ca3a2d194fb22ec23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "0ed4f1c9eebd46f0b5f8068992946262": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_945e24076a2040a3b14ab3a49cd96205",
       "style": "IPY_MODEL_238e4a3dcde54ab99b8f07d9ca089a98",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "0ed75aecce364e49bdd340dbb2ae9358": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d87d7b3d76ef443f97f0d6ad6df1ff78",
       "style": "IPY_MODEL_13a93f086fdc4209bc9e23ea41e53d91",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5397049);\n                color: white\n                ' title='Perturbation-based techniques perturb the input image to assess the importance of certain areas of that image for the task under consideration. Zeiler and Fergus (2014) used an occlusion sensitivity analysis to visualize which parts of the image were most important for classification. For example, they showed that an image of a dog holding a tennis ball was correctly classified by the dog’s breed, except if the face of the dog was occluded, which yielded the incorrect classification ‘tennis ball’. '>\n                            Page 8, Region 5,\n                            Score 0.54\n                        </summary>\n                        Perturbation-based techniques perturb the input image to assess the importance of certain areas of that image for the task under consideration. Zeiler and Fergus (2014) used an occlusion sensitivity analysis to visualize which parts of the image were most important for classification. For example, they showed that an image of a dog holding a tennis ball was correctly classified by the dog’s breed, except if the face of the dog was occluded, which yielded the incorrect classification ‘tennis ball’. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.25743353);\n                color: white\n                ' title='Fong and Vedaldi (2017) introduced meaningful perturbation, where they perturbed the input image to detect changes in the predictions of a trained neural network. Rather than using perturbations such as occlusion sensitivity that block out parts of the image, they suggested simulating naturalistic or plausible effects, leading to more meaningful perturbations, and consequently to more meaningful explanations. They opted for three types of local perturbations, namely a constant value, noise, or blurring. '>\n                            Page 8, Region 10,\n                            Score 0.26\n                        </summary>\n                        Fong and Vedaldi (2017) introduced meaningful perturbation, where they perturbed the input image to detect changes in the predictions of a trained neural network. Rather than using perturbations such as occlusion sensitivity that block out parts of the image, they suggested simulating naturalistic or plausible effects, leading to more meaningful perturbations, and consequently to more meaningful explanations. They opted for three types of local perturbations, namely a constant value, noise, or blurring. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.9461703);\n                color: white\n                ' title='For visual explanation techniques, there is a clear distinction between’ backpropagation-based and __perturbationbased techniques with respect to their computational needs. Backpropagation-based techniques typically make a_ single pass back through the neural network, which is relatively fast. Perturbation-based techniques require, however, extensive perturbation of input images to measure the influence of these perturbations on the output. Therefore, these techniques are generally more computationally-expensive. This can especially be the case in 3-dimensional, 4-dimensional, and/or multi-modality images, which often occur in medical image analysis. '>\n                            Page 13, Region 7,\n                            Score 0.95\n                        </summary>\n                        For visual explanation techniques, there is a clear distinction between’ backpropagation-based and __perturbationbased techniques with respect to their computational needs. Backpropagation-based techniques typically make a_ single pass back through the neural network, which is relatively fast. Perturbation-based techniques require, however, extensive perturbation of input images to measure the influence of these perturbations on the output. Therefore, these techniques are generally more computationally-expensive. This can especially be the case in 3-dimensional, 4-dimensional, and/or multi-modality images, which often occur in medical image analysis. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='Perturbation-based visual explanation techniques often require a choice of the perturbation. For example, both occlusion sensitivity and LIME require the user to define the size and shape of the occluded areas. In meaningful perturbation, the user has to define what kind of perturbation technique is deemed best. '>\n                            Page 13, Region 13,\n                            Score 1.0\n                        </summary>\n                        Perturbation-based visual explanation techniques often require a choice of the perturbation. For example, both occlusion sensitivity and LIME require the user to define the size and shape of the occluded areas. In meaningful perturbation, the user has to define what kind of perturbation technique is deemed best. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.43727702);\n                color: white\n                ' title='Another rapidly growing area of research is focused on improved characterisation of indeterminate liver lesions. In clinical practice, when an abdominal ultrasound shows a new liver lesion, a patient is typically referred for further imaging, with contrast-enhanced CT or MRI. Based on the fulfilment of specific radiologic criteria, certain liver lesions may be considered as having pathognomonic features of HCC, and thus do not require liver biopsy for further histological confirmation. However, liver nodules imaged by CT or MRI often demonstrate indeterminate features, for which current recommendations include either liver biopsy or close interval follow-up with serial imaging.”° This practice is sub-optimal, resulting in numerous imaging studies, patient stress, and the potential for delayed diagnoses of liver cancer. For this reason, a growing body of recent literature has explored AI approaches to improve risk stratification of indeterminate liver lesions, to facilitate earlier and more accurate detection of HCC. '>\n                            Page 4, Region 4,\n                            Score 0.44\n                        </summary>\n                        Another rapidly growing area of research is focused on improved characterisation of indeterminate liver lesions. In clinical practice, when an abdominal ultrasound shows a new liver lesion, a patient is typically referred for further imaging, with contrast-enhanced CT or MRI. Based on the fulfilment of specific radiologic criteria, certain liver lesions may be considered as having pathognomonic features of HCC, and thus do not require liver biopsy for further histological confirmation. However, liver nodules imaged by CT or MRI often demonstrate indeterminate features, for which current recommendations include either liver biopsy or close interval follow-up with serial imaging.”° This practice is sub-optimal, resulting in numerous imaging studies, patient stress, and the potential for delayed diagnoses of liver cancer. For this reason, a growing body of recent literature has explored AI approaches to improve risk stratification of indeterminate liver lesions, to facilitate earlier and more accurate detection of HCC. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Explainable medical imaging AI needs human-centered design a systematic review</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.40811893);\n                color: white\n                ' title='specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions '>\n                            Page 7, Region 4,\n                            Score 0.41\n                        </summary>\n                        specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.19958894);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.2\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.15305097);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.15\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.4971989);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.5\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5120214);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.51\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.04879815);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.05\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.12647966);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.13\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.00460518);\n                color: white\n                ' title='of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. '>\n                            Page 8, Region 6,\n                            Score 0.0\n                        </summary>\n                        of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.0\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7898141);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.79\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.4237025);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.42\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.11234325);\n                color: white\n                ' title='Optimistically, ML/DL systems could help resolve the diagnostic, prognostic and predictive issues that limit liver histopathology image analysis. This would improve and facilitate clinical trials in liver disease in which inclusion criteria, patient strata and histological endpoints are often manually defined by pathologists and therefore subject to intra- and inter-observer variability.*° As in other disease contexts, there is a place in clinical decision making for invasive tissue-based diagnostics. ML/DL approaches could conceivably improve the consistency, quality and amount of information which researchers and healthcare providers can extract from this tissue. The benefits of these ML/DL approaches to histopathological analysis may incentivise patients to undergo an invasive procedure such as liver biopsy. However, for some problems in the management of liver disease, non-invasive radiology images, instead of invasive diagnostics, can be analysed to unveil biomarkers. In the following section, we will review the state of the art in ML/DL approaches applied to such radiology data. '>\n                            Page 4, Region 11,\n                            Score 0.11\n                        </summary>\n                        Optimistically, ML/DL systems could help resolve the diagnostic, prognostic and predictive issues that limit liver histopathology image analysis. This would improve and facilitate clinical trials in liver disease in which inclusion criteria, patient strata and histological endpoints are often manually defined by pathologists and therefore subject to intra- and inter-observer variability.*° As in other disease contexts, there is a place in clinical decision making for invasive tissue-based diagnostics. ML/DL approaches could conceivably improve the consistency, quality and amount of information which researchers and healthcare providers can extract from this tissue. The benefits of these ML/DL approaches to histopathological analysis may incentivise patients to undergo an invasive procedure such as liver biopsy. However, for some problems in the management of liver disease, non-invasive radiology images, instead of invasive diagnostics, can be analysed to unveil biomarkers. In the following section, we will review the state of the art in ML/DL approaches applied to such radiology data. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.39910668);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.4\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.12650388);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.13\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.022322904);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.02\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "0edadb5292334c539eef006d6be75894": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "0f17ca5cb5944585a3fbf8530f88ad77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0f7eeceee7de49eca560692a3f1ab621": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "0fb1f4ac52ec4a3cb46992247bbd6de3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0fbd53432ae046108176e0059c9e3f31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5fc061c4578f4776bca8b06cc295cdb6",
       "style": "IPY_MODEL_467fb7ce955e4776a673372d34531cde",
       "value": "words:"
      }
     },
     "0fffa52983f1498cb9fab9fb9e905ffa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "101981998945498a8ad2c9894819e529": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_66b75abff93d412180a46e380361a843"
      }
     },
     "103456ebf7bb4c5f8aca08727ca0d4d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "104033f861f8484e8742316c29698554": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_14e04506bd274353a6d18c56d2a21880",
       "style": "IPY_MODEL_8d6040674d3c4515b784c52810dbbba8",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "106183ba847046d88064416060143347": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "10840a4be51543fe92390c1322928db4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_68e377660bb64cfab30c0a210586c993",
        "IPY_MODEL_d94c9508308c43809286df9645c88f22"
       ],
       "layout": "IPY_MODEL_88eccd4c254d41a2b47347b212a3badb"
      }
     },
     "109df0bdc9a84e25a8d06c8ab2984f17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "10bcba80c3254c14821653739101812b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "10be2c32ef9c43838354d9128f54dbc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_079cb1bc90ba4602b2792b4a330e272d",
       "style": "IPY_MODEL_bda2ba9588fd4e21be36644adb04e80c",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "10cdfd0781e04962876bed0fd4d0b2ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_b03d6123c9ac40a9b26f3b3246e011e2",
       "style": "IPY_MODEL_b026ed74e17842f1915d506fbac004f0",
       "value": "An overview of the latest developments in AI technologies and their applications in the clinical management of liver cancer."
      }
     },
     "11189e0f3121495abba2285a3c8fa259": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "117acc032eb24139b0afdf4ff880e7be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d313704fced44c96a94df964e6b39302",
       "style": "IPY_MODEL_9094304c640647389caa0e60e7118170",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Explainable medical imaging AI needs human-centered design a systematic review</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.48109844);\n                color: white\n                ' title='specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions '>\n                            Page 7, Region 4,\n                            Score 0.48\n                        </summary>\n                        specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.47666976);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.48\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Multi-task deep learning network to predict future macrovascular invasion in hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.1918835);\n                color: white\n                ' title='In this regard, deep learning can be informative and helpful. By indepth mining and efficient analysis of data both within and beyond the traditional visual system, deep learning algorithms bring medicine to the data-driven era [15]. Considering hepatology, deep learning has outperformed the traditional shear wave elastography in assessing liver fibrosis [16]. Researchers have also proven that deep learning outperforms conventional machine learning models in differentiating HCC from cirrhotic parenchyma [17]. Moreover, it has promising performance in predicting disease progression and the OS of HCC [18]. Nevertheless, overfitting is a common problem in deep learning algorithms. Multi-task learning was introduced to control overfitting. Achieving positive feedbacks among related tasks can enrich information and increase the accuracy of each task; thereby, improving the performance the overall model [19]. Therefore, we constructed a multi-task deep learning neural network (MTnet) to construct models to predict macrovascular invasion and assist in early intervention. '>\n                            Page 2, Region 11,\n                            Score 0.19\n                        </summary>\n                        In this regard, deep learning can be informative and helpful. By indepth mining and efficient analysis of data both within and beyond the traditional visual system, deep learning algorithms bring medicine to the data-driven era [15]. Considering hepatology, deep learning has outperformed the traditional shear wave elastography in assessing liver fibrosis [16]. Researchers have also proven that deep learning outperforms conventional machine learning models in differentiating HCC from cirrhotic parenchyma [17]. Moreover, it has promising performance in predicting disease progression and the OS of HCC [18]. Nevertheless, overfitting is a common problem in deep learning algorithms. Multi-task learning was introduced to control overfitting. Achieving positive feedbacks among related tasks can enrich information and increase the accuracy of each task; thereby, improving the performance the overall model [19]. Therefore, we constructed a multi-task deep learning neural network (MTnet) to construct models to predict macrovascular invasion and assist in early intervention. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.61809254);\n                color: white\n                ' title='Deep learning algorithms have proven to be advantageous in constructing models for diagnosis and prognosis of cancers, especially for liver diseases [16—18,27]. Meanwhile, among all the types of deep learning algorithms, multi-task learning combines severally related tasks during the training process and these can benefit from each other. Multi-task learning has attracted considerable attention in the field of medical image analysis [28—29]; however, its application in HCC has been limited to microvascular invasion rather than macrovascular invasion [30]. Considering the potential advantages of multi-task learning, we constructed our MTnet to predict macrovascular invasion. '>\n                            Page 8, Region 8,\n                            Score 0.62\n                        </summary>\n                        Deep learning algorithms have proven to be advantageous in constructing models for diagnosis and prognosis of cancers, especially for liver diseases [16—18,27]. Meanwhile, among all the types of deep learning algorithms, multi-task learning combines severally related tasks during the training process and these can benefit from each other. Multi-task learning has attracted considerable attention in the field of medical image analysis [28—29]; however, its application in HCC has been limited to microvascular invasion rather than macrovascular invasion [30]. Considering the potential advantages of multi-task learning, we constructed our MTnet to predict macrovascular invasion. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='Multiple instance learning can be used for visualizing explanations. In multiple instance learning, training sets consist of bags of instances (Dietterich et al., 1997). These bags are labeled, but the instances are not. In medical image analysis, multiple instance learning can for example be done using a patch-based approach: An image represents the bag, and patches from that image represent the instances (Cheplygina et al., 2019). '>\n                            Page 8, Region 17,\n                            Score 1.0\n                        </summary>\n                        Multiple instance learning can be used for visualizing explanations. In multiple instance learning, training sets consist of bags of instances (Dietterich et al., 1997). These bags are labeled, but the instances are not. In medical image analysis, multiple instance learning can for example be done using a patch-based approach: An image represents the bag, and patches from that image represent the instances (Cheplygina et al., 2019). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.17965719);\n                color: white\n                ' title='Peng et al. (2019) used example-based explanation in colorectal cancer histology. They first trained a CNN using a triplet loss, hashing, and k hard-negatives to learn an embedding that preserves similarity. In testing, a coarse-to-fine search yielded the 10 nearest examples from a testing database related to the input image. This provided explanation on which images similar to the image that was being analyzed the network based a decision. '>\n                            Page 10, Region 14,\n                            Score 0.18\n                        </summary>\n                        Peng et al. (2019) used example-based explanation in colorectal cancer histology. They first trained a CNN using a triplet loss, hashing, and k hard-negatives to learn an embedding that preserves similarity. In testing, a coarse-to-fine search yielded the 10 nearest examples from a testing database related to the input image. This provided explanation on which images similar to the image that was being analyzed the network based a decision. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.033624876);\n                color: white\n                ' title='These conflicting results demonstrate that more research is desired for visual explanation techniques in medical image analysis. For textual and example-based XAI, such rigorous comparison studies have not yet been performed. '>\n                            Page 13, Region 3,\n                            Score 0.03\n                        </summary>\n                        These conflicting results demonstrate that more research is desired for visual explanation techniques in medical image analysis. For textual and example-based XAI, such rigorous comparison studies have not yet been performed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.3676607);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.37\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.38786843);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.39\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.14489706);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.14\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.29248264);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.29\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.041575797);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.04\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0007413689);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.0\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.43969938);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.44\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.59847444);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.6\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.15702863);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.16\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.11133707);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.11\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>GUT 2020 Exploring prognostic indicators in the pathological images of hepatocellular carcinoma based on deep learning</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.10266302);\n                color: white\n                ' title='Figure 1 Data and workflow for the prognostic analysis of liver cancer with deep learning. We first developed the classification network using 260 whole-slide images (WSls) as the category-based sampling. The network was then used to analyse the remaining WSls and generate the segmentation maps. We randomly sampled tiles from each type of tissue based on these segmentation maps. Next, we trained the prognostic network and calculated a tumour risk score (TRS) for each patient. Finally, we used TRS to predict patient prognosis, and integrate transcriptomics, genomics and neural network heatmaps to identify interpretable features. TCGA, The Cancer Genome Atlas. '>\n                            Page 2, Region 6,\n                            Score 0.1\n                        </summary>\n                        Figure 1 Data and workflow for the prognostic analysis of liver cancer with deep learning. We first developed the classification network using 260 whole-slide images (WSls) as the category-based sampling. The network was then used to analyse the remaining WSls and generate the segmentation maps. We randomly sampled tiles from each type of tissue based on these segmentation maps. Next, we trained the prognostic network and calculated a tumour risk score (TRS) for each patient. Finally, we used TRS to predict patient prognosis, and integrate transcriptomics, genomics and neural network heatmaps to identify interpretable features. TCGA, The Cancer Genome Atlas. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Exploring pathological signatures for predicting the recurrence of early-stage hepatocellular carcinoma based on deep learning</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.06800375);\n                color: white\n                ' title='The emergence of AI has reformed multiple aspects of cancer management. The combination of deep learning and '>\n                            Page 9, Region 6,\n                            Score 0.07\n                        </summary>\n                        The emergence of AI has reformed multiple aspects of cancer management. The combination of deep learning and \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Implementation of deep learning in liver pathology optimizes diagnosis of benign lesions and adenocarcinoma metastasis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='In summary, we show for the first time that a comprehensive series of automated identification and classification of common benign and malignant lesions in the liver is possible by deep learning on scanned histological tissue sections. Our work can contribute to an objective and efficient workflow in routine diagnostics for highly relevant diagnostic questions, such as the differentiation between benign and malignant structures and the origin of frequent types of metastasis. This tool may aid pathologists, especially in situations where limited tissue is available, to establish and confirm the diagnosis. Furthermore, we provide an exceptional annotated liver dataset for the development and validation of deep learning algorithms which we provided to the scientific community. At the end, this may be a step towards improved personalized oncology therapy concepts, which will in the future integrate large clinical, radiological and pathological data sets using artificial intelligence. '>\n                            Page 12, Region 6,\n                            Score 0.0\n                        </summary>\n                        In summary, we show for the first time that a comprehensive series of automated identification and classification of common benign and malignant lesions in the liver is possible by deep learning on scanned histological tissue sections. Our work can contribute to an objective and efficient workflow in routine diagnostics for highly relevant diagnostic questions, such as the differentiation between benign and malignant structures and the origin of frequent types of metastasis. This tool may aid pathologists, especially in situations where limited tissue is available, to establish and confirm the diagnosis. Furthermore, we provide an exceptional annotated liver dataset for the development and validation of deep learning algorithms which we provided to the scientific community. At the end, this may be a step towards improved personalized oncology therapy concepts, which will in the future integrate large clinical, radiological and pathological data sets using artificial intelligence. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "1188b6d855a949a984e571b5e6c0a570": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1198d1fafb024c7a8635453a4256489d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "121f2c1154d4436598bb07177854bcbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d944210efc184ebdb5eb697c90513801",
        "IPY_MODEL_3f811633ac9f49d2862935c6f2d49836"
       ],
       "layout": "IPY_MODEL_0068b18936ac4914845d0c2602d6d8c9"
      }
     },
     "1233988ff48c42999c5e6545ada30824": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_6687c399adc24050a81af7d449bb1850",
       "placeholder": "",
       "style": "IPY_MODEL_1198d1fafb024c7a8635453a4256489d",
       "value": "500"
      }
     },
     "125231da709c441a8ff952665e4232ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2ab39325b0bb4ca085f8869654f67f5c",
        "IPY_MODEL_cfbd614d8a2e4814b206cf59b203a956"
       ],
       "layout": "IPY_MODEL_e68259c5526c4e5eb0096910bd4e5914"
      }
     },
     "128290c84f3f4471b19d0c42bf63a693": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8dd3cecb35c44f979fa703b4c958d994",
        "IPY_MODEL_32ec22ba6f2f4e98b465ca947dfa6ea0"
       ],
       "layout": "IPY_MODEL_0819539caa4d4b50a85c7d133512bfec"
      }
     },
     "1298a46f4b7d4f29b23df2760d5958f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "12affdc9e34d4acb91d6f002a7ee7260": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "12baa644f3a841f2a6f6d02d99320384": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5013dffaeb4342a5862fc747d591795e",
        "IPY_MODEL_09a30498eba542269fdc3cea963d8c7f"
       ],
       "layout": "IPY_MODEL_e43783a04cac4df9859b7fbb28a75e91"
      }
     },
     "12c7d4366e8a4075b63992ed92a34f66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_3d34d24d18eb4f0e9d01f7a991cd3577",
       "placeholder": "",
       "style": "IPY_MODEL_8e06a631c790468ea4fffc9ac9bc8d68",
       "value": "500"
      }
     },
     "12ca52328d334805adb20f841322eb3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_629e5a970e4644419aac0156295ddd04",
        "IPY_MODEL_4e3760bdfb234b388c275bb7afbd6c5b"
       ],
       "layout": "IPY_MODEL_9b1c0f32594740d79b08971a04fa97b8"
      }
     },
     "12cfbab39eef4050bacc83c02b13d156": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "12e15e667e7a4ab6ab4cc78fc947f1c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "12fc144615184f1b8d6f8d7873860a1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "130772658b3643aa94f3e329d9f3bd0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_981074ae1e204d38a809586134a3f492",
       "style": "IPY_MODEL_63165eb468984153a0ecb82006b9f10c",
       "value": "1.1: AI-based prognostication of liver cancer"
      }
     },
     "1394efa36f30442997a73a073999d2e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "139dd0f09e984442bdbf0710b27b7f1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_520b1f0f984245dcbb3a6cd6243e2085",
        "IPY_MODEL_c463cc84c8364315a7b677aed6f97dbe"
       ],
       "layout": "IPY_MODEL_f5cce34691a94385ada25f2f8160dc63"
      }
     },
     "13a93f086fdc4209bc9e23ea41e53d91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "13b254b04c2b48ec9c57f47664d41511": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_c39226cff0ac47f5bfd44c139b597dd9"
      }
     },
     "14092ecef0914a1296fa78a8e98eb0d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_0a6ead8eded74242a2301b10d30f7432",
       "style": "IPY_MODEL_5005b1dfcb7e41f8a13f1d7b7102806d",
       "tooltip": "Retrieve related references"
      }
     },
     "148e68ae28474ea0a98010b45ccbe98c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "14ab93e0ac024bf68443cfdbfb38c0d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "14be51735f434ba3b005e13779c6f9c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3fb9ff8b02d74401b755f65e6eaeb6bb",
       "style": "IPY_MODEL_11189e0f3121495abba2285a3c8fa259"
      }
     },
     "14e04506bd274353a6d18c56d2a21880": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "14edc3b97670495eaba7af7153d770c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "152091df89954c9fa800e37ed3dfab62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_32ed49e68149459bbf63214adf30c6e1",
       "placeholder": "20",
       "style": "IPY_MODEL_55bb1de9e29c49b4ab2affe8dfeda0f7",
       "value": "20"
      }
     },
     "156b9ad114a7479aae26a226ffec19be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "15c2ed75773b4ffa80754fc9f79783b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "15d5b2d0381b46dfac8618eb7488fa2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "15dd2934fb4142b58c1297de12b935a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "1602c6c50ed54ec6b233ca1f2fdf983f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "164b4e84d339442788599f9eac06057b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "1671a6cfbfd54667980b94a0e2490751": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "169edeb383cd4663a759cf652d0aae03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "16c6dd32fad14307b3ffe2f3c4bf7f40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "16f2126a8db147f4902e920376339b25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_01a7b4b31eb143228b6ba5c097e04d31",
       "style": "IPY_MODEL_5ca8b6f96de142aeb579257d58988080",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "1728e328be57414ea39c581946cb19db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e3a7667b8af74643916d2902c35e6901",
       "style": "IPY_MODEL_1b3488ba282b4f1e88be49ab00647ebb",
       "value": "queries,"
      }
     },
     "1739d827889445838956b95852f31afe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "173d3eb0c7804760be049d211f6232ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "17487543800443c5a776ae63d0415979": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "176174276647453eb3876528451c410e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "177831bef9b44f2d8a83f4c71e47cf8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "17a0cc8185d94467a193d0893e267ff5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "17afcc4088b34d9c8036209f89af82dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_76dc7c227ef44e5c8a55ff13d0d245cf",
       "style": "IPY_MODEL_524f05bbb40a46b6adb9c894201c13fe",
       "value": "Insight into backpropagation-based approaches for visual explanation in AI-based liver cancer analysis."
      }
     },
     "17b06b101ad146cd90e7626deee1b047": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_56fa6ef96dca47248fcbb5a2885b4ec0",
       "style": "IPY_MODEL_5034d1fd58d74644b550089d09498597",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "17dd6c7618c6438f9d22cb349e69539f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_14e04506bd274353a6d18c56d2a21880",
       "style": "IPY_MODEL_218c8e51bb1c408d87a896b6efc2e5da",
       "tooltip": "Retrieve related references"
      }
     },
     "17e36b30b3824aff8acf2e7a4215b32e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bdfe669853b543599b52968afef029e3",
        "IPY_MODEL_87ce71ea5bdf405488617217c47a840e"
       ],
       "layout": "IPY_MODEL_4f4a81bc6fb743528067093b7c4c362a"
      }
     },
     "180620e441c044a9bed20459fc22b38a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_d8d0a703038c49fda7b941813440173a",
       "style": "IPY_MODEL_989cd17860ba4f078b87bdc028d9e27b",
       "tooltip": "Retrieve related references"
      }
     },
     "186750046f19408ba0577c9d9468c2a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4cb0368ad8c44e17898ed61475490643",
        "IPY_MODEL_a344881ccdc649b5b812784f36aa4ecc"
       ],
       "layout": "IPY_MODEL_663b603bde9641d89b897fb36515e1b9"
      }
     },
     "187e28bb4e8d4f929e11c04d273e7c62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_80d349f84a424b33abb1470e84ddd747",
       "placeholder": "",
       "style": "IPY_MODEL_4a6517bfdc6446bdb1546febb1f22885"
      }
     },
     "189f4216e4294ded9a040092bddc414a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "18b6b5b9cb7d4d92b2684705b4dfc004": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "display": "none",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "18f9006f98fa412cbfb4d907d1b9bbaa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "18fab915fde544cbbb95f41c6edf7e82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_87c66515e17648f4b0183c4b9fc013d6",
       "style": "IPY_MODEL_f0d9675d1b244ed795dbc3c6a6122c0f",
       "value": "queries,"
      }
     },
     "19145ed162844bdfad6e74a019f65ea3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "192d8eff5b21460e8021d6a84a350b7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_67fa6afacace435aadfb8b4b670ff1d4",
       "style": "IPY_MODEL_c8ac3dbea4dc40638424daa712a651ae",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "1949dcc398f84ac4aa34d484edee023e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8cfd335851a44830b7f61172c919a17c",
        "IPY_MODEL_1bd39f27338f4d4aa0148498a4e7d0ff",
        "IPY_MODEL_e2170c5149164c96a5f82ab174b0cbcb"
       ],
       "layout": "IPY_MODEL_cc64820d557a46cb801943e85ce22a7e"
      }
     },
     "194b2b8cf4dc4965a524aa93531357d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "195d9e52cc2e407897d7138afdb7ac40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "19a3642b6c3f4209a7ac5a3ee55ef5cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "19d3108656964a04868a81e0bdc75a9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_40d8eed5417f4b2e83f5adf7f6bb5fea",
       "style": "IPY_MODEL_701e5ef75c3e4cc39126808da5c45fdd",
       "value": " to "
      }
     },
     "19dcb228d732487a862a32e36b6b6d90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_79a8fc5c6cdd416680e1846f7870160c",
       "style": "IPY_MODEL_62dacf9509a3413c9b41608fc38e8159"
      }
     },
     "1a2f5c2ad61d44679b53a3b76099e2e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1a439b02ed4849e98e3b04fdcfc63b9b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1a93d0dc7b214d888347796e0e6d6c5d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "1acc435fe085486a95e3aedb7be9a7ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "1ad71bb1050b452fb266a4249f8d6694": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "1b02abbf90b1455f84dcbc811101e969": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "1b08e623febd451cb5451c4df7845106": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "1b3488ba282b4f1e88be49ab00647ebb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1b5572ae65b94567b8a0c421cc9678cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_88af4473af2f42e2816c9c03b28fe084",
       "style": "IPY_MODEL_7ad66f96529749b69adccaa8c8c0bcc7",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "1b82a29b61f242309d0306644b173409": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1bbc8dc0d0724150ac9395264e66ac0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "1bd39f27338f4d4aa0148498a4e7d0ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_f94216d4ee254fc1bced67491fe482b8",
       "placeholder": "",
       "style": "IPY_MODEL_b642f52ece994add9bce0bfe4ea3b471",
       "value": "500"
      }
     },
     "1bf4775372f4433d88d8924d8d60736e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c2d69d77aa147e7b77e4c3700cc4649": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_164b4e84d339442788599f9eac06057b",
       "style": "IPY_MODEL_5aefb9f3db754ee29064da6a3f773902",
       "tooltip": "Retrieve related references"
      }
     },
     "1c338ad3b1634b648eaee4f57ac68a1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_c01285c1c274460da50b09fa8a0e54a8",
       "style": "IPY_MODEL_30c180c43b624ceca2c0d07a4dde00ee",
       "tooltip": "Retrieve related references"
      }
     },
     "1c75599849104efa8cbe6b67ef2eac51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "1c7d12f6bd484820b537f3215fb91551": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c89cc1c10624d68961688d1d1e67b9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c9c3ab952aa42cb99f41adc336f6b65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "1cbbbc41ee264b94b836909e49cc1ed2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7c3cf2518002447fa3fb5ef3af1da338",
       "style": "IPY_MODEL_07456fb1d30241c88dc1a26fd76cb786"
      }
     },
     "1d860d05dc8f4adaaec0a6505a34287e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "1da1d6f7bfe14ce29ee94bbbaa1ccb5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6f9ab7df715b4d41a42f54d42fd73d44",
       "style": "IPY_MODEL_9d2d980577564ab29190b03ddcab0984",
       "value": " to "
      }
     },
     "1dcc170a3cda487f963107832550274f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5cee6420603c441481d48f0d2de454d2",
       "style": "IPY_MODEL_a2805669f40546c6bfffc105244dd6b2",
       "value": "words:"
      }
     },
     "1dcee06f2dce46b58f3db50899c40690": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "1de826bcc02141b084a52bf669141bf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_47d3baabb74a4f7bb31f8c0c000a10e8",
       "style": "IPY_MODEL_8cb91bfa4d7a44e495600aece5b6240e",
       "value": "queries,"
      }
     },
     "1e34d12cadf64241b544fb1ab5ccb1f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1f16904bd51b4e0eba50fac4e8242ce2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "1fe4a9a2306349d3842fcfe0258a038c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_f4bc48b06dae4a56b6267b535efd9813",
       "placeholder": "",
       "style": "IPY_MODEL_993e72482d724921a4eec1ab6812c1ff"
      }
     },
     "1ff4573f9968423692866165feab6c9f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "201ee6145ae2408495899be455562164": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "20df985d44fb485ab2edb94654581bc1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "20fbff9590dc40edb8c6ae23bd61fc34": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_53108e5077df415085e5664c58d979e2"
      }
     },
     "21113547ba354201ad73aea3b1875fc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b065da259c6d42b5bb3a9191344bf2fc",
        "IPY_MODEL_bb9a72c2ea5e4ec38afeadb44f95307b"
       ],
       "layout": "IPY_MODEL_c3ca9851b1884b2c8077901072df6698"
      }
     },
     "2111dd25f6424c778ecd9e9c4b3f9a8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "213a99b585104477af1d2a4b21141190": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b5a91ab8edf44141a84b5cd222bf255f",
       "style": "IPY_MODEL_23adad181bef46afadc623e1aae8e905",
       "value": "words:"
      }
     },
     "2152a561b80442e89687e3fb5bf3e749": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_17afcc4088b34d9c8036209f89af82dc",
        "IPY_MODEL_6245c1cc6f8f4729b3f5531f66c30a15"
       ],
       "layout": "IPY_MODEL_96a9b8a9024e462a9708d4dca3a58e6a"
      }
     },
     "218c8e51bb1c408d87a896b6efc2e5da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "218dbb4deee541e99ff872248ad960e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "21999ae90f8543389b23f5a735ff9a89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "21b42787f0774b9a9746a20684968175": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "21c567080bbc498b8d3b44f9c14226e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c36fd5a559354cb0bbd596a0bc92fb59",
        "IPY_MODEL_aa1abf754f54481ca06bf6ca630f23bb"
       ],
       "layout": "IPY_MODEL_b926ffbab3664f45b171d72d12aa6fb4"
      }
     },
     "223f3d4b1b1348518439ac22c3da3399": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_88af4473af2f42e2816c9c03b28fe084",
       "style": "IPY_MODEL_f57709a9953840a18ca55f7f52cf14a9",
       "tooltip": "Retrieve related references"
      }
     },
     "226057af62724dafb0706c79ba4e78aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "22837da3bdde45b0aab15a3b8c04d49f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2288250ebaf34a03b359031b92e1e943": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_92eab35ac7fc4c8bb248b064dab8916f",
       "placeholder": "",
       "style": "IPY_MODEL_1298a46f4b7d4f29b23df2760d5958f9",
       "value": "500"
      }
     },
     "22a91a5509ff48c280541b8dea86d04b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_9f37c73d13f540ca805d0aa0c2a1e4a7",
       "placeholder": "",
       "style": "IPY_MODEL_c592e75539d846d8ac99fa8d5afb791b"
      }
     },
     "22bae548c0214a329c27f2dc0c820389": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3886a115ac894abaa823e7ecbc4b4368",
        "IPY_MODEL_0b41f4586a06404896ab6a19638ae578"
       ],
       "layout": "IPY_MODEL_595541ae312d43158c43384817a0d361"
      }
     },
     "22e24e3a952e47c4bcaa27ef6822dc03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_598568449f9e4cdfb688bad8c568c641",
       "placeholder": "",
       "style": "IPY_MODEL_dc9e5c5d9ba04caea0e7f954ac602a56"
      }
     },
     "23257a9df8ed473cad690f3037be5ed6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "233121d2ebd14ab08f9c0af5820169dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bdef0015671645a4ac541ef76728e5c7",
       "style": "IPY_MODEL_9a4a066304a04042983fdfd20bf85175",
       "value": " to "
      }
     },
     "236c957b787d476f85995cc9cbe44bd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "238e4a3dcde54ab99b8f07d9ca089a98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "2391a92c08be4f59a6e4011031abe0df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "23adad181bef46afadc623e1aae8e905": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2431f981398746c499b382f12693260e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "244cb21f24fe4ad8946dd01b0d8477e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "2457c7216d9440cb8526b8b4ec0462e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "24674b33400046c68c65e0d99767624e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "248c9ad2645d48a38185a213319087dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_326f20f960344f4b859ec1ae4f540a2a",
       "style": "IPY_MODEL_341c1357ed314e3bbeae19d0ec7d3b6f",
       "value": "queries,"
      }
     },
     "2534b6fec4244ed7895ae18bd4d65f3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2583560af88f4d769b68a80b387df735": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_e6980b87c9b6418a890f43ffc01965ac"
      }
     },
     "261325b862c0468581723bc2e0026bb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_17dd6c7618c6438f9d22cb349e69539f",
        "IPY_MODEL_8384935c3d684ddc9f2da87cb7a50cbb"
       ],
       "layout": "IPY_MODEL_f116a9bd3db24418851f7a1a5d251b8e"
      }
     },
     "262f866383af459aa05b16f7e9fa4cf8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_309c6cc89adf46e383308a9159eec215"
       ],
       "layout": "IPY_MODEL_f5ec7cfb0c4c4dc2a5acc8acdb861804"
      }
     },
     "2655858f11b44996b6e9785b7705781f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_d16cb108a21442aba8b26952501a03ad",
       "placeholder": "20",
       "style": "IPY_MODEL_0c3d99ce97734e61ae987c231c7b1152",
       "value": "20"
      }
     },
     "2671bfe6e5b74c79b48eac9f30c54e58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5b382827917445a6a7fb4193dd6bbf23",
       "style": "IPY_MODEL_5f6c534340fe4bd9904177aac086e2b6"
      }
     },
     "26a384dfe9f048dc91bd0de948341341": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_acae1f52f42c4e4c8475827c52c60045",
       "style": "IPY_MODEL_1b82a29b61f242309d0306644b173409",
       "value": "queries,"
      }
     },
     "26bd11907b6c4238a9966c77aea637fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "26ea10d872304fba92cf710bab4ca820": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "279602c0e3004ed1b75927c7fda924f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "27b05885388845b9935640726138a08c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "27c31f332df94feebc843774c0f6b228": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "27d0890347504c618d60e635275f53b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_c01285c1c274460da50b09fa8a0e54a8",
       "style": "IPY_MODEL_0f7eeceee7de49eca560692a3f1ab621",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "27fd710b2f4e41bd9dc7039f0b69472a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "280b888d8a7044c2918c83c917b99e4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "281c4bd01b0b479d83f2a2aa5b3ea1b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_14092ecef0914a1296fa78a8e98eb0d2",
        "IPY_MODEL_4e79953a6d6c4a08bba50c65b16d2e23"
       ],
       "layout": "IPY_MODEL_882c2dfe3b8942a1a2a04fd0cb548974"
      }
     },
     "286cf6a621514cdda5b91c4ab782f520": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "28ed839b291c4979b2f87cc83b8bb903": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_48d1d0b6407c4f7d8d381cd092c9e9e8",
        "IPY_MODEL_eef3512720ae44a3ab8f1592c1c62a13"
       ],
       "layout": "IPY_MODEL_f600a734e9984ff490f7c9a8e5c0d668"
      }
     },
     "296dfc1185604fdd90e8b0867e5aaf83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_aaffe5746100410288def54a80137ab7",
        "IPY_MODEL_ea6c38dac39b4edbaacd75e931cd2566"
       ],
       "layout": "IPY_MODEL_71ff6ac4c2db4fc2a7098f663072f5ab"
      }
     },
     "29d392f71c32400ca31706ef328807a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_d65418de9605423895928d09a6b6ec64",
       "style": "IPY_MODEL_643fcd58a558456a8d4a4e1bbb561a1f",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "2a33cb2627e54eae9fb2b32b4969e369": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_43c2f645492549118212640aa1091070",
       "style": "IPY_MODEL_a40e5b3ef3aa4ac1bc9b8fc3b1ae6cfd",
       "value": "Final thoughts on the current state of AI in liver cancer analysis and potential future developments in the field."
      }
     },
     "2a38bf4720084ac88066f03acc8717b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2a6435d20e7f4a8fb112e0d55b9c1c8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2a9a17910bea46f2a36148a6b479886a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2ab39325b0bb4ca085f8869654f67f5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_dd639006f3ed459298a66f6b1e404f0c",
        "IPY_MODEL_ce2b4b108ef54fc98745a16b0bd5db67"
       ],
       "layout": "IPY_MODEL_578e256a4ead4c08bd14684cd13b15b2"
      }
     },
     "2ae26bad3e1b400cb2690dc130cdfcb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "2b13f59935224b65b4d59fd60db06898": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b9df31605195437d9dd6e989afd8c148",
       "style": "IPY_MODEL_d2d207a872944e6f926a715238b88735",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.6512623);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.65\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 1.0\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.41923094);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.42\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.13860029);\n                color: white\n                ' title='According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in '>\n                            Page 2, Region 13,\n                            Score 0.14\n                        </summary>\n                        According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.52251196);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.52\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7903731);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.79\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.41907883);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.42\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.11706248);\n                color: white\n                ' title='In the cross-analysis of data types and diseases, we found that biopsy was used as an important data type in studies of AI in liver fibrosis. This is mainly because the histopathological examination of liver biopsy is still the gold standard for the diagnosis of liver fibrosis (45). Conventional CT/MRI examinations can observe morphological changes of the liver; however, quantitative assessment of early-stage liver fibrosis is still difficult and is therefore less used. Although ultrasound elastography and magnetic resonance elastography (MRE) are highly effective non-invasive assessment methods in the diagnosis '>\n                            Page 8, Region 5,\n                            Score 0.12\n                        </summary>\n                        In the cross-analysis of data types and diseases, we found that biopsy was used as an important data type in studies of AI in liver fibrosis. This is mainly because the histopathological examination of liver biopsy is still the gold standard for the diagnosis of liver fibrosis (45). Conventional CT/MRI examinations can observe morphological changes of the liver; however, quantitative assessment of early-stage liver fibrosis is still difficult and is therefore less used. Although ultrasound elastography and magnetic resonance elastography (MRE) are highly effective non-invasive assessment methods in the diagnosis \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.31651425);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.32\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.43948013);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.44\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.9297762);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.93\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.16289139);\n                color: white\n                ' title='Chen et al. 2019 proposed to use typical examples as explanation (i.e., prototypes), which they described as ‘this-looks-likethat’. The method reflected case-based reasoning that humans perform. For example, when a person explains why a picture contains a car, they can internally reason that this is a car because it looks like a car they have seen before. A prototype layer was added to the neural network, which grouped training inputs according to their classes in the latent space. A prototype was picked for each class, consisting of a typical example of that class. During testing, the method utilized parts of the test image that resembled these trained prototypes. The output was a weighted combination of the similarities to these prototypes. Hence, the explanation was an actual computation of the neural network, not a post hoc approximation. '>\n                            Page 11, Region 3,\n                            Score 0.16\n                        </summary>\n                        Chen et al. 2019 proposed to use typical examples as explanation (i.e., prototypes), which they described as ‘this-looks-likethat’. The method reflected case-based reasoning that humans perform. For example, when a person explains why a picture contains a car, they can internally reason that this is a car because it looks like a car they have seen before. A prototype layer was added to the neural network, which grouped training inputs according to their classes in the latent space. A prototype was picked for each class, consisting of a typical example of that class. During testing, the method utilized parts of the test image that resembled these trained prototypes. The output was a weighted combination of the similarities to these prototypes. Hence, the explanation was an actual computation of the neural network, not a post hoc approximation. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5457633);\n                color: white\n                ' title='Uehara et al. (2019) used prototypes to explain why a neural network classified patches of histology images as cancer or as notcancer. The network was able to identify on which parts of the image it based its decision, and to what extent these parts of the image were similar to prototypical examples learned from the training set. '>\n                            Page 11, Region 4,\n                            Score 0.55\n                        </summary>\n                        Uehara et al. (2019) used prototypes to explain why a neural network classified patches of histology images as cancer or as notcancer. The network was able to identify on which parts of the image it based its decision, and to what extent these parts of the image were similar to prototypical examples learned from the training set. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.20963478);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.21\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.049070913);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.05\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.14268479);\n                color: white\n                ' title='Decision making in clinical routine is rarely based on a single data modality. Usually, healthcare providers integrate a number of different data types into clinical decisions. This is especially true in hepatology - a field in which it is rare for diseases to be directly observed and the differential diagnosis can be uncertain. For example, one of the most common hepatology consults is an incidental finding of elevated liver enzymes. Diagnosing the aetiology of this abnormality requires a battery of tests, including detailed clinical history, additional laboratory tests, ultrasound, and even histopathology. Supporting, and ultimately mimicking, human decision making in such complex tasks is currently out of reach for narrow and specialised AI systems. At present, different AI approaches are required to process various types of clinical '>\n                            Page 8, Region 3,\n                            Score 0.14\n                        </summary>\n                        Decision making in clinical routine is rarely based on a single data modality. Usually, healthcare providers integrate a number of different data types into clinical decisions. This is especially true in hepatology - a field in which it is rare for diseases to be directly observed and the differential diagnosis can be uncertain. For example, one of the most common hepatology consults is an incidental finding of elevated liver enzymes. Diagnosing the aetiology of this abnormality requires a battery of tests, including detailed clinical history, additional laboratory tests, ultrasound, and even histopathology. Supporting, and ultimately mimicking, human decision making in such complex tasks is currently out of reach for narrow and specialised AI systems. At present, different AI approaches are required to process various types of clinical \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.009156636);\n                color: white\n                ' title='Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) '>\n                            Page 1, Region 4,\n                            Score 0.01\n                        </summary>\n                        Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2775675);\n                color: white\n                ' title='n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) '>\n                            Page 1, Region 5,\n                            Score 0.28\n                        </summary>\n                        n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.011752544);\n                color: white\n                ' title='Viral hepatitis is a significant cause of CLD. Liver fibrosis and CLD are risk factors for hepatocellular carcinoma (HCC) and hence death. It is practically impossible to perform a liver biopsy in all patients; hence AI algorithms have been developed for non-invasive evaluation of liver fibrosis. Some of the studies done using AI algorithms will be mentioned in the following sections. Wang D. et al'° proposed a bayesian learning algorithm to develop a three-layer artificial neural network (ANN) in patients with CHB. Age, platelet count, aspartate aminotransferase (AST), alanine aminotransferase (ALT), and gammaglutamyl transferase (GGTP) were the most critical factors '>\n                            Page 4, Region 8,\n                            Score 0.01\n                        </summary>\n                        Viral hepatitis is a significant cause of CLD. Liver fibrosis and CLD are risk factors for hepatocellular carcinoma (HCC) and hence death. It is practically impossible to perform a liver biopsy in all patients; hence AI algorithms have been developed for non-invasive evaluation of liver fibrosis. Some of the studies done using AI algorithms will be mentioned in the following sections. Wang D. et al'° proposed a bayesian learning algorithm to develop a three-layer artificial neural network (ANN) in patients with CHB. Age, platelet count, aspartate aminotransferase (AST), alanine aminotransferase (ALT), and gammaglutamyl transferase (GGTP) were the most critical factors \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.0\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "2b42733244a84ec2b99fc0cb51cd7265": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2b4adbf1321342ed8090297f74a0705a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "2b6ad6fa778646b199de5bd0f5f49ba3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2b89e81a9a4e405a9fa3eaa7f7d0d9aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "2bbfda8ed4244bb8ac1401244513754a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "2be64f8e92d64a4fb3ae51275afafb27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "2c2ff66a7ed245d58378b4fa4a333191": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2c5556b3ec4b44a9a2eed8e0eae19074": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2c593427fbb944dda492f9a33bc0cd09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "2c889466d0c34a74a5bf260b9069682f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "2c8de77c705f424e9b529fbbe5a571e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_efcea57926e244589e664b2db0da7be9",
       "placeholder": "",
       "style": "IPY_MODEL_3d530826a5534c01af6476231645028d"
      }
     },
     "2c95f04e7b1e4873a9878b41853d5b92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_c31a28229a8e4dc1a37612bff1d22ba3",
       "style": "IPY_MODEL_f7b8fdc6cfc9457ebdd5ece8cf0ad97f",
       "value": "3.1.2: Supervised learning vs. weakly supervised learning vs. unsupervised learning"
      }
     },
     "2c96c71c83e84f3d9644263c0f2c30e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "2cfe5abc0da8460290c10e8b33fb36b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "2d647745b3a7494086728ac0d0206c43": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "2da7aececd534d62ad9869b641c83e20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_86c77b512ae04a09803ca12d1914a1b1"
       ],
       "layout": "IPY_MODEL_f0b2bac40e2c448282a5c4f9c5dabef3"
      }
     },
     "2dae6f856c63448e96fb0ef559d1cee0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "2db861e9d2cc48b382835a79e0341447": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e7b62835cddc4a298e46a977b4f5a0de",
        "IPY_MODEL_d087f20b19f04dca80c0f3fe78ffa252"
       ],
       "layout": "IPY_MODEL_6ff6b6b4cec84b71ad58e78d64359e20"
      }
     },
     "2e0453f834b841628cd141850571f337": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "2e3f72b6dd314109b184563eea84be8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "2e49e40320c3498c936fdd121ea998bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "2eeff19b07324c4aa8cf334610ed61ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ce71b7bd16484a60ae6b0ccb435bada0",
       "style": "IPY_MODEL_dd8a625922e245f491d68a2faed704fa"
      }
     },
     "2f250540652c480fae6f99502ad6019b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "2f310150628b48dea4db6cf96512ebc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_8ec035b5b6ab47a08bc6adf724b34a0d",
       "style": "IPY_MODEL_c8b79be7efe2450cbe293e880cab90e6",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "2f3d62387c4a47bc80b6af09eb311e8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b83f0a53d99c4b8991f760f67e643467",
        "IPY_MODEL_afc5b5df60ff4922bc54d90134857f91"
       ],
       "layout": "IPY_MODEL_9d2835c9330d49d6b298e69cb6a7f0f4"
      }
     },
     "2fc2b5efe8574da68ecda5ac68b70ffe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "2fed2e63ae5b4df085764356d6b90883": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "300c727d8ce9461aa3e2fa4e198513c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_cf301cf547214704aba560e219fc5be0",
        "IPY_MODEL_87afaeb9c6f3420db2acf5c07adec03a"
       ],
       "layout": "IPY_MODEL_09d234d5778a4208b2e0410deeb0e129"
      }
     },
     "30228f53b94e4dcf958cbc5da158d091": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "305da98f2e014aca8249b7a577ec8981": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d8dd6d655a9a48cf8994d778bc67238a",
        "IPY_MODEL_7d62e300347340748634aecab90799e9"
       ],
       "layout": "IPY_MODEL_7c9761a6de144acb9a595e840d34b0c4"
      }
     },
     "309c6cc89adf46e383308a9159eec215": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3958943c08ef4189ad019c70a7a29f8c",
        "IPY_MODEL_3fcc9880c080444abc427e338afb553c"
       ],
       "layout": "IPY_MODEL_1739d827889445838956b95852f31afe"
      }
     },
     "309e2afa2d1b40d381a5d638ea298c72": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "display": "none",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "30a16f50c11340a9a9be5a0898b2399c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "30aabef282004dca9eb276e09b577269": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "30c180c43b624ceca2c0d07a4dde00ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "30e4cccc81f54c5fa4cb9ad1fae53a6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "31291063fea34bad8f59aba0e2780671": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_12baa644f3a841f2a6f6d02d99320384",
        "IPY_MODEL_0c474a41ba9848ee8c7971d34ddbd794"
       ],
       "layout": "IPY_MODEL_45fe3c2f372d45d68b5184699a117a85"
      }
     },
     "314a018c20624ed2917b66bafe98a513": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_88af4473af2f42e2816c9c03b28fe084",
       "style": "IPY_MODEL_3c1b579a82054159acad17c330d9161b",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "314eacb8e4234a549422747ba873209b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a49a9a23dd0948f6a99630d8da5289dd",
       "style": "IPY_MODEL_c7717daa5b804c2981c8d097eb6da18b"
      }
     },
     "316ddae8f0714009adfe1f65aa3a6dfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_156b9ad114a7479aae26a226ffec19be",
       "style": "IPY_MODEL_6b73fc919adf4728a15b61d5a6f2c07b",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.992309);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.99\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.25189713);\n                color: white\n                ' title='Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) '>\n                            Page 1, Region 4,\n                            Score 0.25\n                        </summary>\n                        Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.040538207);\n                color: white\n                ' title='Various Al-based applications and models, developed using clinical, laboratory and radiology data, play an important role in diagnosis, prediction of severity and prognostication of liver diseases (Figure 3). '>\n                            Page 8, Region 5,\n                            Score 0.04\n                        </summary>\n                        Various Al-based applications and models, developed using clinical, laboratory and radiology data, play an important role in diagnosis, prediction of severity and prognostication of liver diseases (Figure 3). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 1.0\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.6225731);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.62\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5363744);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.54\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.39473617);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.39\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.02789176);\n                color: white\n                ' title='According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in '>\n                            Page 2, Region 13,\n                            Score 0.03\n                        </summary>\n                        According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.06039511);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.06\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.544107);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.54\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.56462485);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.56\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.62258166);\n                color: white\n                ' title='Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. '>\n                            Page 8, Region 8,\n                            Score 0.62\n                        </summary>\n                        Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.067145266);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.07\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.83244497);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.83\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5272159);\n                color: white\n                ' title='Patients with liver disease, particularly those with liver cancer, undergo multiple imaging studies to establish a diagnosis, preoperatively plan interventions, and monitor response to therapy (Table S2). Each of these imaging studies contain numerous data points that could be potentially analysed to improve predictions. However, there is a formidable challenge in transforming this burden of clinical and imaging data into something of clinical value. '>\n                            Page 4, Region 15,\n                            Score 0.53\n                        </summary>\n                        Patients with liver disease, particularly those with liver cancer, undergo multiple imaging studies to establish a diagnosis, preoperatively plan interventions, and monitor response to therapy (Table S2). Each of these imaging studies contain numerous data points that could be potentially analysed to improve predictions. However, there is a formidable challenge in transforming this burden of clinical and imaging data into something of clinical value. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2513945);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.25\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5360997);\n                color: white\n                ' title='Owing to the broad heterogeneity in HCC risk factors and pathogenesis, established strategies for prediction and prognostication are still limited. Recently, artificial intelligence (AI) has emerged as a unique opportunity to improve the full spectrum of HCC clinical care, by: i) improving the prediction of future HCC risk in patients with established liver disease; ii) improving the accuracy of HCC '>\n                            Page 1, Region 12,\n                            Score 0.54\n                        </summary>\n                        Owing to the broad heterogeneity in HCC risk factors and pathogenesis, established strategies for prediction and prognostication are still limited. Recently, artificial intelligence (AI) has emerged as a unique opportunity to improve the full spectrum of HCC clinical care, by: i) improving the prediction of future HCC risk in patients with established liver disease; ii) improving the accuracy of HCC \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.046215903);\n                color: white\n                ' title='Due to the broad heterogeneity in risk factors for HCC and the lack of established strategies for prediction or prognostication, AI has recently emerged as a unique opportunity to improve the full spectrum of HCC clinical care. '>\n                            Page 2, Region 8,\n                            Score 0.05\n                        </summary>\n                        Due to the broad heterogeneity in risk factors for HCC and the lack of established strategies for prediction or prognostication, AI has recently emerged as a unique opportunity to improve the full spectrum of HCC clinical care. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='It has been posited that improved HCC risk prediction models leveraging AI techniques could be used to personalise HCC surveillance strategies by improving risk stratification of patients with chronic liver disease. For example, Ioannou and colleagues found that targeting patients with the uppermost 51% of their NN-derived HCC risk score would include 80% of patients who would develop HCC within the subsequent 3 years.° Such an approach could be useful in resource-limited settings that do not have sufficient capacity for regular HCC surveillance in all at-risk patients. However, to date, the clinical utility of this and other Al-based scores for predicting risk of HCC is unclear, particularly as these data have limited generalisability, given their reliance on the size and diversity of the training dataset. '>\n                            Page 3, Region 5,\n                            Score 0.0\n                        </summary>\n                        It has been posited that improved HCC risk prediction models leveraging AI techniques could be used to personalise HCC surveillance strategies by improving risk stratification of patients with chronic liver disease. For example, Ioannou and colleagues found that targeting patients with the uppermost 51% of their NN-derived HCC risk score would include 80% of patients who would develop HCC within the subsequent 3 years.° Such an approach could be useful in resource-limited settings that do not have sufficient capacity for regular HCC surveillance in all at-risk patients. However, to date, the clinical utility of this and other Al-based scores for predicting risk of HCC is unclear, particularly as these data have limited generalisability, given their reliance on the size and diversity of the training dataset. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Deep learning in hepatocellular carcinoma Current status and future perspectives</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.10073385);\n                color: white\n                ' title='Hepatocellular carcinoma (HCC) is among the leading causes of cancer incidence and death. Despite decades of research and development of new treatment options, the overall outcomes of patients with HCC continue to remain poor. There are areas of unmet need in risk prediction, early diagnosis, accurate prognostication, and individualized treatments for patients with HCC. Recent years have seen an explosive growth in the application of artificial intelligence (AI) technology in medical research, with the field of HCC being no exception. Among the various AI-based machine learning algorithms, deep learning algorithms are considered state-of-the-art techniques for handling and processing complex multimodal data ranging from routine clinical variables to high-resolution medical images. This article will provide a comprehensive review of the recently published studies that have applied deep learning for risk prediction, diagnosis, prognostication, and treatment planning for patients with HCC. '>\n                            Page 1, Region 14,\n                            Score 0.1\n                        </summary>\n                        Hepatocellular carcinoma (HCC) is among the leading causes of cancer incidence and death. Despite decades of research and development of new treatment options, the overall outcomes of patients with HCC continue to remain poor. There are areas of unmet need in risk prediction, early diagnosis, accurate prognostication, and individualized treatments for patients with HCC. Recent years have seen an explosive growth in the application of artificial intelligence (AI) technology in medical research, with the field of HCC being no exception. Among the various AI-based machine learning algorithms, deep learning algorithms are considered state-of-the-art techniques for handling and processing complex multimodal data ranging from routine clinical variables to high-resolution medical images. This article will provide a comprehensive review of the recently published studies that have applied deep learning for risk prediction, diagnosis, prognostication, and treatment planning for patients with HCC. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "318b610160e14902a18713f822b08b7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_70f71695bd294ce0abde65747ce0058b",
        "IPY_MODEL_8c913ad1f9f94762b7c09b21a0fec481"
       ],
       "layout": "IPY_MODEL_cb5773adfb144ae2ac5d93fc0f7ce50b"
      }
     },
     "318b9501a0394080a4c3c95fc63b52b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "319e47decfc2443da090149db53bc7d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2b13f59935224b65b4d59fd60db06898"
       ],
       "layout": "IPY_MODEL_77a0cb45b0904e179a13f21bdd820fef"
      }
     },
     "31c81c4d1c1b47e28e46f3add061fc0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b2d93eb490964f1b8e09357f3e759c0e",
        "IPY_MODEL_1949dcc398f84ac4aa34d484edee023e"
       ],
       "layout": "IPY_MODEL_545e66fe985b421abc7d7d8461f85423"
      }
     },
     "324e020f69fd47b394503a7e2342a24b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "326f20f960344f4b859ec1ae4f540a2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "328e6c7985d7490e8acc5e6fb02ec5ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2a6435d20e7f4a8fb112e0d55b9c1c8d",
       "style": "IPY_MODEL_5822485942aa4d48ac90388f0ccd697d"
      }
     },
     "329fd034917840b197e88d37c91eb321": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_3894445a14ef4125aa3cb17253c194bd"
      }
     },
     "32bdfbca04814e28815bbb5a00a03979": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_1f16904bd51b4e0eba50fac4e8242ce2",
       "placeholder": "20",
       "style": "IPY_MODEL_fd8237afc74f452a9575803f8624b16a",
       "value": "20"
      }
     },
     "32ec22ba6f2f4e98b465ca947dfa6ea0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ea5b1e11162b470da532a4a1f850e307",
        "IPY_MODEL_d7ed3d813b304f0e96e6a447ccc73134"
       ],
       "layout": "IPY_MODEL_43bd53004fcc4457be5b60dc5f039d89"
      }
     },
     "32ed49e68149459bbf63214adf30c6e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "33238ef0a70e4d458c877621872f11d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "33578c858bd54c1fa3e24fa9bda8b10d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "341c1357ed314e3bbeae19d0ec7d3b6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "34234a7e95f345d9a1732845c1741c4b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "343311a0ddaf4640ab1cc6bfa2620f1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "34688c69670f460585cf3a881cd59c92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "34a9736c7d784afda81f6556c854004f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "34bfecdc947d4202b522cbfd267ee6ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "34d0f747f116473da75122f8f9ebba64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_195d9e52cc2e407897d7138afdb7ac40",
       "style": "IPY_MODEL_7a156ccc7e194a07b515bdbd95fec6d7",
       "value": "queries,"
      }
     },
     "3518211d7228477abb2f822a540469c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_9e769fa43b1e444c90b12332985e814e",
       "placeholder": "",
       "style": "IPY_MODEL_787e38b33e674ef9aea90fa6c50f814c"
      }
     },
     "351d672f3402482186a446f38b6b0936": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d0d24538e4534a49a0150cd4d04a7b08",
       "style": "IPY_MODEL_c7013c381705433ba97fbfe6dbabc19d"
      }
     },
     "35491db10f744a4a9f6980defef5fc95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_9e2da0f0005040f984e3a86e35329dde",
        "IPY_MODEL_b345a4eaf2d24cd7b094e3171901f53b"
       ],
       "layout": "IPY_MODEL_b6cea8ad164348e3a579e9471253288f"
      }
     },
     "35d322a8686f4ede8824734fb99579ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "365a8d68668544a9b986b54db7c5ff13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_aa9f6c07522544fcb608d280dcfde4bf",
        "IPY_MODEL_983b962db6b44587b3bca86f134e7f1b"
       ],
       "layout": "IPY_MODEL_d493c9acebbb4c7baa92699f28075d98"
      }
     },
     "366be16e3d3a4378b4178ca68e296e4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "36c8e10a87f54e1f8711cab7a7b9089f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "3711706215654c98be554018cca181cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "373ae0308aa14f16a5a43465d1f28055": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "376a0d0d8de04bcb86e9d8b2664c478c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "37778f57d97648fcbbca108937fce16c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2a33cb2627e54eae9fb2b32b4969e369",
        "IPY_MODEL_2da7aececd534d62ad9869b641c83e20"
       ],
       "layout": "IPY_MODEL_c91ad674340249de8b85eadf723661f2"
      }
     },
     "37a3f3b63ff2404d83b93cdb5bb2cd9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "37e64dd3daf2417cbc89179cc8beab50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "3886a115ac894abaa823e7ecbc4b4368": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_89234edd81214fbf87c536eab1d839b1",
        "IPY_MODEL_17e36b30b3824aff8acf2e7a4215b32e"
       ],
       "layout": "IPY_MODEL_6df7552c452542eb84b8572f65892472"
      }
     },
     "3894445a14ef4125aa3cb17253c194bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "390e53c6536f4a6ebd4e79bf05f8bc45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_152091df89954c9fa800e37ed3dfab62",
        "IPY_MODEL_89ce9e9d2d204dcb8cd3298b79ffab95"
       ],
       "layout": "IPY_MODEL_bfca834b697b46f0bc2884c0c1483cc3"
      }
     },
     "3912faf4baba404991fe73b55b9ea4b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5e97fab890304257a71df4c48befde53",
        "IPY_MODEL_01ebc972134c4fa090df26a63b239dae"
       ],
       "layout": "IPY_MODEL_52dcedb1ffb64386b1674c0539fcddaa"
      }
     },
     "3948255fe051435faf1f8deab77d07bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_4faa7a9a13ec42fba8586be2ecbd5317",
       "style": "IPY_MODEL_1bbc8dc0d0724150ac9395264e66ac0b",
       "value": "Exploration of AI's role in identifying predictive indicators for therapy response in liver cancer treatment."
      }
     },
     "3958943c08ef4189ad019c70a7a29f8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve All",
       "icon": "wikipedia-w",
       "layout": "IPY_MODEL_7b99d280099a4c5e8243461d2fcc0b99",
       "style": "IPY_MODEL_02ecc3c69fdc4603b246191133186f54",
       "tooltip": "Retrieve references for all sections with one click"
      }
     },
     "3996119138314cc9aa0c1e47f65178c6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "39ad1c66d5f146268a514117c0848d09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5c619bccd26a43dd8e97012660002ae9",
       "style": "IPY_MODEL_78620c66a52744f39ed46d09728faaf0",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5123528);\n                color: white\n                ' title='Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) '>\n                            Page 1, Region 4,\n                            Score 0.51\n                        </summary>\n                        Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Deep learning in hepatocellular carcinoma Current status and future perspectives</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.3071749);\n                color: white\n                ' title='accurate predictions about the outcome when provided with a new set of input data [9]. Examples of supervised learning algorithms include traditional techniques such as linear regression and logistic regression, as well as more sophisticated techniques including support vector machines, random forest and gradient boosting. On the other hand, unsupervised learning algorithms train on unlabeled sample data and analyze the underlying structure or distribution within the data to discover new clusters or patterns[10]. Examples of unsupervised learning algorithms include K-means and principle component analysis among many others. '>\n                            Page 3, Region 3,\n                            Score 0.31\n                        </summary>\n                        accurate predictions about the outcome when provided with a new set of input data [9]. Examples of supervised learning algorithms include traditional techniques such as linear regression and logistic regression, as well as more sophisticated techniques including support vector machines, random forest and gradient boosting. On the other hand, unsupervised learning algorithms train on unlabeled sample data and analyze the underlying structure or distribution within the data to discover new clusters or patterns[10]. Examples of unsupervised learning algorithms include K-means and principle component analysis among many others. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Multi-task deep learning network to predict future macrovascular invasion in hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.24937218);\n                color: white\n                ' title='In this regard, deep learning can be informative and helpful. By indepth mining and efficient analysis of data both within and beyond the traditional visual system, deep learning algorithms bring medicine to the data-driven era [15]. Considering hepatology, deep learning has outperformed the traditional shear wave elastography in assessing liver fibrosis [16]. Researchers have also proven that deep learning outperforms conventional machine learning models in differentiating HCC from cirrhotic parenchyma [17]. Moreover, it has promising performance in predicting disease progression and the OS of HCC [18]. Nevertheless, overfitting is a common problem in deep learning algorithms. Multi-task learning was introduced to control overfitting. Achieving positive feedbacks among related tasks can enrich information and increase the accuracy of each task; thereby, improving the performance the overall model [19]. Therefore, we constructed a multi-task deep learning neural network (MTnet) to construct models to predict macrovascular invasion and assist in early intervention. '>\n                            Page 2, Region 11,\n                            Score 0.25\n                        </summary>\n                        In this regard, deep learning can be informative and helpful. By indepth mining and efficient analysis of data both within and beyond the traditional visual system, deep learning algorithms bring medicine to the data-driven era [15]. Considering hepatology, deep learning has outperformed the traditional shear wave elastography in assessing liver fibrosis [16]. Researchers have also proven that deep learning outperforms conventional machine learning models in differentiating HCC from cirrhotic parenchyma [17]. Moreover, it has promising performance in predicting disease progression and the OS of HCC [18]. Nevertheless, overfitting is a common problem in deep learning algorithms. Multi-task learning was introduced to control overfitting. Achieving positive feedbacks among related tasks can enrich information and increase the accuracy of each task; thereby, improving the performance the overall model [19]. Therefore, we constructed a multi-task deep learning neural network (MTnet) to construct models to predict macrovascular invasion and assist in early intervention. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.36272696);\n                color: white\n                ' title='Deep learning algorithms have proven to be advantageous in constructing models for diagnosis and prognosis of cancers, especially for liver diseases [16—18,27]. Meanwhile, among all the types of deep learning algorithms, multi-task learning combines severally related tasks during the training process and these can benefit from each other. Multi-task learning has attracted considerable attention in the field of medical image analysis [28—29]; however, its application in HCC has been limited to microvascular invasion rather than macrovascular invasion [30]. Considering the potential advantages of multi-task learning, we constructed our MTnet to predict macrovascular invasion. '>\n                            Page 8, Region 8,\n                            Score 0.36\n                        </summary>\n                        Deep learning algorithms have proven to be advantageous in constructing models for diagnosis and prognosis of cancers, especially for liver diseases [16—18,27]. Meanwhile, among all the types of deep learning algorithms, multi-task learning combines severally related tasks during the training process and these can benefit from each other. Multi-task learning has attracted considerable attention in the field of medical image analysis [28—29]; however, its application in HCC has been limited to microvascular invasion rather than macrovascular invasion [30]. Considering the potential advantages of multi-task learning, we constructed our MTnet to predict macrovascular invasion. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 1.0\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.51181567);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.51\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.27571622);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.28\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.10662432);\n                color: white\n                ' title='The Web of Science Core Collection (WoSCC), which is a standardized and comprehensive dataset, was used to compile the publication dataset in this study. AI is a branch of computer science and a technology that uses machines to simulate human intelligence. Al in this paper mainly includes traditional machine learning and the most popular deep learning algorithms. Therefore, the searching query string was described as follows: TS = (((liver OR hepatic) NEAR/1 (cancer* OR tumor* OR tumor* OR disease OR lesion* OR carcinoma”)) OR “hepatocellular carcinoma” OR “HCC”) AND TS = (((automated OR intelligent) NEAR/1 (classification OR diagnosis OR segment* OR detect*)) OR “artificial intelligence” OR “deep learning” OR “convolutional neural network*” OR “machine learning” OR “CNNs” OR “artificial neural network*” OR “computer-aided” OR “Bayes* network*” OR “supervised learning” OR “unsupervised clustering” OR “computer-assisted” OR (deep NEAR/1 network*) OR “ensemble learning”). The retrieval was carried out on January 18, 2022. Figure 1 shows the workflow of the retrieval strategy in this research. '>\n                            Page 2, Region 11,\n                            Score 0.11\n                        </summary>\n                        The Web of Science Core Collection (WoSCC), which is a standardized and comprehensive dataset, was used to compile the publication dataset in this study. AI is a branch of computer science and a technology that uses machines to simulate human intelligence. Al in this paper mainly includes traditional machine learning and the most popular deep learning algorithms. Therefore, the searching query string was described as follows: TS = (((liver OR hepatic) NEAR/1 (cancer* OR tumor* OR tumor* OR disease OR lesion* OR carcinoma”)) OR “hepatocellular carcinoma” OR “HCC”) AND TS = (((automated OR intelligent) NEAR/1 (classification OR diagnosis OR segment* OR detect*)) OR “artificial intelligence” OR “deep learning” OR “convolutional neural network*” OR “machine learning” OR “CNNs” OR “artificial neural network*” OR “computer-aided” OR “Bayes* network*” OR “supervised learning” OR “unsupervised clustering” OR “computer-assisted” OR (deep NEAR/1 network*) OR “ensemble learning”). The retrieval was carried out on January 18, 2022. Figure 1 shows the workflow of the retrieval strategy in this research. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.16721416);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.17\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.029501423);\n                color: white\n                ' title='cancer, including HCC, CT was the most used, followed by ultrasound and MRI. In addition, CT was mainly used for the research of liver fibrosis, ultrasound was mainly used for the research of fatty liver disease, and biopsy was mainly used for liver fibrosis research (Figure 8E). The differential diagnosis of HCC, are the key points, followed by the diagnosis of liver cirrhosis, liver fibrosis and fatty liver disease, are key points among the specific diagnosis, classification, and treatment of liver diseases. In terms of the prognosis of liver disease, the prognosis of HCC is a key focus, and the surgical methods for its treatment mainly include radiofrequency ablation and transarterial chemoembolization (Figure 8F). Three quarters of these papers were about diagnosis, classification, segmentation, or prediction, with relatively less attention to prognosis. Moreover, most liver cancer studies used CNNs, with a minority exclusively using more traditional techniques like support vector machine and decision trees (Figure 8D). '>\n                            Page 5, Region 7,\n                            Score 0.03\n                        </summary>\n                        cancer, including HCC, CT was the most used, followed by ultrasound and MRI. In addition, CT was mainly used for the research of liver fibrosis, ultrasound was mainly used for the research of fatty liver disease, and biopsy was mainly used for liver fibrosis research (Figure 8E). The differential diagnosis of HCC, are the key points, followed by the diagnosis of liver cirrhosis, liver fibrosis and fatty liver disease, are key points among the specific diagnosis, classification, and treatment of liver diseases. In terms of the prognosis of liver disease, the prognosis of HCC is a key focus, and the surgical methods for its treatment mainly include radiofrequency ablation and transarterial chemoembolization (Figure 8F). Three quarters of these papers were about diagnosis, classification, segmentation, or prediction, with relatively less attention to prognosis. Moreover, most liver cancer studies used CNNs, with a minority exclusively using more traditional techniques like support vector machine and decision trees (Figure 8D). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.13595851);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.14\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.080693394);\n                color: white\n                ' title='FIGURE 6 A dual-map overlap of journals with studies researching artificial intelligence in liver cancer. '>\n                            Page 7, Region 11,\n                            Score 0.08\n                        </summary>\n                        FIGURE 6 A dual-map overlap of journals with studies researching artificial intelligence in liver cancer. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.20724966);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.21\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7229073);\n                color: white\n                ' title='In terms of methods used, some studies used traditional algorithms (51-53), such as support vector machine and random forests models, which were mainly concentrated in the early research stage. Since 2012, deep learning with CNNs has been widely used in the field, involving common tasks in the field of machine learning '>\n                            Page 8, Region 9,\n                            Score 0.72\n                        </summary>\n                        In terms of methods used, some studies used traditional algorithms (51-53), such as support vector machine and random forests models, which were mainly concentrated in the early research stage. Since 2012, deep learning with CNNs has been widely used in the field, involving common tasks in the field of machine learning \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.0\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.3890831);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.39\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Deep learning for prediction of hepatocellular carcinoma recurrence after resection or liver transplanatation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2522521);\n                color: white\n                ' title='Deep learning-driven methods in medical image processing have proven to be extremely useful in standardizing cancer diagnosis as well as in improving patient stratification [19, 31]. Recently, a pioneering survey reported that deep learning-based models can detect and categorize lung cancer cases with accuracy similar to that of pathologists [20]. Previous studies suggest that deep learning is highly efficient in developing markers, which utilize basic morphology for the prediction of outcomes in cancer patients [32, 33]. A deep learning-based model developed by Coudray et al. could even predict six of the most frequent genetic alterations directly from the slides [20]. In cases of gastrointestinal cancer, a deep learning-based model could directly estimate microsatellite instability based on just histological images [27]. Kather et al. also reported that a CNN could extract the tumor components and predict patient survival directly from histology images [21]. Saillard et al. predicted the survival of HCC patients by extracting features from images using a pre-trained CNN, following which the network selected 25 tiles having maximum and minimum scores to predict survival [22]. In our study, a different method was used to develop the MobileNetV2_HCC_class to improve the prediction of prognosis in HCC patients treated by surgical resection and LT. The innovative features of our method were: (1) random tiles were used for each patient, like Skrede et al. [17], (2) the MobileNet V2 was trained using MIL, which allowed for training on tile collections labeled with the associated whole-slide image, and (3) the use of nuclear architectural information in building of the model, which proved to be efficient for cancer grading and prediction of patient outcomes [24]. Genetic instability was demonstrated through diversifying nuclear shape and texture, which had a major effect on metastasis and proliferation that might lead to cancer recurrence. The MobileNetV2_HCC_class proved to be a strong predictor of RFS in HCC patients treated with resection or LT and generalized in the TCGA set across different centers. '>\n                            Page 8, Region 7,\n                            Score 0.25\n                        </summary>\n                        Deep learning-driven methods in medical image processing have proven to be extremely useful in standardizing cancer diagnosis as well as in improving patient stratification [19, 31]. Recently, a pioneering survey reported that deep learning-based models can detect and categorize lung cancer cases with accuracy similar to that of pathologists [20]. Previous studies suggest that deep learning is highly efficient in developing markers, which utilize basic morphology for the prediction of outcomes in cancer patients [32, 33]. A deep learning-based model developed by Coudray et al. could even predict six of the most frequent genetic alterations directly from the slides [20]. In cases of gastrointestinal cancer, a deep learning-based model could directly estimate microsatellite instability based on just histological images [27]. Kather et al. also reported that a CNN could extract the tumor components and predict patient survival directly from histology images [21]. Saillard et al. predicted the survival of HCC patients by extracting features from images using a pre-trained CNN, following which the network selected 25 tiles having maximum and minimum scores to predict survival [22]. In our study, a different method was used to develop the MobileNetV2_HCC_class to improve the prediction of prognosis in HCC patients treated by surgical resection and LT. The innovative features of our method were: (1) random tiles were used for each patient, like Skrede et al. [17], (2) the MobileNet V2 was trained using MIL, which allowed for training on tile collections labeled with the associated whole-slide image, and (3) the use of nuclear architectural information in building of the model, which proved to be efficient for cancer grading and prediction of patient outcomes [24]. Genetic instability was demonstrated through diversifying nuclear shape and texture, which had a major effect on metastasis and proliferation that might lead to cancer recurrence. The MobileNetV2_HCC_class proved to be a strong predictor of RFS in HCC patients treated with resection or LT and generalized in the TCGA set across different centers. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.16384488);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.16\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>NPJ 2020 Classification and mutation prediction based on</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.14287518);\n                color: white\n                ' title='Pathologists could provide limited information regarding cancer reorganization from normal liver tissue and assess its histopathological grade via visual inspection, but it still lacks the underlying biological differences in HCC gene mutations associated with overall survival. The recent advances in artificial intelligence (Al) provided a novel way to assist clinicians to classify medical information and images'*'’. Recently, Lin et al.'® used multiphoton microscopy with deep learning in the automated classification of HCC differentiation. Furthermore, Li et al.'® combined extreme learning machine with multiple convolutional neural network methods for nuclei grading in HCC. The development of graphics processing units allows the possibility to train a more complex neural network to satisfy the requirement of accomplishing complex visual recognition tasks, such as distinguishing tumors from normal tissue slides and classifying subtypes of tumors”°?'. To the best of our knowledge, a previous study by Coudray et al.’° utilized the deep convolutional neural network on histopathological images to automatically classify the type and subtype of lung tumors. In addition, a promising result for the classification of colorectal?”** and breast tumors~* using deep learning was also reported. Therefore, deep-learning models could be used to assist pathologists to effectively detect gene mutations and cancer subtypes. However, it remains unclear whether deep learning can be applied to solid tumors, especially for HCC. In addition, advances in Al tools in digital pathology have resulted in an increased demand for predictive assays in frozen slides that enable the selection and stratification of patients for additional treatment during surgery”’. '>\n                            Page 1, Region 11,\n                            Score 0.14\n                        </summary>\n                        Pathologists could provide limited information regarding cancer reorganization from normal liver tissue and assess its histopathological grade via visual inspection, but it still lacks the underlying biological differences in HCC gene mutations associated with overall survival. The recent advances in artificial intelligence (Al) provided a novel way to assist clinicians to classify medical information and images'*'’. Recently, Lin et al.'® used multiphoton microscopy with deep learning in the automated classification of HCC differentiation. Furthermore, Li et al.'® combined extreme learning machine with multiple convolutional neural network methods for nuclei grading in HCC. The development of graphics processing units allows the possibility to train a more complex neural network to satisfy the requirement of accomplishing complex visual recognition tasks, such as distinguishing tumors from normal tissue slides and classifying subtypes of tumors”°?'. To the best of our knowledge, a previous study by Coudray et al.’° utilized the deep convolutional neural network on histopathological images to automatically classify the type and subtype of lung tumors. In addition, a promising result for the classification of colorectal?”** and breast tumors~* using deep learning was also reported. Therefore, deep-learning models could be used to assist pathologists to effectively detect gene mutations and cancer subtypes. However, it remains unclear whether deep learning can be applied to solid tumors, especially for HCC. In addition, advances in Al tools in digital pathology have resulted in an increased demand for predictive assays in frozen slides that enable the selection and stratification of patients for additional treatment during surgery”’. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.008530821);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.01\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "39d93033354e4074886a8d1370081931": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1188b6d855a949a984e571b5e6c0a570",
       "style": "IPY_MODEL_1a2f5c2ad61d44679b53a3b76099e2e7",
       "value": "queries,"
      }
     },
     "39dbd90a7c0a402b85c5dba083008b1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "39e696510c81435ea9f64bd33b8af672": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3d20ee3248cb44c495f1b68482ce77f9",
        "IPY_MODEL_7e231c05793f46068454898a18b48f85"
       ],
       "layout": "IPY_MODEL_627b86eb739745bab0e8a200cb65dfc5"
      }
     },
     "3a171fb18bc4439ab505e53ae41482c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "3a664ebc911448c2a101c5eb1fb7b079": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3a928646497d45fda63570ffc392b6a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_cb5a6fa7341e41cd8607eaa046d889df",
        "IPY_MODEL_f1797caaeb2d4d8da66408249ebc4d0f"
       ],
       "layout": "IPY_MODEL_e35117e077324a3fbff5de1863715381"
      }
     },
     "3b218310004b4cec8e82166cfea744f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_ca4c787f343a41018808b53430bc53e8",
       "style": "IPY_MODEL_af945cca68584f9392ca266cd90d57b1",
       "value": "1.2: Molecular profiling of liver cancer via AI"
      }
     },
     "3b34bb1f9f8e426ca41cb8c83188c7c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bba3e0b6888d41a0bdc4e02ee8a58f4e",
        "IPY_MODEL_d663fe5c85b94ee7a14d851d765847b1"
       ],
       "layout": "IPY_MODEL_557d5f66d2d44b3cac23c0151c103acc"
      }
     },
     "3b57cc435c3b43cf819d2a4c69838fd6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "3b59bc009bc740ab9496ad0f1f5c3dee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_795eebb27eac40739547d11e29acfa21",
        "IPY_MODEL_0baa2b74a2054f50811bb2a39576ec70"
       ],
       "layout": "IPY_MODEL_5f9688d2dd354fddaa6d8d67004d0bb9"
      }
     },
     "3b6c1d9c3a4c48c68d86187847701042": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b3ac931200ed41daa616b3524e7df350",
        "IPY_MODEL_d1ea06430e344566affe843dad9e0ead"
       ],
       "layout": "IPY_MODEL_a078d3d8dd6d404e8916c2b30670ad0e"
      }
     },
     "3b6d68ace59145ecb2fa3135e2d02675": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_93b0d5ad2eb1490a829ed6ab345678c1",
        "IPY_MODEL_c29de9e3e6e34b82beb1c91922894b6c"
       ],
       "layout": "IPY_MODEL_20df985d44fb485ab2edb94654581bc1"
      }
     },
     "3bdb8ac271594d2e91bf72ea76599c09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9615ff12db3b434194ba2d87b6d8d3f8",
       "style": "IPY_MODEL_324e020f69fd47b394503a7e2342a24b",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7143146);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.71\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.88481003);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.88\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.590077);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.59\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.099060856);\n                color: white\n                ' title='According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in '>\n                            Page 2, Region 13,\n                            Score 0.1\n                        </summary>\n                        According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.3550833);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.36\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.011849683);\n                color: white\n                ' title='TABLE 2 Top 10 institutes with publications researching the use of artificial intelligence in liver cancer. '>\n                            Page 5, Region 8,\n                            Score 0.01\n                        </summary>\n                        TABLE 2 Top 10 institutes with publications researching the use of artificial intelligence in liver cancer. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.03754294);\n                color: white\n                ' title='TABLE 3. The 10 most productive authors of publications researching the use of artificial intelligence in liver cancer. '>\n                            Page 6, Region 3,\n                            Score 0.04\n                        </summary>\n                        TABLE 3. The 10 most productive authors of publications researching the use of artificial intelligence in liver cancer. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.41772124);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.42\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.52726555);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.53\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. '>\n                            Page 8, Region 6,\n                            Score 0.0\n                        </summary>\n                        of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.14262526);\n                color: white\n                ' title='Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. '>\n                            Page 8, Region 8,\n                            Score 0.14\n                        </summary>\n                        Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.3286395);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.33\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2758103);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.28\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 1.0\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.20789045);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.21\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.16710022);\n                color: white\n                ' title='Owing to the broad heterogeneity in HCC risk factors and pathogenesis, established strategies for prediction and prognostication are still limited. Recently, artificial intelligence (AI) has emerged as a unique opportunity to improve the full spectrum of HCC clinical care, by: i) improving the prediction of future HCC risk in patients with established liver disease; ii) improving the accuracy of HCC '>\n                            Page 1, Region 12,\n                            Score 0.17\n                        </summary>\n                        Owing to the broad heterogeneity in HCC risk factors and pathogenesis, established strategies for prediction and prognostication are still limited. Recently, artificial intelligence (AI) has emerged as a unique opportunity to improve the full spectrum of HCC clinical care, by: i) improving the prediction of future HCC risk in patients with established liver disease; ii) improving the accuracy of HCC \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.00078214135);\n                color: white\n                ' title='Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) '>\n                            Page 1, Region 4,\n                            Score 0.0\n                        </summary>\n                        Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.026537294);\n                color: white\n                ' title='n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) '>\n                            Page 1, Region 5,\n                            Score 0.03\n                        </summary>\n                        n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.4140871);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.41\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.11707821);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.12\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "3c1b579a82054159acad17c330d9161b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "3c74412001224adb971da7f11aebdedb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "3c76b27383eb4125a9e4ddec00c303aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_f80be323676c4511bb462353674171bb",
       "style": "IPY_MODEL_0cf70f6f3a99463bbe8ad5da952744a2",
       "value": "Exploration of visual explanation techniques like saliency mapping and pathologist-in-the-loop in AI-based liver cancer analysis."
      }
     },
     "3c91d979788448e8a58aa5591be4c1c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7b6274e2bfb049888832f419a795f2d4",
       "style": "IPY_MODEL_06b56d27fa63436dbdadb82445518108",
       "value": "queries,"
      }
     },
     "3c9b21d632d2474b8cf04d3d905f85ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_e57c1327f3aa4eaabd1aa47142027d8c",
       "style": "IPY_MODEL_7c21f743e57f4f44bd26822ed8563ab3",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "3cb1a2f217a148df8f68bef1b526a2c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bcf4d595b8724032a15d96793164c8b6",
       "style": "IPY_MODEL_e180d3ca1235425ba72c665cc74c303f",
       "value": "words:"
      }
     },
     "3cb8886645194155b5b564b67e1dfa8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "3ceb158b106b49c3adaff3a35386667b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "3d0c35a14cf74cdaa49f1cbad7af60fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "3d20ee3248cb44c495f1b68482ce77f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_23257a9df8ed473cad690f3037be5ed6",
       "style": "IPY_MODEL_d888dccf610b4dd4b25ecb31491ba152",
       "tooltip": "Retrieve related references"
      }
     },
     "3d34d24d18eb4f0e9d01f7a991cd3577": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "3d530826a5534c01af6476231645028d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "3da5e07b973843a1949a6157b79d4fad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "3dd7be7cf3534ea596bd384327b59882": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "3e2966e92f374e348ff15cc369ca7c31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_e6ad5e5dd0a649789cc610c5752b74ca",
       "style": "IPY_MODEL_462cf2cddeef452ba2c0affded35c2ba",
       "tooltip": "Retrieve related references"
      }
     },
     "3e3e49aa981d47c6b933bb4cfc7a7034": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4b845274d08041b29b37ce1f7d1707e1",
       "style": "IPY_MODEL_03dd8e1d77ef4f04aa594376bec45a34",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='Model-based explanation is by definition model-specific (Adadi and Berrada, 2018), but model-specific explanation is not necessary model-based. Some post hoc saliency mapping techniques are examples of techniques that are specific to a certain class of convolutional neural networks (CNNs), but are not model-based explanation methods (Murdoch et al., 2019). '>\n                            Page 2, Region 19,\n                            Score 0.0\n                        </summary>\n                        Model-based explanation is by definition model-specific (Adadi and Berrada, 2018), but model-specific explanation is not necessary model-based. Some post hoc saliency mapping techniques are examples of techniques that are specific to a certain class of convolutional neural networks (CNNs), but are not model-based explanation methods (Murdoch et al., 2019). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.4887709);\n                color: white\n                ' title='Example-based explanation is an XAI technique that provides examples relating to the data point that is currently being analyzed. This can be useful when trying to explain why a neural network came to a decision, and is related to how humans reason. For example, when a pathologist examines a biopsy of a patient that shows similarity with an earlier patient examined by the pathologist, the clinical decision may be enhanced by knowing the assessment of that earlier biopsy. '>\n                            Page 10, Region 8,\n                            Score 0.49\n                        </summary>\n                        Example-based explanation is an XAI technique that provides examples relating to the data point that is currently being analyzed. This can be useful when trying to explain why a neural network came to a decision, and is related to how humans reason. For example, when a pathologist examines a biopsy of a patient that shows similarity with an earlier patient examined by the pathologist, the clinical decision may be enhanced by knowing the assessment of that earlier biopsy. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.39505452);\n                color: white\n                ' title='Example-based explanation often optimizes the hidden layers deep in the neural network (i.e., the latent space) in such a way that similar points are close to each other in this latent space, while dissimilar points are further away in the latent space. '>\n                            Page 10, Region 9,\n                            Score 0.4\n                        </summary>\n                        Example-based explanation often optimizes the hidden layers deep in the neural network (i.e., the latent space) in such a way that similar points are close to each other in this latent space, while dissimilar points are further away in the latent space. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='Several papers provided example-based explanation using a triplet network (Hoffer and Ailon, 2015). A triplet network consists of three identical networks with shared parameters. By feeding these networks three input samples, the network calculates two '>\n                            Page 10, Region 12,\n                            Score 1.0\n                        </summary>\n                        Several papers provided example-based explanation using a triplet network (Hoffer and Ailon, 2015). A triplet network consists of three identical networks with shared parameters. By feeding these networks three input samples, the network calculates two \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.9240903);\n                color: white\n                ' title='Peng et al. (2019) used example-based explanation in colorectal cancer histology. They first trained a CNN using a triplet loss, hashing, and k hard-negatives to learn an embedding that preserves similarity. In testing, a coarse-to-fine search yielded the 10 nearest examples from a testing database related to the input image. This provided explanation on which images similar to the image that was being analyzed the network based a decision. '>\n                            Page 10, Region 14,\n                            Score 0.92\n                        </summary>\n                        Peng et al. (2019) used example-based explanation in colorectal cancer histology. They first trained a CNN using a triplet loss, hashing, and k hard-negatives to learn an embedding that preserves similarity. In testing, a coarse-to-fine search yielded the 10 nearest examples from a testing database related to the input image. This provided explanation on which images similar to the image that was being analyzed the network based a decision. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.32142147);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.32\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.51334876);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.51\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.20498773);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.2\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.43437466);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.43\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.10080199);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.1\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.20557351);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.21\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.38696605);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.39\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>GUT 2020 Exploring prognostic indicators in the pathological images of hepatocellular carcinoma based on deep learning</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.25003842);\n                color: white\n                ' title='Figure 1 Data and workflow for the prognostic analysis of liver cancer with deep learning. We first developed the classification network using 260 whole-slide images (WSls) as the category-based sampling. The network was then used to analyse the remaining WSls and generate the segmentation maps. We randomly sampled tiles from each type of tissue based on these segmentation maps. Next, we trained the prognostic network and calculated a tumour risk score (TRS) for each patient. Finally, we used TRS to predict patient prognosis, and integrate transcriptomics, genomics and neural network heatmaps to identify interpretable features. TCGA, The Cancer Genome Atlas. '>\n                            Page 2, Region 6,\n                            Score 0.25\n                        </summary>\n                        Figure 1 Data and workflow for the prognostic analysis of liver cancer with deep learning. We first developed the classification network using 260 whole-slide images (WSls) as the category-based sampling. The network was then used to analyse the remaining WSls and generate the segmentation maps. We randomly sampled tiles from each type of tissue based on these segmentation maps. Next, we trained the prognostic network and calculated a tumour risk score (TRS) for each patient. Finally, we used TRS to predict patient prognosis, and integrate transcriptomics, genomics and neural network heatmaps to identify interpretable features. TCGA, The Cancer Genome Atlas. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.21893026);\n                color: white\n                ' title='It has been posited that improved HCC risk prediction models leveraging AI techniques could be used to personalise HCC surveillance strategies by improving risk stratification of patients with chronic liver disease. For example, Ioannou and colleagues found that targeting patients with the uppermost 51% of their NN-derived HCC risk score would include 80% of patients who would develop HCC within the subsequent 3 years.° Such an approach could be useful in resource-limited settings that do not have sufficient capacity for regular HCC surveillance in all at-risk patients. However, to date, the clinical utility of this and other Al-based scores for predicting risk of HCC is unclear, particularly as these data have limited generalisability, given their reliance on the size and diversity of the training dataset. '>\n                            Page 3, Region 5,\n                            Score 0.22\n                        </summary>\n                        It has been posited that improved HCC risk prediction models leveraging AI techniques could be used to personalise HCC surveillance strategies by improving risk stratification of patients with chronic liver disease. For example, Ioannou and colleagues found that targeting patients with the uppermost 51% of their NN-derived HCC risk score would include 80% of patients who would develop HCC within the subsequent 3 years.° Such an approach could be useful in resource-limited settings that do not have sufficient capacity for regular HCC surveillance in all at-risk patients. However, to date, the clinical utility of this and other Al-based scores for predicting risk of HCC is unclear, particularly as these data have limited generalisability, given their reliance on the size and diversity of the training dataset. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2526163);\n                color: white\n                ' title='Fig. 3. Explainable artificial intelligence: example of pathology. This virtual model is dedicated to the prediction of the tumour or non-tumour nature of images from digital slides. The aim of explainable artificial intelligence is to better understand, through transparency, semantics and explanation, how the model makes its predictions. Transparency (1) consists of having an in-depth knowledge of the structure of the neural network and the activation status of its different neurons/nodes. Semantics will provide insights on the type of objects that result in the activation of particular parts of the network). Finally, explanation will enable clinicians to understand how the association of different features impact the final prediction. '>\n                            Page 9, Region 2,\n                            Score 0.25\n                        </summary>\n                        Fig. 3. Explainable artificial intelligence: example of pathology. This virtual model is dedicated to the prediction of the tumour or non-tumour nature of images from digital slides. The aim of explainable artificial intelligence is to better understand, through transparency, semantics and explanation, how the model makes its predictions. Transparency (1) consists of having an in-depth knowledge of the structure of the neural network and the activation status of its different neurons/nodes. Semantics will provide insights on the type of objects that result in the activation of particular parts of the network). Finally, explanation will enable clinicians to understand how the association of different features impact the final prediction. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>NPJ 2020 Classification and mutation prediction based on</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.21565841);\n                color: white\n                ' title='for diagnosis and treatment, Al should be validated against current quality standards to ensure clinical effectiveness and safety in clinical practice***. In this study, an independent database from our center was used to validate the performance of our models. It was demonstrated that convolutional neural networks could be used to assist in the classification and mutation prediction, based on histopathological H&E slides in liver cancer. However, the model still needs to be improved and validated by larger studies in the future. Even though it is impossible for Al to completely replace humans in practice nowadays, it is still a useful and effective tool to assist clinicians in dealing with repetitive work to provide important prognostic and therapeutic information. For example, mutation prediction could serve as_ prescreening to improve cost-efficiency before immunohistochemistry or next-generation sequencing. '>\n                            Page 4, Region 10,\n                            Score 0.22\n                        </summary>\n                        for diagnosis and treatment, Al should be validated against current quality standards to ensure clinical effectiveness and safety in clinical practice***. In this study, an independent database from our center was used to validate the performance of our models. It was demonstrated that convolutional neural networks could be used to assist in the classification and mutation prediction, based on histopathological H&E slides in liver cancer. However, the model still needs to be improved and validated by larger studies in the future. Even though it is impossible for Al to completely replace humans in practice nowadays, it is still a useful and effective tool to assist clinicians in dealing with repetitive work to provide important prognostic and therapeutic information. For example, mutation prediction could serve as_ prescreening to improve cost-efficiency before immunohistochemistry or next-generation sequencing. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.026345542);\n                color: white\n                ' title='Overall, the study demonstrates that convolutional neural networks can predict histopathological grade and mutation in liver cancer. Although Al is likely to be a useful tool to assist surgeons and pathologists in classification of WSIs of HCC, the black box that how to get the conclusion is unclear and should be further studied. Besides, it is the first study to predict the gene mutation in HCC, meanwhile, internal and external validation cohorts were utilized to improve the accuracy of the model. In addition, the information on pathology and gene mutations may potentially be significant in applying the appropriate targeted therapy to HCC patients, thereby improving the performance of precision medicine. '>\n                            Page 4, Region 11,\n                            Score 0.03\n                        </summary>\n                        Overall, the study demonstrates that convolutional neural networks can predict histopathological grade and mutation in liver cancer. Although Al is likely to be a useful tool to assist surgeons and pathologists in classification of WSIs of HCC, the black box that how to get the conclusion is unclear and should be further studied. Besides, it is the first study to predict the gene mutation in HCC, meanwhile, internal and external validation cohorts were utilized to improve the accuracy of the model. In addition, the information on pathology and gene mutations may potentially be significant in applying the appropriate targeted therapy to HCC patients, thereby improving the performance of precision medicine. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.114419);\n                color: white\n                ' title='In conclusion, our study demonstrated that the convolutional neural networks could assist pathologists in the classification of liver cancer and the detection of gene mutation. It also revealed that this method might be successfully adopted for other types of solid tumors. '>\n                            Page 4, Region 13,\n                            Score 0.11\n                        </summary>\n                        In conclusion, our study demonstrated that the convolutional neural networks could assist pathologists in the classification of liver cancer and the detection of gene mutation. It also revealed that this method might be successfully adopted for other types of solid tumors. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Explainable medical imaging AI needs human-centered design a systematic review</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.091442324);\n                color: white\n                ' title='specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions '>\n                            Page 7, Region 4,\n                            Score 0.09\n                        </summary>\n                        specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.07352536);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.07\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "3eef018eb9ae42c1a8158a24911388e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "3ef951f4a19d48c3ac9f2c1e3de490cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3f7456eb16c04af893b896fbb53fc69f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "3f811633ac9f49d2862935c6f2d49836": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bd660b8f603241a283a579c1fb3ea7e9",
       "style": "IPY_MODEL_0db611051adb448ba974bc65da1b80a0",
       "value": "queries,"
      }
     },
     "3f9222be70d8499fae3a5e045328ad26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7ff56c3a828a41059870b399c7472784",
        "IPY_MODEL_f176e64767d74a79ad1f46e49366f397"
       ],
       "layout": "IPY_MODEL_eae845c9baba4c73aa8599fabc4e2391"
      }
     },
     "3fb9ff8b02d74401b755f65e6eaeb6bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3fcc9880c080444abc427e338afb553c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize All",
       "icon": "radiation",
       "layout": "IPY_MODEL_7b99d280099a4c5e8243461d2fcc0b99",
       "style": "IPY_MODEL_74ffddb97ca8409a9d8ef7ee5dc5f87f",
       "tooltip": "Summarize all sections with one click"
      }
     },
     "3ff8d2c60b614081bb191b8510607e80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d1977101a1a54fc6a24596b8ad28c697",
        "IPY_MODEL_03b7d0d6fd9145a5877f123464043e2d"
       ],
       "layout": "IPY_MODEL_33238ef0a70e4d458c877621872f11d0"
      }
     },
     "4086897d8a80434fa88d05f08fdaeaad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "40d8eed5417f4b2e83f5adf7f6bb5fea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "412f4516628744f79d065d6eff70dff8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_65eac6e37f5d4ee5aa748f9893ab3970",
        "IPY_MODEL_659083598f094f6f994fa400f32f5e07"
       ],
       "layout": "IPY_MODEL_caf372f3d2424e0282df147882ab3fa3"
      }
     },
     "416f69f88ae14667aa254e4f899af3b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_f28ecc4de59e4b36be273d5bc3175b19",
       "placeholder": "20",
       "style": "IPY_MODEL_2fed2e63ae5b4df085764356d6b90883",
       "value": "20"
      }
     },
     "418808a1cd3d4262861e9da5056bd2a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "41ba7a3f84854074a5d85425edebe07b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41d97d44ff3a4ccf84c126d547722b83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_e6ad5e5dd0a649789cc610c5752b74ca",
       "style": "IPY_MODEL_4a7a1c333e434071bd6a590641254262",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "42a9cb3b15f94b28aed3d725b446f73b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c6f6181d293345058bbf1a3d13d45e14",
        "IPY_MODEL_d37366d4603c4d10835b5b37c37c0097"
       ],
       "layout": "IPY_MODEL_0229b0880554442fbbeb750c4882e65d"
      }
     },
     "438851cad19d4fb98b190a2010fa5472": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8b753fb4212943a29d89b331e804de92",
        "IPY_MODEL_802e922cd8224839b312b676f4e2e9ba"
       ],
       "layout": "IPY_MODEL_6e36be87f3744ae8bbdf2b23b9560f7d"
      }
     },
     "438a7f42274649db8830f901338f209d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_194b2b8cf4dc4965a524aa93531357d2",
       "style": "IPY_MODEL_2c2ff66a7ed245d58378b4fa4a333191"
      }
     },
     "43bd53004fcc4457be5b60dc5f039d89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "43c1f12657de4ac78c8cb553921e37cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_680b7f61a3784ac88532f88da0356192",
        "IPY_MODEL_c24a396a5ebb46f1b55bcce0c8d7683b"
       ],
       "layout": "IPY_MODEL_d7d679c5a281483594b57d42b03b3fb2"
      }
     },
     "43c2f645492549118212640aa1091070": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "442e53b481d249cd8566df50daf58d4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "448e163e1621471f8886992083c8ecf1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_43c1f12657de4ac78c8cb553921e37cc",
        "IPY_MODEL_c3993b7251b2419dad813cec6cc44366"
       ],
       "layout": "IPY_MODEL_0b0c2ea3863549e8a1beda77d59e3e1f"
      }
     },
     "44b43d37a71e43a2afc99b5b5c7f9a66": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "44f46c2c3097455daaacc16a631d3098": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "4503c57a38524581b400b5d230624c7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "45482ed51e78490fac837e935d939c2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "45592cf1cc044094b0d660d7d8a1d090": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "459c801d8fa7477aaf516503118439da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "45d942b2744b4471916341c0947b1199": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_39ad1c66d5f146268a514117c0848d09"
       ],
       "layout": "IPY_MODEL_dd0fab92c00643b29bbdba7d87cd2364"
      }
     },
     "45fe3c2f372d45d68b5184699a117a85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "462cf2cddeef452ba2c0affded35c2ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "4662dd073a534092aac3e27f04095afb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_eba8d90e47a245d3ae253c4b571baf29",
       "style": "IPY_MODEL_7084603e8e6343928305cc2435882439",
       "value": "words:"
      }
     },
     "46642b3842c5432691c9084c21faed64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_ce3ea150d76b42ecac100e3b323c104d",
       "placeholder": "",
       "style": "IPY_MODEL_ffe95e3f92fe45bc84c0933529809c72",
       "value": "500"
      }
     },
     "466d617e82cc45a8a10ca9c877ab8ed4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "467fb7ce955e4776a673372d34531cde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "46945bd813d64267bd3af558207d5841": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "46e19b99d84d460cad67aa67fbb167cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "46e7716369364a6ea9742ae15e971a84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f57f070eb4a148a6b78c1443e6989b1b",
        "IPY_MODEL_f3e294b6c53349c7b0d870285323155e"
       ],
       "layout": "IPY_MODEL_3da5e07b973843a1949a6157b79d4fad"
      }
     },
     "46f15d0fc76647818eb88e77cf2f8979": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_7d134349b91b475bac4a8728af1548b3",
       "style": "IPY_MODEL_3ceb158b106b49c3adaff3a35386667b",
       "tooltip": "Retrieve related references"
      }
     },
     "46f1681c86c441f2a8a22970f2f9b7cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_2111dd25f6424c778ecd9e9c4b3f9a8d",
       "placeholder": "",
       "style": "IPY_MODEL_4970e5660efb432fbe0e8c9bb5b16b90",
       "value": "500"
      }
     },
     "4769aefabb9f40a18f88c7d11e7a0072": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "47af31b1512941fcbfb8102d302ed7fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_69d486a3ff1549878457bdd4c8895ab8",
       "style": "IPY_MODEL_83e699cbdd9b48b4822f3c1a472a847c",
       "value": "3.1.4.2: Prototypes"
      }
     },
     "47ba4ed2be2c4073be9c26fc35f45221": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_e63b6721ce424effab2471be829f2e51",
       "placeholder": "",
       "style": "IPY_MODEL_177831bef9b44f2d8a83f4c71e47cf8f"
      }
     },
     "47d3baabb74a4f7bb31f8c0c000a10e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4887717b4dbf4222bf90295c4c5fa10f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "48bda2fbb52d4b629eee1a7a5b6bcb8e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "48d1d0b6407c4f7d8d381cd092c9e9e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_c53eabe819984411a7fb30a623e9c18a",
       "placeholder": "20",
       "style": "IPY_MODEL_615ddebb491f4d9882d40a7efbcd0a4a",
       "value": "20"
      }
     },
     "493420701d384c22b8f0bf46c4eaaafb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_88400557d6394b7fbb9fcd95affba71f",
       "style": "IPY_MODEL_15c2ed75773b4ffa80754fc9f79783b2",
       "value": "queries,"
      }
     },
     "49499a6fbf3342caa6366879d90cb29b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "4970e5660efb432fbe0e8c9bb5b16b90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "498589f85c1d444f965ebd1dc163475d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_933a4b117ff84228ae994acb87e1a7c7",
        "IPY_MODEL_1cbbbc41ee264b94b836909e49cc1ed2"
       ],
       "layout": "IPY_MODEL_867d7aff5d6746deb6aad565a4d73822"
      }
     },
     "49907888ac124aa3be543c2259ba5dcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cfc7d3c753a240a89071e8092d5f4cbd",
       "style": "IPY_MODEL_b987d2145da042c4a95b823f68f6151f"
      }
     },
     "499440aee80d4c53a57ff7e3a52e0bc5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "49b0f761e87748ada135fc163c492a44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4a6517bfdc6446bdb1546febb1f22885": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "4a7a1c333e434071bd6a590641254262": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "4aa9c0ce64ba4daabafb0b891b26772c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "4b809589de74427f82917ba7031b551d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b845274d08041b29b37ce1f7d1707e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b8f1f7addad42bcbe4d391df1f0f696": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_218dbb4deee541e99ff872248ad960e7",
       "style": "IPY_MODEL_5fc316ec95064f39a7a449b111b365b9",
       "value": "words:"
      }
     },
     "4ba196d578d043b88498a57ea745cb8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_2e3f72b6dd314109b184563eea84be8b",
       "placeholder": "",
       "style": "IPY_MODEL_3711706215654c98be554018cca181cb"
      }
     },
     "4c0534c9b4b04d6f98c9c1875a7556f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_97b39a5f81364133835a504ac3c4f7a1",
       "style": "IPY_MODEL_5ea98f51f80741709233827e6cfc3951",
       "value": " to "
      }
     },
     "4c216e576ebd4fa89f1ea33f372eb060": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_abbcdfa5eb73497c87a80040b575dc36",
       "style": "IPY_MODEL_21b42787f0774b9a9746a20684968175",
       "value": "words:"
      }
     },
     "4c4b48a2199842ef96f9f4a592cf0a31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bf43af8e93574919ab5d94d5bc466f82",
        "IPY_MODEL_3ff8d2c60b614081bb191b8510607e80"
       ],
       "layout": "IPY_MODEL_8661ba456a784ebaa923fb77b45980f3"
      }
     },
     "4cb0368ad8c44e17898ed61475490643": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_95ec822807fa42bcb1c88cb37da6acb5",
        "IPY_MODEL_092488cf3d0041768dd8de85fd21ae93"
       ],
       "layout": "IPY_MODEL_35d322a8686f4ede8824734fb99579ed"
      }
     },
     "4cba08dec39f48d7827ddd54489f05e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4d69e2f061994e448ecbb616b4a0b1d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4d78eab806be49de8ab30c1be2bfe06d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_992c39fcbccb42349a474990102af1d4",
        "IPY_MODEL_5e150ae5a60e40c8bcc69021ac1ebbc4"
       ],
       "layout": "IPY_MODEL_7e29f9fdd3c94a3ea80b2e1494078693"
      }
     },
     "4dbcb2273e8a4d4d83da65945eed353e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "4e29bd137d8a44b6a8ec61d3fa284a52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4e3760bdfb234b388c275bb7afbd6c5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_dc9736c27e8447219b7a0d325f077f53",
        "IPY_MODEL_45d942b2744b4471916341c0947b1199"
       ],
       "layout": "IPY_MODEL_e4563c0ece72441c95fad22cfbac3b6c"
      }
     },
     "4e57f4807409466e8a5ebaff057f37ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "4e79953a6d6c4a08bba50c65b16d2e23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_32bdfbca04814e28815bbb5a00a03979",
        "IPY_MODEL_3c91d979788448e8a58aa5591be4c1c1"
       ],
       "layout": "IPY_MODEL_6a4a01997dba455db6c0e62d1a399dbe"
      }
     },
     "4e7b557b9ebc4537927678d04e32dcf3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e5c4246a5a72462c8d1e974b550c731c",
        "IPY_MODEL_bdb1cc8a491a4fe7bbf769446604b9dc"
       ],
       "layout": "IPY_MODEL_b0218bc33bbb4fd1bae624ef223fbf16"
      }
     },
     "4f4a81bc6fb743528067093b7c4c362a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "none"
      }
     },
     "4f93c362c6f449919cddad73439b3671": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4faa7a9a13ec42fba8586be2ecbd5317": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "4fc5f91eb37c41269af94d2e69592bb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4fe832fffddb4950b506e1b78307aa87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_418808a1cd3d4262861e9da5056bd2a3",
       "placeholder": "",
       "style": "IPY_MODEL_236c957b787d476f85995cc9cbe44bd5",
       "value": "500"
      }
     },
     "5005b1dfcb7e41f8a13f1d7b7102806d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "5013dffaeb4342a5862fc747d591795e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_6f6e0dc5903d42769aae7444b28d003b",
       "style": "IPY_MODEL_d737400c1a594029a77bebd193bc1339",
       "value": "3: Strategies for unraveling the “black-box” of AI-based pathological analysis of liver cancer"
      }
     },
     "502355bfdcc5476ab76e55ed7e4eb93d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_18b6b5b9cb7d4d92b2684705b4dfc004",
       "style": "IPY_MODEL_ee9d5c45f57d40559134e1d1a712b9fc",
       "value": "A comprehensive discussion on various strategies to decode the complex workings of AI in pathological analysis of liver cancer."
      }
     },
     "50331502177a4790ac3eb467ec6b43d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "5034d1fd58d74644b550089d09498597": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "5039b58fd3ed4ff2a861ee34168ce5fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a2fdaeeb3e844e7e946185d442632a6e",
        "IPY_MODEL_6cb7c13e65c049879fa6c497b665ff47"
       ],
       "layout": "IPY_MODEL_b117e0b343ca4198a976ff3a000c4a29"
      }
     },
     "504550124a384a3aa4a2dfc7247189ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "505f43289bce49d0a2ecd3d1626e4508": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_1d860d05dc8f4adaaec0a6505a34287e",
       "style": "IPY_MODEL_c5c880e20204411491c8433160afd9f1",
       "tooltip": "Retrieve related references"
      }
     },
     "50752c2a8fcb459ca14221524764541b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_ddb4d2cb810549ffa89be527ff1af753",
       "placeholder": "20",
       "style": "IPY_MODEL_d27236d330ff48c2931d720b879d3be1",
       "value": "20"
      }
     },
     "5077f675a22940eb91cf465b4a8e1718": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7a5ce28ef86f4babad87c8e370823811",
        "IPY_MODEL_498589f85c1d444f965ebd1dc163475d"
       ],
       "layout": "IPY_MODEL_ba8c8b71e14e4b89aaebbb006dc0dc9b"
      }
     },
     "50c0cdb21ed74813be1eb36b13024a92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "512d7070a36041f9be5dfc8f3dd10c07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_cf5773a2ebfa463087bfa92844d9a350",
       "placeholder": "",
       "style": "IPY_MODEL_a97af0189b9b44da8cb3c89a3bd1bff7",
       "value": "500"
      }
     },
     "5164e10c613d4377bdb021d2c94122b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_78715c0132c84b2f983534fa8a2559a9",
       "placeholder": "",
       "style": "IPY_MODEL_2e49e40320c3498c936fdd121ea998bf"
      }
     },
     "51a55dcd613947b88873b5222f50aa5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "51a57990af88439ba5a301fbc846442f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7c1dce0a3d184f24adc60e269ae1d7b1",
        "IPY_MODEL_54b75aec40e34939857bc262623ba8c6"
       ],
       "layout": "IPY_MODEL_b331c86710c04ada923bb9a5b3b8dc9f"
      }
     },
     "51adda7d5ac941cc9ff4cd32f9219365": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "520b1f0f984245dcbb3a6cd6243e2085": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f1198faa874746c5b3a68e8782279ee5",
        "IPY_MODEL_8327758abc13446c85daaf471a912301"
       ],
       "layout": "IPY_MODEL_d2fe474438df45e58e5fd0151f55bb6b"
      }
     },
     "524f05bbb40a46b6adb9c894201c13fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "526106c89b0a47b69db2b9efbe58331d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "529598a3d9b44947a26c2cab9cf429bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a2804d23bbd04f38ae6619a9e045e658",
       "style": "IPY_MODEL_d51bcd9074e046fb980ef31dc6fd7965",
       "value": " to "
      }
     },
     "52bd1f17b4034701bc1281d38ac976eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a0df8e28d9fc4948931cecd107d1dac2",
        "IPY_MODEL_e3a7571e4a4a4368b45eea9902c1265d",
        "IPY_MODEL_08c2917582184039a7c9d16b558c1092"
       ],
       "layout": "IPY_MODEL_bf5b7e9e1e3a414eaaa6eaac290fc092"
      }
     },
     "52dcedb1ffb64386b1674c0539fcddaa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "52f5264e15d64fa6bfafac3e64fabdf6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "53108e5077df415085e5664c58d979e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "533ac58145364543bfb69dad7676ac31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "53526918d45f480a9c805c33f0b8be56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "53a8cb313322497cb1b354adf8758d4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "53b96e1a9fdb4befb2cf8283714550d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "53db944396864cb3a4f1efeb24505a15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "53efff40a0d34d46ab84abfe4b6a36d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_286cf6a621514cdda5b91c4ab782f520",
       "style": "IPY_MODEL_26ea10d872304fba92cf710bab4ca820",
       "value": " to "
      }
     },
     "545e66fe985b421abc7d7d8461f85423": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "54b75aec40e34939857bc262623ba8c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d3ec155a1825423bafabc0f50e1c5a08",
       "style": "IPY_MODEL_d9df366f03ea4446ba93cf92e9474a58",
       "value": "queries,"
      }
     },
     "54f5137c5c9d4181beba060039902959": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "5514f030f81049d6beaeba8c8e3836a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f1eb106fb4a14bcb99bdf66ce5d0c05a",
        "IPY_MODEL_db5ee9dc278543809d1e0e5bbbcc27e3"
       ],
       "layout": "IPY_MODEL_37a3f3b63ff2404d83b93cdb5bb2cd9e"
      }
     },
     "5532e57e35d746afbfba4386b9929838": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "557d5f66d2d44b3cac23c0151c103acc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "5597819ef1a847d9b00b9afa95900262": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_945e24076a2040a3b14ab3a49cd96205",
       "style": "IPY_MODEL_0ec7cfd608344b4ca3a2d194fb22ec23",
       "tooltip": "Retrieve related references"
      }
     },
     "55bb1de9e29c49b4ab2affe8dfeda0f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "55daa4aa448c47b28128b45dda0dcfbc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "55db91a9330747dfb014fa80d646d8ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_d4490c93a5e447e9acc538bb136028bb",
       "style": "IPY_MODEL_79a20f5002d84d3a94d68bef04d2eb42",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "5614f3055aaf4960af17f71b6d815b3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5617c41bfc1d4e0ebc03f5cf91d557fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f4b851fb9b144ad69b12dee269327fdd",
       "style": "IPY_MODEL_103456ebf7bb4c5f8aca08727ca0d4d0",
       "value": " to "
      }
     },
     "5639ce12d6e94d0faaed5ad90ff8bdc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3a928646497d45fda63570ffc392b6a7",
        "IPY_MODEL_35491db10f744a4a9f6980defef5fc95"
       ],
       "layout": "IPY_MODEL_d469e27f87794c0e90e6392801741e21"
      }
     },
     "56483a06b4fd4f08a43afe0cded13600": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "56a611cfbd2145ea98531fd15981e284": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_164b4e84d339442788599f9eac06057b",
       "style": "IPY_MODEL_e3b2bc00dabc4c96b1c5e6d80cc635d2",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "56b5005f6e354e49a9d6dc106061b7bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "56fa6ef96dca47248fcbb5a2885b4ec0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "5780f840227143f2b9f27e133d8bc587": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e8248055741648c1b87ce3580466773d",
        "IPY_MODEL_46f1681c86c441f2a8a22970f2f9b7cb",
        "IPY_MODEL_c5ac14f0cb7948d98539b80b37437f2b"
       ],
       "layout": "IPY_MODEL_15d5b2d0381b46dfac8618eb7488fa2c"
      }
     },
     "578e256a4ead4c08bd14684cd13b15b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "57e449eeb61a4dd7b8d7eee941756933": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "580a2031be4a4124afb2eb8e30e86a6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "5822485942aa4d48ac90388f0ccd697d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "58ccd9a979ed4f79b2154a2ac2859163": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_697851376f8c48169f9bc7c4836b1fc7",
        "IPY_MODEL_059d17fcfaae47609ec578626d4838d4"
       ],
       "layout": "IPY_MODEL_d0b905ca6d3d410a91683eb0e65bb2ec"
      }
     },
     "595541ae312d43158c43384817a0d361": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "5968064dc67748c783c4f1e22ac3ca64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_ded7b25b60aa4cb0b7da281e597c4c28",
       "style": "IPY_MODEL_ff07296e2ecc4da6aa975b55ab453c41",
       "value": "3.2.1.3: Multiple instance learning-based approaches"
      }
     },
     "598568449f9e4cdfb688bad8c568c641": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "59b95a3ead924587879e1d7becb4a122": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "display": "none",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "59dec56c69d04c8a9f7f48775c7ac33e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bf62e3b66277477f80533c55103e6c4d",
        "IPY_MODEL_e7a41e5faf1c4ef59245fd65b47b5925",
        "IPY_MODEL_3cb1a2f217a148df8f68bef1b526a2c4"
       ],
       "layout": "IPY_MODEL_e8dcec46504848c6be14eb9b2bf884be"
      }
     },
     "5a128a9af3cc44f4abc15b3f0346d5bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_009fe58f8f6a4365afde11a67638566c",
       "style": "IPY_MODEL_2dae6f856c63448e96fb0ef559d1cee0",
       "value": "3.1: Model-based explanation"
      }
     },
     "5a9d3aacd763457ca6dc2b11fb547aba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fde76fd605f941598f090c2f9126695e",
        "IPY_MODEL_4fe832fffddb4950b506e1b78307aa87",
        "IPY_MODEL_f2e430c3aae74dcb96b3ffa169525c2a"
       ],
       "layout": "IPY_MODEL_4e29bd137d8a44b6a8ec61d3fa284a52"
      }
     },
     "5aecc44674844a36bf684a7893319346": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5aefb9f3db754ee29064da6a3f773902": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "5b03f8ebe19c4087a9fbf2239f1c2e83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ecc1a8016e214ec288ec15a96d235631",
        "IPY_MODEL_c7d155ccd3ec41b1945e85f9ff03e3b7",
        "IPY_MODEL_a6a90623c63f4deab79073d4c546fae6"
       ],
       "layout": "IPY_MODEL_a19adc7520c64875bf8c405d32df1a76"
      }
     },
     "5b278d7e9de0436188e5fca57bdfaf59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5b382827917445a6a7fb4193dd6bbf23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b690177d5e545a0a051b97ae9658053": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "5bb00fde46c2477f98a3a8030ec32c9b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5c0d5883a90843c7803a2bd1ee6de2d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_f751eb81226b4f05bfafaa49d8dd50ee",
       "style": "IPY_MODEL_10bcba80c3254c14821653739101812b",
       "value": "3.1.1: Support vector machine or random forests vs. deep learning"
      }
     },
     "5c619bccd26a43dd8e97012660002ae9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5c8e6b74c8bf480992fbcf3cbd0d54ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_62ebeffdab564357b6e321f99967dc83",
       "style": "IPY_MODEL_ae2de312d95f4ca394e4ac97dc4634fc",
       "value": " to "
      }
     },
     "5ca8b6f96de142aeb579257d58988080": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "5caffb89e88a4ede9d23913794ac9bac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_70f816009c104bd6adfc0406b98bcc17",
       "style": "IPY_MODEL_de008c269a894c19965e95e37be8872d",
       "value": "3.2.2: Textual explanation"
      }
     },
     "5cd4a3acae5743da8188eceec657de92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_54f5137c5c9d4181beba060039902959",
       "style": "IPY_MODEL_7419e5b0e0874bd19b2392115f620db1",
       "value": "Exploring how example-based explanations can be used in post hoc analysis of AI's decisions in liver cancer."
      }
     },
     "5cda57119e00425fb600d97320bd3758": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "5cdb9ac5a7114081968ce2c0643c3c74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5cee6420603c441481d48f0d2de454d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5d2c41e6de0044feba55fdf6ddb12fa4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_a396875fff04407a8d44d40bcc0c09dc",
       "style": "IPY_MODEL_8b184d1db73e425d8d592257f24d882b",
       "tooltip": "Retrieve related references"
      }
     },
     "5d59e03db94b4a24899d4d4075b04180": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5dcc502d68304692a6e89763157fb3b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5e150ae5a60e40c8bcc69021ac1ebbc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d6d4a90c77804c7c89a52449fb2aae0c",
       "style": "IPY_MODEL_5fac05d4aa4745ea9ff1c2b55069a2e7",
       "value": "queries,"
      }
     },
     "5e720a4c601846fbb307450be54537b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "5e95dc8b292d409fb84f5fbc6d442a30": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5e97fab890304257a71df4c48befde53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ea158190faa341d982ec238ff53239ae",
        "IPY_MODEL_6f1be6526161496a83f6a69e4ac29c01"
       ],
       "layout": "IPY_MODEL_34234a7e95f345d9a1732845c1741c4b"
      }
     },
     "5ea98f51f80741709233827e6cfc3951": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5ed5fe0a669346f8a290bc7897786d5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5f6c534340fe4bd9904177aac086e2b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5f9688d2dd354fddaa6d8d67004d0bb9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5fac05d4aa4745ea9ff1c2b55069a2e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5fc061c4578f4776bca8b06cc295cdb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5fc316ec95064f39a7a449b111b365b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "614e2079ddd748c98801e6317c8f489a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_a396875fff04407a8d44d40bcc0c09dc",
       "style": "IPY_MODEL_ac1bdabfa6fc4a04a1852097e07a8e7b",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "615ddebb491f4d9882d40a7efbcd0a4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "615f88d28a4a4fbe9d24eec54a5c4a3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8f471ae2dbbf4ba4a8d4b43e4248c696",
        "IPY_MODEL_305da98f2e014aca8249b7a577ec8981"
       ],
       "layout": "IPY_MODEL_d62407c3557f48ed99bc8c024f473671"
      }
     },
     "619554e2ac7c4e3fb2cde1216f199ce0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "61a65711f2a245698325d0644f6052a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "61d20fd9dd0d466db3b2d0ebf3f4d693": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "62025d75629748aea5ed21d3b01b7737": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_104033f861f8484e8742316c29698554",
        "IPY_MODEL_9a447af3f9794e3eaa55ebc762019e79"
       ],
       "layout": "IPY_MODEL_e1c053ad4ad44f33ad27139fd2cb6e09"
      }
     },
     "6245c1cc6f8f4729b3f5531f66c30a15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_cc5cac6306034184a2c907e59a73e445"
       ],
       "layout": "IPY_MODEL_14edc3b97670495eaba7af7153d770c9"
      }
     },
     "624d97c3eac14b7d8454adebce47251d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_55daa4aa448c47b28128b45dda0dcfbc",
       "placeholder": "",
       "style": "IPY_MODEL_a38cbe8130a541d98de4a2fbaf5b0847",
       "value": "500"
      }
     },
     "625270a22a334d23aa94e69c18db927b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_9e8d16f9d5aa4807b513d147274e4668",
       "placeholder": "",
       "style": "IPY_MODEL_1671a6cfbfd54667980b94a0e2490751"
      }
     },
     "6269a6cfd9634bbca02c90e94dded4e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_51adda7d5ac941cc9ff4cd32f9219365",
       "placeholder": "",
       "style": "IPY_MODEL_d9f7cef8733042b5aa1c29244486c940"
      }
     },
     "627b86eb739745bab0e8a200cb65dfc5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "629e5a970e4644419aac0156295ddd04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5c0d5883a90843c7803a2bd1ee6de2d1",
        "IPY_MODEL_8eb6775f91704fc9be972e0dab1872be"
       ],
       "layout": "IPY_MODEL_b47b9592339543c784502c7854a05e24"
      }
     },
     "62dacf9509a3413c9b41608fc38e8159": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "62dcc27e7a7b4e13993a698d96c1d0de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "62e3f15827c241bb96f85c3433a2efb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "62ebeffdab564357b6e321f99967dc83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "63165eb468984153a0ecb82006b9f10c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "63309b071af745ea9d7ee40d0b6c8f6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "633e6859e4d34fad89677f94e297e6d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_b108b8b38f924491bd42efc6a2afc507",
       "style": "IPY_MODEL_93526ec373814eecacc269009446beb9",
       "value": "An in-depth look into model-based explanations for AI's decision-making process in liver cancer analysis."
      }
     },
     "63d689657d4f4c189c3a561469ddd3a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "63e576f577f9415c8e8cd57089a1e47f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "63ef01b039354a2895c961ae6b99eaf0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_52f5264e15d64fa6bfafac3e64fabdf6",
       "style": "IPY_MODEL_ffbef41f7d994e6ea7bd8bd06738cc7f"
      }
     },
     "643fcd58a558456a8d4a4e1bbb561a1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "6467021beddf42fcad1213af136094ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f2f44090f31b440dbf4141d327b43195",
       "style": "IPY_MODEL_e522a6b3d41f4c69a916674afd5a2a02",
       "value": "words:"
      }
     },
     "649e318fa3454104bc1450d3df87116b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_ee459328e1814e3681ef821b44be48c0",
       "style": "IPY_MODEL_45482ed51e78490fac837e935d939c2d",
       "value": "Exploration of how prototypes are used as examples to explain AI decisions in liver cancer analysis."
      }
     },
     "64cbee8cad354d20a3d7cda4e3b192bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_56fa6ef96dca47248fcbb5a2885b4ec0",
       "style": "IPY_MODEL_a6376c2e8dc649fbbde859279f78d605",
       "tooltip": "Retrieve related references"
      }
     },
     "6523c75230d348e3ba958adaf828c275": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "658815e3be9d438c9f7b265cc7c2f8da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_67fa6afacace435aadfb8b4b670ff1d4",
       "style": "IPY_MODEL_66d10f16df2b49778efe6c43a420ef97",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "659083598f094f6f994fa400f32f5e07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_53efff40a0d34d46ab84abfe4b6a36d9",
        "IPY_MODEL_8c33cb91ed714063b36c348c7cd9b049",
        "IPY_MODEL_b2a73932c6ff4104a77d10cb1f2d63e6"
       ],
       "layout": "IPY_MODEL_0663457f2f0648c599421f94fe891612"
      }
     },
     "65eac6e37f5d4ee5aa748f9893ab3970": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_8ec035b5b6ab47a08bc6adf724b34a0d",
       "style": "IPY_MODEL_b83dd471daf74b07b2836d0afe82d7ad",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "663b603bde9641d89b897fb36515e1b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "6687c399adc24050a81af7d449bb1850": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "66b75abff93d412180a46e380361a843": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "66d10f16df2b49778efe6c43a420ef97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "670e653ddea74dad93ef27960528b755": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "677c5f927cbd4487a26c6d186d76166e": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_bbb6f2aece3c41e784760ba1892e5beb"
      }
     },
     "67c538b4563e4d9092d9648988862d5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_c0d51b7feddb4b95b2fafa261096b6dd",
       "placeholder": "",
       "style": "IPY_MODEL_97ef4ae69c574bd4a0214f70e572b110"
      }
     },
     "67d99ea98b014f5589ab59e3fceaf5ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "67fa6afacace435aadfb8b4b670ff1d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "680b7f61a3784ac88532f88da0356192": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1c338ad3b1634b648eaee4f57ac68a1d",
        "IPY_MODEL_d6b9d9989c0142ceb3c2992e549a274a"
       ],
       "layout": "IPY_MODEL_4dbcb2273e8a4d4d83da65945eed353e"
      }
     },
     "686484db02f840ce86cfab06a0b86f4d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68acd03ded0e4ee0a492bf9bd5346b5b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "none"
      }
     },
     "68b10eef9f02482db42777597308f379": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "68e377660bb64cfab30c0a210586c993": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_97aeae4cf3f042c1a4945aa2280151d8",
        "IPY_MODEL_31c81c4d1c1b47e28e46f3add061fc0b"
       ],
       "layout": "IPY_MODEL_dcd3778529de4f57a33359b9daae0bbb"
      }
     },
     "690b5119a031426598eee4f15eafe41f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "697580f9c0ec4db6b86218a3469cb8aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "697851376f8c48169f9bc7c4836b1fc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3b218310004b4cec8e82166cfea744f8",
        "IPY_MODEL_2f3d62387c4a47bc80b6af09eb311e8d"
       ],
       "layout": "IPY_MODEL_0976635289fd4571b03bc80cc5b1a22f"
      }
     },
     "69c67ff295384556a856a5719c3492f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "69d486a3ff1549878457bdd4c8895ab8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "6a0bd6808aac47be94520d841f604a29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_23257a9df8ed473cad690f3037be5ed6",
       "style": "IPY_MODEL_366be16e3d3a4378b4178ca68e296e4b",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "6a4a01997dba455db6c0e62d1a399dbe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6b0eb20b0ff84ff19fbd78442e2ae3b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_56a611cfbd2145ea98531fd15981e284",
        "IPY_MODEL_9769b1d00f6d45f2a11c99a921cd5932"
       ],
       "layout": "IPY_MODEL_2be64f8e92d64a4fb3ae51275afafb27"
      }
     },
     "6b73fc919adf4728a15b61d5a6f2c07b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b966b89f113474481598638a0cdf95c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ea237a0eb50f4b19bfaec399156130d9",
        "IPY_MODEL_318b610160e14902a18713f822b08b7b"
       ],
       "layout": "IPY_MODEL_5b690177d5e545a0a051b97ae9658053"
      }
     },
     "6c31b413fe48455d901beee1350acf2e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6c7eadb925874ed2b6a708dccfa45f47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6cb7c13e65c049879fa6c497b665ff47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bae38a17fc09403d9a5c773344c5fd6b",
        "IPY_MODEL_2583560af88f4d769b68a80b387df735"
       ],
       "layout": "IPY_MODEL_a048a4b88124484c99ec2891e0090ffe"
      }
     },
     "6cc05aac9a814470a58238a6b6f2c614": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "6cc847e9e0f143ffb440f6a61e93ee2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5c8e6b74c8bf480992fbcf3cbd0d54ee",
        "IPY_MODEL_dfb591bf136e4d5e8c1b93dd1aff8e08",
        "IPY_MODEL_81d70c19aeda48e291d4c2eb122eef2d"
       ],
       "layout": "IPY_MODEL_690b5119a031426598eee4f15eafe41f"
      }
     },
     "6d004e368cce4fd786055255a4e18da0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_3b57cc435c3b43cf819d2a4c69838fd6",
       "placeholder": "",
       "style": "IPY_MODEL_0a1305b666b84c52b09e19b74a534717"
      }
     },
     "6d08dc1e3d234508b4c4d873734abcd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6d2e2ab5869d44aa87368f786e817a47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6d5d896f2fce4b749a8356b053a1ee38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_a696241ad06f43d8a78a236f8411f15d",
       "placeholder": "",
       "style": "IPY_MODEL_3c74412001224adb971da7f11aebdedb"
      }
     },
     "6dd9063a6f7c41a892490cda805df9b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ddddbb262a24b5492abe6c7a1572ef9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6df7552c452542eb84b8572f65892472": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "6e36be87f3744ae8bbdf2b23b9560f7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "6e3b5859401841b49374594ce418289f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6e48d590ec2843d99646796ccb1b85e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "6e4a338982a54de6aae032892831c15c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "6e923a41b8904be98b5865b4c7d5dac5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "6ebf61513af84eaea5f0c6098ec4377c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ec886221993418db21757de313b4ffa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "6f1be6526161496a83f6a69e4ac29c01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e4158ca0d44247de9cf06f251078dab6",
        "IPY_MODEL_52bd1f17b4034701bc1281d38ac976eb"
       ],
       "layout": "IPY_MODEL_b9cf3248c5ea466ea6da82ed3652fc27"
      }
     },
     "6f2ce04dae044a82821a0fbd5a88bc73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "6f44c684d1a44952be2b2cf88b26f1cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7c6eedda9d9d4c54990c0e6a44555b35",
        "IPY_MODEL_365a8d68668544a9b986b54db7c5ff13"
       ],
       "layout": "IPY_MODEL_7b22ffc1c0884e0b8441b072901bd12d"
      }
     },
     "6f6e0dc5903d42769aae7444b28d003b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "6f74fd95c2cb4798a206dffbe13a2b41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6f7ce3f5e36042e7af2010bae47a062e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_d4490c93a5e447e9acc538bb136028bb",
       "style": "IPY_MODEL_d1c61156c0c646e791769a72988339bb",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "6f9ab7df715b4d41a42f54d42fd73d44": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6faf735937094c69bacb598c6d9975eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_316ddae8f0714009adfe1f65aa3a6dfb"
       ],
       "layout": "IPY_MODEL_8349dc1c96a142ffbcf4cfeaaf40415b"
      }
     },
     "6ff4cb3983a54d798a554022e93294cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "6ff6b6b4cec84b71ad58e78d64359e20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "701d296b44e8411ea9b1e6bef8c7e72b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ccf328291f594acd94f2ee365cd545cf",
        "IPY_MODEL_3912faf4baba404991fe73b55b9ea4b6"
       ],
       "layout": "IPY_MODEL_ed7a54c72ad248de88064026b7cf63ab"
      }
     },
     "701e5ef75c3e4cc39126808da5c45fdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7084603e8e6343928305cc2435882439": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "70a671d0b40a49a2b5292935e5027f45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "70f71695bd294ce0abde65747ce0058b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_a6c00537dda54e47bfe81ef820721110",
       "style": "IPY_MODEL_2c889466d0c34a74a5bf260b9069682f",
       "value": "Discussion on how AI is being used to predict the progression and outcome of liver cancer."
      }
     },
     "70f816009c104bd6adfc0406b98bcc17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "70ff88c614d04ecc96dc520ccd4d73ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "713d99e386ed401fa0321bd9a9f35e8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7198459c258440488804f5e7716940f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "71ff6ac4c2db4fc2a7098f663072f5ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "7205b25f21b34149ba5bf0ece65bbcd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b72bc54c8f4d4ec6a8f033ab824f4a5b",
        "IPY_MODEL_3f9222be70d8499fae3a5e045328ad26"
       ],
       "layout": "IPY_MODEL_1c9c3ab952aa42cb99f41adc336f6b65"
      }
     },
     "72750fe1c1b4452eb28ed183cd3bd51c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "7374e220effb4f988a57fba2aebcd956": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a7d42fef79294203971e552a6dd84067",
        "IPY_MODEL_248c9ad2645d48a38185a213319087dd"
       ],
       "layout": "IPY_MODEL_f4172ce9ecb441218808a5dc3d1211b6"
      }
     },
     "7419e5b0e0874bd19b2392115f620db1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "741ebfaae67149bcbfb0750f4a2787e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "7436da714fcb4972923b303fd692f9ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e1ca0ad9af244524b6957a06bec7cdc2",
        "IPY_MODEL_34d0f747f116473da75122f8f9ebba64"
       ],
       "layout": "IPY_MODEL_d7452445c15842549dd17f430b8a8db6"
      }
     },
     "7449895b932147f4852825608a4f1541": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d28e32602d9341d69e8f632c91772a5e",
        "IPY_MODEL_5b03f8ebe19c4087a9fbf2239f1c2e83"
       ],
       "layout": "IPY_MODEL_499440aee80d4c53a57ff7e3a52e0bc5"
      }
     },
     "744efa524e774b1da21e060627183d9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_c9481ab8552f4b98b47968f00390caae",
       "style": "IPY_MODEL_85975b4deab149d2b994cee61e7179ea",
       "value": "Discussion on how multiple instance learning-based approaches contribute to visual explanations in AI-based liver cancer analysis."
      }
     },
     "747c51833dbc48e1a3444f971cc21fbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fdb83e2731d4477480e2398552bcdf32",
        "IPY_MODEL_b95119c1f3304ffab3841ac0bf34168c",
        "IPY_MODEL_4b8f1f7addad42bcbe4d391df1f0f696"
       ],
       "layout": "IPY_MODEL_8c73fdcf97594480953c747c879010c5"
      }
     },
     "74c0d2198ed0423c85ccfa52d980eea0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_615f88d28a4a4fbe9d24eec54a5c4a3a",
        "IPY_MODEL_0375240e2eaf40b5b7e59ad6c56d3fab"
       ],
       "layout": "IPY_MODEL_27fd710b2f4e41bd9dc7039f0b69472a"
      }
     },
     "74ffddb97ca8409a9d8ef7ee5dc5f87f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7534ded267bb4e47aeab94a1ce3c52b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1bf4775372f4433d88d8924d8d60736e",
       "style": "IPY_MODEL_1394efa36f30442997a73a073999d2e6",
       "value": "queries,"
      }
     },
     "75995e7b104d422fb0edf89fea22a7f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_670e653ddea74dad93ef27960528b755",
       "placeholder": "",
       "style": "IPY_MODEL_042a51824b25420d84cdec561ab5bc91",
       "value": "500"
      }
     },
     "75de1810e2ec4b169cf1fd95cdde269c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_75ea9c39960f4c56a250a47df667e297",
        "IPY_MODEL_12c7d4366e8a4075b63992ed92a34f66",
        "IPY_MODEL_0fbd53432ae046108176e0059c9e3f31"
       ],
       "layout": "IPY_MODEL_8f98c464d88f437e82483a5add0a8bcb"
      }
     },
     "75ea9c39960f4c56a250a47df667e297": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ff357e1b6cd0482dbdf34ab8c60453d0",
       "style": "IPY_MODEL_6f74fd95c2cb4798a206dffbe13a2b41",
       "value": " to "
      }
     },
     "76bce9577e474bc9a3fa2f0af62ba213": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "76c19729a08a4d378851c0b2767e48ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "76dc7c227ef44e5c8a55ff13d0d245cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "77045c62c02e40a2bfd8b818870f393c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "7727028662b54482969df602588a4a1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_94f1f00800af4d9e80c63c3613e7cd41",
       "placeholder": "",
       "style": "IPY_MODEL_aada7c0200d04cfbb00bfa6cf5f77941"
      }
     },
     "7759cab5c396493ca1aac5af20e0199f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3e3e49aa981d47c6b933bb4cfc7a7034"
       ],
       "layout": "IPY_MODEL_b93aabe35cb34be794848b4c7eb37045"
      }
     },
     "778d769071394172afa6aa2873d66d19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "77a0cb45b0904e179a13f21bdd820fef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "77fc4118e8354015adf3afb4da82b225": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_16f2126a8db147f4902e920376339b25",
        "IPY_MODEL_e1ffc9e73d8548e9bf7b3a111526ae29"
       ],
       "layout": "IPY_MODEL_f5bce05c77b448998c1f04b863ca28b9"
      }
     },
     "7850706c5f50404db8224ff3b7ec842a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7850f6f1b2bb4b978cdd682b97c0c4f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "78620c66a52744f39ed46d09728faaf0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "78715c0132c84b2f983534fa8a2559a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "787e38b33e674ef9aea90fa6c50f814c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "795eebb27eac40739547d11e29acfa21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_1a93d0dc7b214d888347796e0e6d6c5d",
       "placeholder": "20",
       "style": "IPY_MODEL_ae2bd67bd11440888352bfb1f143db2f",
       "value": "20"
      }
     },
     "79a20f5002d84d3a94d68bef04d2eb42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "79a6d033333c427ca084c7f5f191c9b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "79a8fc5c6cdd416680e1846f7870160c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "79d2d56a8b9b4f659f5871492915fdb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f467b4b728864207ab004172245cba9e",
       "style": "IPY_MODEL_95ac1abab5ea413d9c3ff99bde97ae2b",
       "value": "words:"
      }
     },
     "7a156ccc7e194a07b515bdbd95fec6d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7a5ce28ef86f4babad87c8e370823811": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_073e5d15308042e98bee27401616a533",
        "IPY_MODEL_a3018c625fef433eb7e1ce4907c8672a"
       ],
       "layout": "IPY_MODEL_9c12d3ab60334ae394d1497a7f25714e"
      }
     },
     "7ad66f96529749b69adccaa8c8c0bcc7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7afa82ee55e94132a42e36b840994a90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_a067d94b99f14a119c311ce726a84d3b",
       "style": "IPY_MODEL_bd90e9786de5477eb0f3613e7ff3d42f",
       "value": "1.3: Exploring predictive indicators for therapy response"
      }
     },
     "7b00f0c533364b6b84a0f12e11909fe1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_df962dd95868437b80884bb4c874abfb",
       "placeholder": "20",
       "style": "IPY_MODEL_d67d7ce6bdb44d78a0b1682c4dc9e61c",
       "value": "20"
      }
     },
     "7b22ffc1c0884e0b8441b072901bd12d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "7b3aec589e464bf59a6c60931b7f76a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7b3b3200df0e46e9b4df20dfabd92cc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_99f85f9bc50b4287b5c8b30698610d05",
       "style": "IPY_MODEL_a113d535b735427db55c31000775aa10"
      }
     },
     "7b6274e2bfb049888832f419a795f2d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7b99d280099a4c5e8243461d2fcc0b99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "7c1ac99db34a44449c7bea9396771d90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "7c1dce0a3d184f24adc60e269ae1d7b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_cd96534da6d0493b95b360696e3cce6a",
       "placeholder": "20",
       "style": "IPY_MODEL_12fc144615184f1b8d6f8d7873860a1c",
       "value": "20"
      }
     },
     "7c21f743e57f4f44bd26822ed8563ab3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7c3cf2518002447fa3fb5ef3af1da338": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7c6eedda9d9d4c54990c0e6a44555b35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_d3796c231ed44b7e9e20e989177b6914",
       "style": "IPY_MODEL_7fe9bfd3d47d4ec89f35a3346e5f4440",
       "value": "0: Unraveling the “black-box” of artificial intelligence-based pathological analysis of liver cancer"
      }
     },
     "7c9761a6de144acb9a595e840d34b0c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "7ccdeebe361e440c88fc6d26cb08d2a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_12affdc9e34d4acb91d6f002a7ee7260",
       "style": "IPY_MODEL_80e0f91cfee54a7594db5cb5d145212d",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "7d134349b91b475bac4a8728af1548b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "7d62e300347340748634aecab90799e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f8aeb69a1a404afc8c48a1aa3bfef6e6",
        "IPY_MODEL_512d7070a36041f9be5dfc8f3dd10c07",
        "IPY_MODEL_213a99b585104477af1d2a4b21141190"
       ],
       "layout": "IPY_MODEL_53b96e1a9fdb4befb2cf8283714550d5"
      }
     },
     "7de55bd58b704c03938e80eb31f5ed37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7dfbd069395c4a96936cc0f3e83ab2a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7e231c05793f46068454898a18b48f85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_416f69f88ae14667aa254e4f899af3b9",
        "IPY_MODEL_1728e328be57414ea39c581946cb19db"
       ],
       "layout": "IPY_MODEL_18f9006f98fa412cbfb4d907d1b9bbaa"
      }
     },
     "7e29f9fdd3c94a3ea80b2e1494078693": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7e51b5bc8b1d4e90b60cfbd12aab53ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7e63a7d032624dc0b0446930d84d2ef3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7e7b5922b10041e18e12d47976803acc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7eeffdb08c7346248f4038587a18f7c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "7f4ab3ece61a410c8c1b8495e7ee2379": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_d4e64d8d9dd74381ada8b52136c6d552",
       "placeholder": "",
       "style": "IPY_MODEL_442e53b481d249cd8566df50daf58d4b",
       "value": "500"
      }
     },
     "7f69097c0f814e7eb712c27b14f4f71f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_94614e7598c3424dab56113747a9b9f5",
       "style": "IPY_MODEL_34bfecdc947d4202b522cbfd267ee6ab",
       "tooltip": "Retrieve related references"
      }
     },
     "7f851422a0d14cd893c8c6df995bab66": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "none"
      }
     },
     "7f86c4a71d284b669b784bd7881ed8bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7f879a0e1d1f41338f62df8012251214": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7faf77e9325b48eab518456869ab6ede": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7fe9bfd3d47d4ec89f35a3346e5f4440": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "7ff56c3a828a41059870b399c7472784": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_1ad71bb1050b452fb266a4249f8d6694",
       "placeholder": "20",
       "style": "IPY_MODEL_08d4203db7684d5b88802687ab94f949",
       "value": "20"
      }
     },
     "802e922cd8224839b312b676f4e2e9ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bdbbd12760824bb2b5ff3a5da666f193",
        "IPY_MODEL_d928f5be474946819cea45f7e68f1546"
       ],
       "layout": "IPY_MODEL_63d689657d4f4c189c3a561469ddd3a5"
      }
     },
     "80622304edd347cf811b8f92d24c0fad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8066150bb1c9435e9c264443e2f468c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8084c35185f44bbf918def7196cfb78f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "80c99dd52110462e8d66b2447d3e0341": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "80d349f84a424b33abb1470e84ddd747": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "80d553412d204161ab4574f63923aacc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "80e0f91cfee54a7594db5cb5d145212d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "8138b247e1e3490c956213b9afa75999": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_873444c79d8b4debb655cb5923c4aaaa",
        "IPY_MODEL_c9beb4c5a8e44fc982802ec02cea4d41"
       ],
       "layout": "IPY_MODEL_a95d4dc1f88040d0831b8ab3defbfb5a"
      }
     },
     "813af26bfe564838b8614ed4b0d319fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_d40f855db0224e08aed7eef31b4a0d1a",
       "placeholder": "20",
       "style": "IPY_MODEL_9261a4bb0c294473b93ad29358e363c8",
       "value": "20"
      }
     },
     "816861eea38447ca88c8ffbbc399e36c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "818cb8e99cee497db955410f24a053f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_9f87ad7d600a4248904e3d414fd2972d",
       "placeholder": "20",
       "style": "IPY_MODEL_f8a9bb4b64494a5896246a44eec1631a",
       "value": "20"
      }
     },
     "81d70c19aeda48e291d4c2eb122eef2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_106183ba847046d88064416060143347",
       "style": "IPY_MODEL_96050941610f436c803835a557857485",
       "value": "words:"
      }
     },
     "820a40f8bd854fa78b82a29dbd165ada": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a2b6c03296994c4695495b7cef86d659",
       "style": "IPY_MODEL_5aecc44674844a36bf684a7893319346"
      }
     },
     "823e9232a1de4ef48b070a9a97369b66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "82a060d934654ebc9e8cb5cd59328809": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "8309b005ad2b4a45990991b1246f4a79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7b00f0c533364b6b84a0f12e11909fe1",
        "IPY_MODEL_493420701d384c22b8f0bf46c4eaaafb"
       ],
       "layout": "IPY_MODEL_8f9c7b5ddef04d18ba1e9e4bf36c3f4c"
      }
     },
     "83149feb7b9240fb8f4773f971e5f9a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8327758abc13446c85daaf471a912301": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_29d392f71c32400ca31706ef328807a9",
        "IPY_MODEL_a1485bfa776243cf8b91bb4c3499407e"
       ],
       "layout": "IPY_MODEL_0297760bd9cf46f3bf972abb40c498d1"
      }
     },
     "8349dc1c96a142ffbcf4cfeaaf40415b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8384935c3d684ddc9f2da87cb7a50cbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2655858f11b44996b6e9785b7705781f",
        "IPY_MODEL_7534ded267bb4e47aeab94a1ce3c52b6"
       ],
       "layout": "IPY_MODEL_d94d173cd6314c92ba5bf8c6bd275fb8"
      }
     },
     "8393a18c48a04593925c7dcbb1011252": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "83e699cbdd9b48b4822f3c1a472a847c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "8449968cde19412295bb7f4ee2b9a976": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_261325b862c0468581723bc2e0026bb5",
        "IPY_MODEL_62025d75629748aea5ed21d3b01b7737"
       ],
       "layout": "IPY_MODEL_0b5560022fa24917a5c2fbf00d69002b"
      }
     },
     "8493f59216474df58401db9996a30d64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8506e0ac2a7a4f6cadac249da11c31c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_46945bd813d64267bd3af558207d5841",
       "style": "IPY_MODEL_76bce9577e474bc9a3fa2f0af62ba213",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "854dd39513f84d41a7d8812c8dc8d68d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "856041e439c844df91dcbdda4d65c992": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c01ce5358eb94e0181a33b128281938c",
        "IPY_MODEL_d97b185418ca48a88bc90f26d046ed64"
       ],
       "layout": "IPY_MODEL_2fc2b5efe8574da68ecda5ac68b70ffe"
      }
     },
     "85975b4deab149d2b994cee61e7179ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "8661ba456a784ebaa923fb77b45980f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "867d7aff5d6746deb6aad565a4d73822": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "868bf9d6b3a648769af2b2f32c12068b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "86c77b512ae04a09803ca12d1914a1b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_41ba7a3f84854074a5d85425edebe07b",
       "style": "IPY_MODEL_dee9890805cc4112b11b7859008c754d",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.20070525);\n                color: white\n                ' title='Objective: To provide the current research progress, hotspots, and emerging trends for Al in liver cancer, we have compiled a relative comprehensive and quantitative report on the research of liver disease using artificial intelligence by employing bibliometrics in this study. '>\n                            Page 1, Region 11,\n                            Score 0.2\n                        </summary>\n                        Objective: To provide the current research progress, hotspots, and emerging trends for Al in liver cancer, we have compiled a relative comprehensive and quantitative report on the research of liver disease using artificial intelligence by employing bibliometrics in this study. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.19248655);\n                color: white\n                ' title='development of multimodal treatment plans for liver cancer could become the major trend of future research in Al in liver cancer. '>\n                            Page 2, Region 2,\n                            Score 0.19\n                        </summary>\n                        development of multimodal treatment plans for liver cancer could become the major trend of future research in Al in liver cancer. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.43112049);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.43\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.8253401);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.83\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.90162355);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.9\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.94155645);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.94\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.24334544);\n                color: white\n                ' title='FIGURE 2 Global trend of publications and citations on artificial intelligence research in liver cancer from 2003 to 2022. '>\n                            Page 4, Region 2,\n                            Score 0.24\n                        </summary>\n                        FIGURE 2 Global trend of publications and citations on artificial intelligence research in liver cancer from 2003 to 2022. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.88949215);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.89\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.3626391);\n                color: white\n                ' title='Research on AI in liver cancer mainly started in 2003 and entered a stage of rapid development in 2017. China is the most productive country with 35.33% of total publications; however, the USA ranked first according to the H-index, citations, and average citations per paper. It is notable that China, as a country with a high incidence of liver cancer, has a high number of studies on AI in liver cancer. However, most studies in China have limited impact, which may need further improvement from topic selection and research implementation. The League Of European Research Universities is the most productive institution, followed by Sun Yat Sen University and Zhejiang University. This is consistent with the conclusion of the most productive country above. We also found that cooperation '>\n                            Page 7, Region 6,\n                            Score 0.36\n                        </summary>\n                        Research on AI in liver cancer mainly started in 2003 and entered a stage of rapid development in 2017. China is the most productive country with 35.33% of total publications; however, the USA ranked first according to the H-index, citations, and average citations per paper. It is notable that China, as a country with a high incidence of liver cancer, has a high number of studies on AI in liver cancer. However, most studies in China have limited impact, which may need further improvement from topic selection and research implementation. The League Of European Research Universities is the most productive institution, followed by Sun Yat Sen University and Zhejiang University. This is consistent with the conclusion of the most productive country above. We also found that cooperation \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2655563);\n                color: white\n                ' title='The top three most productive journals had JCR scores of at least Q2. This shows that the field of AI in liver cancer is relatively mature and has a high level of concern and recognition. Moreover, most of the top 10 journals in this field are medical journals and include a small number of engineering journals, showing that the medical field pays more attention to AI in liver cancer. This suggests that we can consider and design studies from both medical and engineering aspects when conducting research, especially in medicine. '>\n                            Page 7, Region 8,\n                            Score 0.27\n                        </summary>\n                        The top three most productive journals had JCR scores of at least Q2. This shows that the field of AI in liver cancer is relatively mature and has a high level of concern and recognition. Moreover, most of the top 10 journals in this field are medical journals and include a small number of engineering journals, showing that the medical field pays more attention to AI in liver cancer. This suggests that we can consider and design studies from both medical and engineering aspects when conducting research, especially in medicine. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='FIGURE 6 A dual-map overlap of journals with studies researching artificial intelligence in liver cancer. '>\n                            Page 7, Region 11,\n                            Score 0.0\n                        </summary>\n                        FIGURE 6 A dual-map overlap of journals with studies researching artificial intelligence in liver cancer. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.36060905);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.36\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7806483);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.78\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.57902676);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.58\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 1.0\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.39473426);\n                color: white\n                ' title='Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) '>\n                            Page 1, Region 4,\n                            Score 0.39\n                        </summary>\n                        Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.22016332);\n                color: white\n                ' title='n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) '>\n                            Page 1, Region 5,\n                            Score 0.22\n                        </summary>\n                        n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.334082);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.33\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.12952687);\n                color: white\n                ' title='Owing to the broad heterogeneity in HCC risk factors and pathogenesis, established strategies for prediction and prognostication are still limited. Recently, artificial intelligence (AI) has emerged as a unique opportunity to improve the full spectrum of HCC clinical care, by: i) improving the prediction of future HCC risk in patients with established liver disease; ii) improving the accuracy of HCC '>\n                            Page 1, Region 12,\n                            Score 0.13\n                        </summary>\n                        Owing to the broad heterogeneity in HCC risk factors and pathogenesis, established strategies for prediction and prognostication are still limited. Recently, artificial intelligence (AI) has emerged as a unique opportunity to improve the full spectrum of HCC clinical care, by: i) improving the prediction of future HCC risk in patients with established liver disease; ii) improving the accuracy of HCC \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.117169276);\n                color: white\n                ' title='Another rapidly growing area of research is focused on improved characterisation of indeterminate liver lesions. In clinical practice, when an abdominal ultrasound shows a new liver lesion, a patient is typically referred for further imaging, with contrast-enhanced CT or MRI. Based on the fulfilment of specific radiologic criteria, certain liver lesions may be considered as having pathognomonic features of HCC, and thus do not require liver biopsy for further histological confirmation. However, liver nodules imaged by CT or MRI often demonstrate indeterminate features, for which current recommendations include either liver biopsy or close interval follow-up with serial imaging.”° This practice is sub-optimal, resulting in numerous imaging studies, patient stress, and the potential for delayed diagnoses of liver cancer. For this reason, a growing body of recent literature has explored AI approaches to improve risk stratification of indeterminate liver lesions, to facilitate earlier and more accurate detection of HCC. '>\n                            Page 4, Region 4,\n                            Score 0.12\n                        </summary>\n                        Another rapidly growing area of research is focused on improved characterisation of indeterminate liver lesions. In clinical practice, when an abdominal ultrasound shows a new liver lesion, a patient is typically referred for further imaging, with contrast-enhanced CT or MRI. Based on the fulfilment of specific radiologic criteria, certain liver lesions may be considered as having pathognomonic features of HCC, and thus do not require liver biopsy for further histological confirmation. However, liver nodules imaged by CT or MRI often demonstrate indeterminate features, for which current recommendations include either liver biopsy or close interval follow-up with serial imaging.”° This practice is sub-optimal, resulting in numerous imaging studies, patient stress, and the potential for delayed diagnoses of liver cancer. For this reason, a growing body of recent literature has explored AI approaches to improve risk stratification of indeterminate liver lesions, to facilitate earlier and more accurate detection of HCC. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "8725669db5794796af2dcd76b349d32e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_192d8eff5b21460e8021d6a84a350b7c",
        "IPY_MODEL_59dec56c69d04c8a9f7f48775c7ac33e"
       ],
       "layout": "IPY_MODEL_2b4adbf1321342ed8090297f74a0705a"
      }
     },
     "873444c79d8b4debb655cb5923c4aaaa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_7d134349b91b475bac4a8728af1548b3",
       "style": "IPY_MODEL_504550124a384a3aa4a2dfc7247189ea",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "8738c53c57d04c25ba4a080483037bfd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8797ba17844b4c5db0a1464ffc6d591d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c841dd65cfc4419fb46e95eee30e33f3",
       "style": "IPY_MODEL_5dcc502d68304692a6e89763157fb3b5"
      }
     },
     "87afaeb9c6f3420db2acf5c07adec03a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_962372e7de404d14962bcd6f29656898"
       ],
       "layout": "IPY_MODEL_b46d1db385314e2ba1184395320d7237"
      }
     },
     "87ba071c45c04586ac9c84ce109da4e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_164b4e84d339442788599f9eac06057b",
       "style": "IPY_MODEL_49499a6fbf3342caa6366879d90cb29b",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "87c66515e17648f4b0183c4b9fc013d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "87ce71ea5bdf405488617217c47a840e": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_bfc0c0ca24914d69bb49de0e486e7553"
      }
     },
     "87d0427d7bba4dba8d424e8dc4ab2ac1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4f93c362c6f449919cddad73439b3671",
       "style": "IPY_MODEL_0c9eee9e0eb44218a7bd87e7a3ed5c3a",
       "value": "queries,"
      }
     },
     "882c2dfe3b8942a1a2a04fd0cb548974": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "88400557d6394b7fbb9fcd95affba71f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "884a5a1f9be04f2d984efc0e5109c85e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7f69097c0f814e7eb712c27b14f4f71f",
        "IPY_MODEL_b04d7141215446a695f681e2016d390b"
       ],
       "layout": "IPY_MODEL_9cc0cc7aa6ac4519a1c428b77c8a8581"
      }
     },
     "8889f871d8d64d5980c9a1be08a77c58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "888a266f77124e87a51cd88bb6ae6f1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a54edd62e8da445d83eee1a895893b44",
        "IPY_MODEL_fb77388d36c74019b88e1f164a315618"
       ],
       "layout": "IPY_MODEL_90d9556069c7416ab4be4586ef82eb4f"
      }
     },
     "88a8e0f4d400412a96d5644ad62c2c56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "88af4473af2f42e2816c9c03b28fe084": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "88eccd4c254d41a2b47347b212a3badb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "none"
      }
     },
     "88f1f5652a6347ad9f76609285e4873d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "89234edd81214fbf87c536eab1d839b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_9d1a8acfcfbe49778590242be841cdcf",
       "style": "IPY_MODEL_4aa9c0ce64ba4daabafb0b891b26772c",
       "value": "3.1.3: Textual explanation"
      }
     },
     "8929ae86fd2e41deabc07b88715f117a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "89ce9e9d2d204dcb8cd3298b79ffab95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f03e7ab62044409ab5cb15afe65fd610",
       "style": "IPY_MODEL_e16593fd7ba3478b845bd28b8297b29e",
       "value": "queries,"
      }
     },
     "89cf022f35724c77870e34db236211ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8a311fee2adf48fe8aebaff84a95a2fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8a78f1a949734fa6a9a8afd3bb50659e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8aa6cda57dd640afaaeb68671e410bcb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "8ae23346cff94d4fbe129d46ba197d09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "8b1421de724043c4aa51121ffdf2bd32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_884a5a1f9be04f2d984efc0e5109c85e",
        "IPY_MODEL_ab6e5dd824774c6f8a631c7f91163b67"
       ],
       "layout": "IPY_MODEL_e2bc128729b0418088be7cfe6f56f981"
      }
     },
     "8b184d1db73e425d8d592257f24d882b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "8b753fb4212943a29d89b331e804de92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_33578c858bd54c1fa3e24fa9bda8b10d",
       "style": "IPY_MODEL_cb0e7e7c1bc447c1a2d58cd725394b0b",
       "tooltip": "Retrieve related references"
      }
     },
     "8baf6f7652d9446993908eb8e961f19c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "8c26bb734fb743a488d5f22cdabc1b94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_cd4214746a904ea2b681068b3acf2472",
       "placeholder": "20",
       "style": "IPY_MODEL_61d20fd9dd0d466db3b2d0ebf3f4d693",
       "value": "20"
      }
     },
     "8c281b7dcb5f4766bbe6308ca4d75f16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_df4434e0eb2d48e4865348a6b1dd8e28",
        "IPY_MODEL_13b254b04c2b48ec9c57f47664d41511"
       ],
       "layout": "IPY_MODEL_cbb115f467db48f9b31fdd5f03898e90"
      }
     },
     "8c33cb91ed714063b36c348c7cd9b049": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_01d9ba735e0d473ea1b06161a28a367f",
       "placeholder": "",
       "style": "IPY_MODEL_ec0e9407a1cc4f74a3d0eb10e002cc5e",
       "value": "500"
      }
     },
     "8c44225f3f3043cfba9e4501567cc975": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8c73fdcf97594480953c747c879010c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c913ad1f9f94762b7c09b21a0fec481": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3bdb8ac271594d2e91bf72ea76599c09"
       ],
       "layout": "IPY_MODEL_aa4eb84736664c6dbbcf7642750cec21"
      }
     },
     "8cb91bfa4d7a44e495600aece5b6240e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8cc1d33ce58046ca908687148c291559": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5bb00fde46c2477f98a3a8030ec32c9b",
       "style": "IPY_MODEL_e2054c2a31c24711a1bf1aa9de559753",
       "value": "queries,"
      }
     },
     "8cfd335851a44830b7f61172c919a17c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_686484db02f840ce86cfab06a0b86f4d",
       "style": "IPY_MODEL_a82e76e90ca042ed93c88e1e9eab2353",
       "value": " to "
      }
     },
     "8d6040674d3c4515b784c52810dbbba8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "8dc5da1f5cd0432a925c87b005053c72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8dd3cecb35c44f979fa703b4c958d994": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2c95f04e7b1e4873a9878b41853d5b92",
        "IPY_MODEL_ae2cffeb2c164f21bc19374dfc6cdd5d"
       ],
       "layout": "IPY_MODEL_d655a1de46544df3ad189db56b8d603d"
      }
     },
     "8e06a631c790468ea4fffc9ac9bc8d68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8eb6775f91704fc9be972e0dab1872be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8449968cde19412295bb7f4ee2b9a976",
        "IPY_MODEL_0853aacb77d94959a5e1f2fb88237501"
       ],
       "layout": "IPY_MODEL_46e19b99d84d460cad67aa67fbb167cb"
      }
     },
     "8ec035b5b6ab47a08bc6adf724b34a0d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "8ec44b551f93419ebc1e4417c2da82b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f471ae2dbbf4ba4a8d4b43e4248c696": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_505f43289bce49d0a2ecd3d1626e4508",
        "IPY_MODEL_121f2c1154d4436598bb07177854bcbe"
       ],
       "layout": "IPY_MODEL_466d617e82cc45a8a10ca9c877ab8ed4"
      }
     },
     "8f6060d70b0741d09049bc994e069dcd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "8f787aa01b6a487fbe30eae647d6f3bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f98c464d88f437e82483a5add0a8bcb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8f9c7b5ddef04d18ba1e9e4bf36c3f4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8fb8642dcfa2493fbce94a302af00b10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_12affdc9e34d4acb91d6f002a7ee7260",
       "style": "IPY_MODEL_70a671d0b40a49a2b5292935e5027f45",
       "tooltip": "Retrieve related references"
      }
     },
     "8fbc3274f7204e01a58903948c863f1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "none"
      }
     },
     "8ff23e078ae24e07b7d8f0da3074d4a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_8929ae86fd2e41deabc07b88715f117a",
       "placeholder": "",
       "style": "IPY_MODEL_5b278d7e9de0436188e5fca57bdfaf59",
       "value": "500"
      }
     },
     "901a05af035f44c3aa5c523af652d2fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "904a4f6e750e477b8e63776ffa6f8abf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9094304c640647389caa0e60e7118170": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "90d9556069c7416ab4be4586ef82eb4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "91016b79d47c4e14b3313adcc856c032": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "display": "none",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "910b2bd4f7304dd6b4408a37db02f38c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "91f1bda5b26f4a50adefed56c57b2169": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "925e2e9fb0c447b18dbcb52bb64707e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6523c75230d348e3ba958adaf828c275",
       "style": "IPY_MODEL_6e3b5859401841b49374594ce418289f",
       "value": "queries,"
      }
     },
     "9261a4bb0c294473b93ad29358e363c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "926579d666e3480d92423b5c220345b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "928e80be193e43779b562ea68b8e9aff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5039b58fd3ed4ff2a861ee34168ce5fe",
        "IPY_MODEL_d6b548f994354693a6e2c42bab71b49c"
       ],
       "layout": "IPY_MODEL_2f250540652c480fae6f99502ad6019b"
      }
     },
     "92bdb55fa4e447499f4be6ad3a273ca1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "92eab35ac7fc4c8bb248b064dab8916f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "931cc4a615ef459dbd6f88d42f5e6119": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "93255c424df3402087329cbd640fb1af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_956b4f8700df4e9ebc1100a61fb92ecf",
       "style": "IPY_MODEL_cdc27d0adec24f29a91291a11c822ab1"
      }
     },
     "933a4b117ff84228ae994acb87e1a7c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_03cf1792dae343d78877ab86fd849920",
       "style": "IPY_MODEL_c6893461251541af872fbe6caf19c3b1",
       "value": "Exploring how AI uses examples to provide explanations for its decisions in liver cancer analysis."
      }
     },
     "93526ec373814eecacc269009446beb9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "93ab294a3e874f41a836911bdca8e862": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "93b0d5ad2eb1490a829ed6ab345678c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fe6070b6d2b04ab9bb3740f00250374a",
        "IPY_MODEL_8c281b7dcb5f4766bbe6308ca4d75f16"
       ],
       "layout": "IPY_MODEL_dc45549bbeac466f9cd8a86879e70453"
      }
     },
     "93dcafd63ad048569e9749d029cae9c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_46f15d0fc76647818eb88e77cf2f8979",
        "IPY_MODEL_390e53c6536f4a6ebd4e79bf05f8bc45"
       ],
       "layout": "IPY_MODEL_8f6060d70b0741d09049bc994e069dcd"
      }
     },
     "93fa1634c2a94ab1aa1f3cd878f08983": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9419dddda7ab49d989eda8b1be800193": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "945e24076a2040a3b14ab3a49cd96205": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "94614e7598c3424dab56113747a9b9f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "9474a3d2e8b442d8b90e27d2d7f1e70a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "94a16c7b43564d59b31ac4ebeea7cf0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "94f1f00800af4d9e80c63c3613e7cd41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "956b4f8700df4e9ebc1100a61fb92ecf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "95814dfbcc244121bacd229133995c35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_d8db84ac9922450d8f747a52dcf3e656",
       "style": "IPY_MODEL_cd6d5336b259466997bf7a3bc5e779bb",
       "value": "3.2.3: Example-based explanation"
      }
     },
     "959f43bc782945d4b68b6208f43ab229": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d76445354e7e4469a13abc3c1951276b",
        "IPY_MODEL_300c727d8ce9461aa3e2fa4e198513c3"
       ],
       "layout": "IPY_MODEL_b40dba7f160f4b028134462c07baa4f9"
      }
     },
     "95a8fc5a389f444089ba551b7883ab9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_fb2071c18bcf4e04ba8a62947386ce1f",
       "placeholder": "20",
       "style": "IPY_MODEL_51a55dcd613947b88873b5222f50aa5c",
       "value": "20"
      }
     },
     "95ac1abab5ea413d9c3ff99bde97ae2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "95d52bb292f3466f8632022c2afd356a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "95ec822807fa42bcb1c88cb37da6acb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_c65f4f81e8c04333a238ee436ead0fbc",
       "style": "IPY_MODEL_b9c8bd19b46b46459596b486c7eb4c26",
       "value": "3.2.1.2: Perturbation-based approaches"
      }
     },
     "95ff44a3c1634819a38087d6cf8a5333": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_de1efe48578c414da4a157b6194455a1",
       "placeholder": "",
       "style": "IPY_MODEL_44f46c2c3097455daaacc16a631d3098"
      }
     },
     "96050941610f436c803835a557857485": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9615ff12db3b434194ba2d87b6d8d3f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "962372e7de404d14962bcd6f29656898": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_daed058da2b94ab1ac76c81f41e975a1",
       "style": "IPY_MODEL_f5bcfe88426b49f3b9e887e7be471c8a",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.70587236);\n                color: white\n                ' title='Textual explanation is a form of XAI that provides textual descriptions. Such descriptions include relatively simple characteristics (e.g. ‘spiculated mass’), up to entire medical reports. We will describe three types of textual explanation: image captioning, image captioning with visual explanation, and testing with concept attribution. '>\n                            Page 8, Region 20,\n                            Score 0.71\n                        </summary>\n                        Textual explanation is a form of XAI that provides textual descriptions. Such descriptions include relatively simple characteristics (e.g. ‘spiculated mass’), up to entire medical reports. We will describe three types of textual explanation: image captioning, image captioning with visual explanation, and testing with concept attribution. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.500319);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.5\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.013382335);\n                color: white\n                ' title='Results: 1724 papers were collected in this study, including 1547 original articles and 177 reviews. The study of Al in liver cancer mostly began from 2003 and has developed rapidly from 2017. China has the largest number of publications, and the United States has the highest H-index and total citation counts. The top three most productive institutions are the League of European Research Universities, Sun Yat Sen University, and Zhejiang University. Jasjit S. Suri and Frontiers in Oncology are the most published author and journal, respectively. Keyword analysis showed that in addition to the research on liver cancer, research on liver cirrhosis, fatty liver disease, and liver fibrosis were also common. Computed tomography was the most used diagnostic tool, followed by ultrasound and magnetic resonance imaging. The diagnosis and differential diagnosis of liver cancer are currently the most widely adopted research goals, and comprehensive analyses of multi-type data and postoperative analysis of patients with advanced liver cancer are rare. The use of convolutional neural networks is the main technical method used in studies of Al on liver cancer. '>\n                            Page 1, Region 13,\n                            Score 0.01\n                        </summary>\n                        Results: 1724 papers were collected in this study, including 1547 original articles and 177 reviews. The study of Al in liver cancer mostly began from 2003 and has developed rapidly from 2017. China has the largest number of publications, and the United States has the highest H-index and total citation counts. The top three most productive institutions are the League of European Research Universities, Sun Yat Sen University, and Zhejiang University. Jasjit S. Suri and Frontiers in Oncology are the most published author and journal, respectively. Keyword analysis showed that in addition to the research on liver cancer, research on liver cirrhosis, fatty liver disease, and liver fibrosis were also common. Computed tomography was the most used diagnostic tool, followed by ultrasound and magnetic resonance imaging. The diagnosis and differential diagnosis of liver cancer are currently the most widely adopted research goals, and comprehensive analyses of multi-type data and postoperative analysis of patients with advanced liver cancer are rare. The use of convolutional neural networks is the main technical method used in studies of Al on liver cancer. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.38484302);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.38\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.499398);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.5\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.75063);\n                color: white\n                ' title='literature in this field. Additionally, a summary and quantitative analyses of the global development trend and research hotspots of AI in liver cancer is of great importance for future research. Bibliometrics is a method of information visualization which can achieve quantitative analysis of literature in a specific research field in a worldwide context through statistical methods and visualizing the results with the help of software (26-29). Bibliometrics plays an important role in sorting out development trends and research hotspots of a given field and has been widely used in many fields (26-29). '>\n                            Page 2, Region 7,\n                            Score 0.75\n                        </summary>\n                        literature in this field. Additionally, a summary and quantitative analyses of the global development trend and research hotspots of AI in liver cancer is of great importance for future research. Bibliometrics is a method of information visualization which can achieve quantitative analysis of literature in a specific research field in a worldwide context through statistical methods and visualizing the results with the help of software (26-29). Bibliometrics plays an important role in sorting out development trends and research hotspots of a given field and has been widely used in many fields (26-29). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5075352);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.51\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 1.0\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.27508274);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.28\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.052615255);\n                color: white\n                ' title='The top three most productive journals had JCR scores of at least Q2. This shows that the field of AI in liver cancer is relatively mature and has a high level of concern and recognition. Moreover, most of the top 10 journals in this field are medical journals and include a small number of engineering journals, showing that the medical field pays more attention to AI in liver cancer. This suggests that we can consider and design studies from both medical and engineering aspects when conducting research, especially in medicine. '>\n                            Page 7, Region 8,\n                            Score 0.05\n                        </summary>\n                        The top three most productive journals had JCR scores of at least Q2. This shows that the field of AI in liver cancer is relatively mature and has a high level of concern and recognition. Moreover, most of the top 10 journals in this field are medical journals and include a small number of engineering journals, showing that the medical field pays more attention to AI in liver cancer. This suggests that we can consider and design studies from both medical and engineering aspects when conducting research, especially in medicine. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.17485632);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.17\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.523509);\n                color: white\n                ' title='In terms of clinical goals, the diagnosis and differential diagnosis of liver cancer on medical imaging are still major research priorities (19, 49-53). However, the clinical diagnosis of liver cancer is a comprehensive process, especially because of the variety and atypical characteristics of focal liver lesions. For example, dysplastic nodules in the state of liver cirrhosis have strong malignant potential, especially high-grade dysplastic nodules, and they are difficult to distinguish from early liver cancer in imaging. A comprehensive evaluation of the clinical indicators of the patient is usually required, including alpha-fetoprotein and abnormal prothrombin (54-58). However, there are still few studies that combine multiple types of data such as genetic data, molecular data, imaging data, and clinical indicators, and lack the support of large data and multi-center studies. '>\n                            Page 8, Region 7,\n                            Score 0.52\n                        </summary>\n                        In terms of clinical goals, the diagnosis and differential diagnosis of liver cancer on medical imaging are still major research priorities (19, 49-53). However, the clinical diagnosis of liver cancer is a comprehensive process, especially because of the variety and atypical characteristics of focal liver lesions. For example, dysplastic nodules in the state of liver cirrhosis have strong malignant potential, especially high-grade dysplastic nodules, and they are difficult to distinguish from early liver cancer in imaging. A comprehensive evaluation of the clinical indicators of the patient is usually required, including alpha-fetoprotein and abnormal prothrombin (54-58). However, there are still few studies that combine multiple types of data such as genetic data, molecular data, imaging data, and clinical indicators, and lack the support of large data and multi-center studies. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.30200365);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.3\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7915613);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.79\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7939791);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.79\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Implementation of deep learning in liver pathology optimizes diagnosis of benign lesions and adenocarcinoma metastasis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.29908702);\n                color: white\n                ' title='In summary, we show for the first time that a comprehensive series of automated identification and classification of common benign and malignant lesions in the liver is possible by deep learning on scanned histological tissue sections. Our work can contribute to an objective and efficient workflow in routine diagnostics for highly relevant diagnostic questions, such as the differentiation between benign and malignant structures and the origin of frequent types of metastasis. This tool may aid pathologists, especially in situations where limited tissue is available, to establish and confirm the diagnosis. Furthermore, we provide an exceptional annotated liver dataset for the development and validation of deep learning algorithms which we provided to the scientific community. At the end, this may be a step towards improved personalized oncology therapy concepts, which will in the future integrate large clinical, radiological and pathological data sets using artificial intelligence. '>\n                            Page 12, Region 6,\n                            Score 0.3\n                        </summary>\n                        In summary, we show for the first time that a comprehensive series of automated identification and classification of common benign and malignant lesions in the liver is possible by deep learning on scanned histological tissue sections. Our work can contribute to an objective and efficient workflow in routine diagnostics for highly relevant diagnostic questions, such as the differentiation between benign and malignant structures and the origin of frequent types of metastasis. This tool may aid pathologists, especially in situations where limited tissue is available, to establish and confirm the diagnosis. Furthermore, we provide an exceptional annotated liver dataset for the development and validation of deep learning algorithms which we provided to the scientific community. At the end, this may be a step towards improved personalized oncology therapy concepts, which will in the future integrate large clinical, radiological and pathological data sets using artificial intelligence. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Explainable medical imaging AI needs human-centered design a systematic review</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2505962);\n                color: white\n                ' title='specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions '>\n                            Page 7, Region 4,\n                            Score 0.25\n                        </summary>\n                        specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) '>\n                            Page 1, Region 5,\n                            Score 0.0\n                        </summary>\n                        n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.3092369);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.31\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.010831462);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.01\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "964b66725afc411ab06e0157af1f4f49": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "96874bfd9fbc4073a6faf8e986cecb74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "96a9b8a9024e462a9708d4dca3a58e6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "96b992ff33234390a754425d71f8a769": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c2012f04077745f6936e69865dc121c1",
        "IPY_MODEL_f4a91ce12f684b4ab95e88036eefe612"
       ],
       "layout": "IPY_MODEL_4503c57a38524581b400b5d230624c7e"
      }
     },
     "9769b1d00f6d45f2a11c99a921cd5932": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_19d3108656964a04868a81e0bdc75a9f",
        "IPY_MODEL_d0dd7962bf1a43079001d3be18d2659c",
        "IPY_MODEL_f2427886019b41b49ba047df07966100"
       ],
       "layout": "IPY_MODEL_0de2632dd52c4ba9b46af10c2a987ebc"
      }
     },
     "97aeae4cf3f042c1a4945aa2280151d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5597819ef1a847d9b00b9afa95900262",
        "IPY_MODEL_edee4603c7dc4544b9ef3cab703a3700"
       ],
       "layout": "IPY_MODEL_0e5527f389b34c9a9d152a140d05e62a"
      }
     },
     "97b39a5f81364133835a504ac3c4f7a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "97b931f4e5a0453399bf82362e388caa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_53526918d45f480a9c805c33f0b8be56",
       "placeholder": "",
       "style": "IPY_MODEL_fcb4e51dd6c541a3b15db1a96256118f"
      }
     },
     "97ef4ae69c574bd4a0214f70e572b110": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "981074ae1e204d38a809586134a3f492": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "9826976d4abe480c9bb6b318cf867f34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "983b962db6b44587b3bca86f134e7f1b": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_6d2e2ab5869d44aa87368f786e817a47"
      }
     },
     "989cd17860ba4f078b87bdc028d9e27b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "98fcc25ef64c4c80bf0b59b8a904205d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "99225c51f7924cc4914943885e3a9e97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "992c39fcbccb42349a474990102af1d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_2cfe5abc0da8460290c10e8b33fb36b7",
       "placeholder": "20",
       "style": "IPY_MODEL_910b2bd4f7304dd6b4408a37db02f38c",
       "value": "20"
      }
     },
     "993e72482d724921a4eec1ab6812c1ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "9951360b74d54e6b9c88f2d2c2753130": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9955f46ea6784f609906ff99ab8bb240": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "99f85f9bc50b4287b5c8b30698610d05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9a200dab8f3a4836b074d481c9635051": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_d4490c93a5e447e9acc538bb136028bb",
       "style": "IPY_MODEL_816861eea38447ca88c8ffbbc399e36c",
       "tooltip": "Retrieve related references"
      }
     },
     "9a24c0facce242158b83e764d796096c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9a447af3f9794e3eaa55ebc762019e79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ef81e602cf7e4ce5a1241754a3458681",
        "IPY_MODEL_2288250ebaf34a03b359031b92e1e943",
        "IPY_MODEL_fdd5b1b1f4e2494eafb9dedaa7c39e68"
       ],
       "layout": "IPY_MODEL_a9961fdc2d1c4b3dba4638d789d3f601"
      }
     },
     "9a4a066304a04042983fdfd20bf85175": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9b1c0f32594740d79b08971a04fa97b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "9b665304f74749b6a9d44a1e459a6d23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e2d69f623e97480ba02b12bc814e6ee6",
        "IPY_MODEL_8cc1d33ce58046ca908687148c291559"
       ],
       "layout": "IPY_MODEL_a5eff92c5a6243c0b7659fa7c9ffb64a"
      }
     },
     "9b66590e1c6d46ba855c5c6bbbf6e877": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_173d3eb0c7804760be049d211f6232ae",
       "placeholder": "",
       "style": "IPY_MODEL_8889f871d8d64d5980c9a1be08a77c58"
      }
     },
     "9c12d3ab60334ae394d1497a7f25714e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "9c81866b1888464d899d8cf8abeaac08": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "9cc0cc7aa6ac4519a1c428b77c8a8581": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "9cc5ddec9dae4b2c8dbb30b3987771c6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9cd265360e254111b6975a7953ca437d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d1a8acfcfbe49778590242be841cdcf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "9d2835c9330d49d6b298e69cb6a7f0f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9d2d980577564ab29190b03ddcab0984": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9da176d99e3e411dba14cedb0d67cd8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "9dbeee34612d4f23ba68013d1e147586": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_0d73a3d0a6b343c48902af7d7cb353c7",
       "style": "IPY_MODEL_cc6f5c272b554b659a834a2b982d6da3",
       "value": "Insight into how AI can generate textual descriptions for pathological images in liver cancer analysis."
      }
     },
     "9e2da0f0005040f984e3a86e35329dde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_309e2afa2d1b40d381a5d638ea298c72",
       "style": "IPY_MODEL_6ec886221993418db21757de313b4ffa",
       "value": "Discussion on post hoc explanations, which provide insights into AI decisions after they have been made in the context of liver cancer analysis."
      }
     },
     "9e3369feb3cd4c55a816c9131163510e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "9e445ab99e5c4d48ae724f303e512d02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e769fa43b1e444c90b12332985e814e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "9e862949626542aeba72f248a4c1f05d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e8d16f9d5aa4807b513d147274e4668": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "9ee2f53f30e14a2bb2c6e839302a7d23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_032de5e8923e46108f2ca8622bda8a9d",
        "IPY_MODEL_2db861e9d2cc48b382835a79e0341447"
       ],
       "layout": "IPY_MODEL_8aa6cda57dd640afaaeb68671e410bcb"
      }
     },
     "9eff32b2b9b84e7a8afdb2bc499d0b31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e0b459e14621424593223bc34aa84ac1",
        "IPY_MODEL_be9b38f12d234b57bcaa5a1b27213fde"
       ],
       "layout": "IPY_MODEL_e00eccbe1cd94c15886b1b7f9f35a89f"
      }
     },
     "9f37c73d13f540ca805d0aa0c2a1e4a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "9f87ad7d600a4248904e3d414fd2972d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "9fa873799b4747ce866fecf1c809ddaa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a048a4b88124484c99ec2891e0090ffe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "none"
      }
     },
     "a067d94b99f14a119c311ce726a84d3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "a06c88a131894251a2b2c5da6d21a602": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "a078d3d8dd6d404e8916c2b30670ad0e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "a0df8e28d9fc4948931cecd107d1dac2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f7f0fc8805204b79879b45d1d0cbb8fe",
       "style": "IPY_MODEL_189f4216e4294ded9a040092bddc414a",
       "value": " to "
      }
     },
     "a113d535b735427db55c31000775aa10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a13b1860a4b74342b7bbe6a9955fd7cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1da1d6f7bfe14ce29ee94bbbaa1ccb5c",
        "IPY_MODEL_1233988ff48c42999c5e6545ada30824",
        "IPY_MODEL_1dcc170a3cda487f963107832550274f"
       ],
       "layout": "IPY_MODEL_67d99ea98b014f5589ab59e3fceaf5ee"
      }
     },
     "a1485bfa776243cf8b91bb4c3499407e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fab6fff9f748475684c68d036babc64c",
        "IPY_MODEL_8ff23e078ae24e07b7d8f0da3074d4a1",
        "IPY_MODEL_4c216e576ebd4fa89f1ea33f372eb060"
       ],
       "layout": "IPY_MODEL_bda170f4a7c04a988375f7fa42bc5313"
      }
     },
     "a17cfba9bd274f61aabb41f9592d6dfc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "a19adc7520c64875bf8c405d32df1a76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a21e3513982046e08bc2fbdfd1a374cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4769aefabb9f40a18f88c7d11e7a0072",
       "style": "IPY_MODEL_823e9232a1de4ef48b070a9a97369b66"
      }
     },
     "a2804d23bbd04f38ae6619a9e045e658": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a2805669f40546c6bfffc105244dd6b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a2b6c03296994c4695495b7cef86d659": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a2eda9834b6a493898eeae793f3b72b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e3143352597b4dccaa77e23bd12622e6",
        "IPY_MODEL_d84e4670add64588ba4df0fb2c4e4f9a"
       ],
       "layout": "IPY_MODEL_931cc4a615ef459dbd6f88d42f5e6119"
      }
     },
     "a2fdaeeb3e844e7e946185d442632a6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_063f610666594d21a937314ab58015a1",
       "style": "IPY_MODEL_6ff4cb3983a54d798a554022e93294cb",
       "value": "3.2.1: Visual explanation (saliency mapping, pathologist-in-the-loop)"
      }
     },
     "a3018c625fef433eb7e1ce4907c8672a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8b1421de724043c4aa51121ffdf2bd32",
        "IPY_MODEL_a654841a67b342e2a9edf702f760755b"
       ],
       "layout": "IPY_MODEL_8fbc3274f7204e01a58903948c863f1a"
      }
     },
     "a324b885cff84f6c9eed967d01f6d9ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "a344881ccdc649b5b812784f36aa4ecc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_04f83343973b469483e5d1169c75ffa9",
        "IPY_MODEL_ff1fef0a4d514b93aa2d471f89773c34"
       ],
       "layout": "IPY_MODEL_dbaf3c24684a4926aaec39c120f2490c"
      }
     },
     "a38cbe8130a541d98de4a2fbaf5b0847": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a396875fff04407a8d44d40bcc0c09dc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "a40e5b3ef3aa4ac1bc9b8fc3b1ae6cfd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "a49a9a23dd0948f6a99630d8da5289dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a54edd62e8da445d83eee1a895893b44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b5e3897334c243069b344369a9fa75f7",
        "IPY_MODEL_6b0eb20b0ff84ff19fbd78442e2ae3b9"
       ],
       "layout": "IPY_MODEL_cf120d7f126e45caa4465161f257e939"
      }
     },
     "a55f422440a04dcca2d01d978f631274": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "a5eff92c5a6243c0b7659fa7c9ffb64a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6376c2e8dc649fbbde859279f78d605": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "a654841a67b342e2a9edf702f760755b": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_93ab294a3e874f41a836911bdca8e862"
      }
     },
     "a677615226344d4cae803b74c63eb02b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "a696241ad06f43d8a78a236f8411f15d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "a6a90623c63f4deab79073d4c546fae6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1e34d12cadf64241b544fb1ab5ccb1f6",
       "style": "IPY_MODEL_926579d666e3480d92423b5c220345b7",
       "value": "words:"
      }
     },
     "a6c00537dda54e47bfe81ef820721110": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "a6cd6c822f1941baade1e210ac6c94ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_46945bd813d64267bd3af558207d5841",
       "style": "IPY_MODEL_68b10eef9f02482db42777597308f379",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "a6ef25b712024faa8f858db73cc51d5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "a77bfc54cdf64065b96fce3a316ddb9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a78c2579c1f94410adc590113fdac7cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "a7d42fef79294203971e552a6dd84067": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_88a8e0f4d400412a96d5644ad62c2c56",
       "placeholder": "20",
       "style": "IPY_MODEL_61a65711f2a245698325d0644f6052a2",
       "value": "20"
      }
     },
     "a82e76e90ca042ed93c88e1e9eab2353": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a849e0e203fd4a48ab8f8f703046ea6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "a8c28c50dc894eff88f42fdd72ce7ed5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "a95d4dc1f88040d0831b8ab3defbfb5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "a97af0189b9b44da8cb3c89a3bd1bff7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a9961fdc2d1c4b3dba4638d789d3f601": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aa0c55d1084f46fd827d1fed66405f32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "aa11a3acdf204a31a177a7a4320d2d10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "aa153e1d637e4d9fa6455bbaba3b1320": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aa1abf754f54481ca06bf6ca630f23bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e0dffe8fe097464bb1be36cb679fd3be",
        "IPY_MODEL_bf904de75cd54c44b8cd8a6666192488"
       ],
       "layout": "IPY_MODEL_f559b1667775498aa2591060573aa976"
      }
     },
     "aa4eb84736664c6dbbcf7642750cec21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aa631bcbc80a4a1680116964c2c94bd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aa9f6c07522544fcb608d280dcfde4bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d1fc91d952f04138888c2991ab06d1b9",
        "IPY_MODEL_d7f5b73991104ed3aebe6a72b7a9e903"
       ],
       "layout": "IPY_MODEL_1dcee06f2dce46b58f3db50899c40690"
      }
     },
     "aada7c0200d04cfbb00bfa6cf5f77941": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "aaffe5746100410288def54a80137ab7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_079cb1bc90ba4602b2792b4a330e272d",
       "style": "IPY_MODEL_7b3aec589e464bf59a6c60931b7f76a2",
       "tooltip": "Retrieve related references"
      }
     },
     "ab6e5dd824774c6f8a631c7f91163b67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_05c82618049543a48914ca149f3c151b",
        "IPY_MODEL_a13b1860a4b74342b7bbe6a9955fd7cc"
       ],
       "layout": "IPY_MODEL_2431f981398746c499b382f12693260e"
      }
     },
     "ab96a947c25b41569298c749c766cd7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aba82963d7d245089408a4cf53e795ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "abbcdfa5eb73497c87a80040b575dc36": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "abcca768afbe42dc9396e9e6562043ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "ac1bdabfa6fc4a04a1852097e07a8e7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "ac8410b9d12840d8b3c183983eba5c16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_8393a18c48a04593925c7dcbb1011252",
       "placeholder": "",
       "style": "IPY_MODEL_b7c9712132464b2a8c6418845abb428c",
       "value": "500"
      }
     },
     "acae1f52f42c4e4c8475827c52c60045": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ad273c3dbcf24d4d889e1b39bf5e75dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_dd46d1296f1f42708a6342dd26ece555",
        "IPY_MODEL_cad623efd0a84e78b171be078a47e739"
       ],
       "layout": "IPY_MODEL_343311a0ddaf4640ab1cc6bfa2620f1f"
      }
     },
     "adc56abe37e04c6b83380b03c61c5df3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2c5556b3ec4b44a9a2eed8e0eae19074",
       "style": "IPY_MODEL_94a16c7b43564d59b31ac4ebeea7cf0e",
       "value": "words:"
      }
     },
     "adf7c53353b74c2e9802ba483daae88a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "ae2bd67bd11440888352bfb1f143db2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ae2cffeb2c164f21bc19374dfc6cdd5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a2eda9834b6a493898eeae793f3b72b1",
        "IPY_MODEL_fbd3061e6c594ab8856b566e6562dfa3"
       ],
       "layout": "IPY_MODEL_f0c116f5b89247e6a5d885611346a7a4"
      }
     },
     "ae2de312d95f4ca394e4ac97dc4634fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ae9514d4cbd347338b1bddd4df042c67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_91016b79d47c4e14b3313adcc856c032",
       "style": "IPY_MODEL_459c801d8fa7477aaf516503118439da",
       "value": "Understanding how textual explanations can be generated from AI models for liver cancer analysis."
      }
     },
     "aec0cd4c1cfa4a6fa4bdd256db239a06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_91f1bda5b26f4a50adefed56c57b2169",
       "style": "IPY_MODEL_7de55bd58b704c03938e80eb31f5ed37"
      }
     },
     "af68b67ec7e9487190905798b039a057": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_82a060d934654ebc9e8cb5cd59328809",
       "style": "IPY_MODEL_deb80324abf2464ba8eb328d9d95d044",
       "value": "Understanding how the triplet network, an example-based explanation model, works in the context of AI-based liver cancer analysis."
      }
     },
     "af945cca68584f9392ca266cd90d57b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "af9dec0af29845449304c92e9b6d90bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "afc5b5df60ff4922bc54d90134857f91": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_93fa1634c2a94ab1aa1f3cd878f08983"
      }
     },
     "afdd62bbfb2e41e6b9e7c37b4379e067": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "b005e4173ca142098a18b9c4ad5d6c8e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b00d29cf641b48179d26ca5594b3a77d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_d65418de9605423895928d09a6b6ec64",
       "style": "IPY_MODEL_b0339b4d1586416d83436f89a2e0ae84",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "b0218bc33bbb4fd1bae624ef223fbf16": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "b026ed74e17842f1915d506fbac004f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "b0339b4d1586416d83436f89a2e0ae84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "b03d6123c9ac40a9b26f3b3246e011e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "display": "none",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "b04d7141215446a695f681e2016d390b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8c26bb734fb743a488d5f22cdabc1b94",
        "IPY_MODEL_39d93033354e4074886a8d1370081931"
       ],
       "layout": "IPY_MODEL_d858ff8ad659472fb60187b5ce9dafc0"
      }
     },
     "b065da259c6d42b5bb3a9191344bf2fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5a128a9af3cc44f4abc15b3f0346d5bd",
        "IPY_MODEL_10840a4be51543fe92390c1322928db4"
       ],
       "layout": "IPY_MODEL_fb6de84608ce4cdabc606de3f5765b21"
      }
     },
     "b108b8b38f924491bd42efc6a2afc507": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "display": "none",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "b117e0b343ca4198a976ff3a000c4a29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "b11ca6d3b9b045b895494b23b9ff057e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b1c791cc493c44d49da569856080efff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_80d553412d204161ab4574f63923aacc",
       "style": "IPY_MODEL_e852f8b85262443c8b34265f57b63076",
       "value": "queries,"
      }
     },
     "b1c8ea4c7514411c9fd01979c8be3b83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b20c748bc74943ddb1252023173c650c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_46945bd813d64267bd3af558207d5841",
       "style": "IPY_MODEL_4887717b4dbf4222bf90295c4c5fa10f",
       "tooltip": "Retrieve related references"
      }
     },
     "b22e9f20041745eead70fe525d9d48ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "b2a73932c6ff4104a77d10cb1f2d63e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f9fd1c7449cf4579a273a9ead14a795f",
       "style": "IPY_MODEL_7f86c4a71d284b669b784bd7881ed8bb",
       "value": "words:"
      }
     },
     "b2d93eb490964f1b8e09357f3e759c0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_945e24076a2040a3b14ab3a49cd96205",
       "style": "IPY_MODEL_3f7456eb16c04af893b896fbb53fc69f",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "b331c86710c04ada923bb9a5b3b8dc9f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b345a4eaf2d24cd7b094e3171901f53b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5614f3055aaf4960af17f71b6d815b3a",
       "style": "IPY_MODEL_1602c6c50ed54ec6b233ca1f2fdf983f"
      }
     },
     "b3ac931200ed41daa616b3524e7df350": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_9a200dab8f3a4836b074d481c9635051",
        "IPY_MODEL_9b665304f74749b6a9d44a1e459a6d23"
       ],
       "layout": "IPY_MODEL_2457c7216d9440cb8526b8b4ec0462e1"
      }
     },
     "b40dba7f160f4b028134462c07baa4f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "b424a2223b9040058d660aa7c164de4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "b43653dd95254f6e9ecd8aa91d3cbb26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "b46d1db385314e2ba1184395320d7237": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b47b9592339543c784502c7854a05e24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "b48b59c4f55041c89dcc64579f8c4ca2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b4f28fa7ccd64c4e901a08f5eacdec13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "b531cb8981884cb6b1991d2251e7fd71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "b536ace00c0d4fcb8f53a1d1d5f9a2b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "b58349a78e81470a9cf0c90c53b61ecf": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_83149feb7b9240fb8f4773f971e5f9a9"
      }
     },
     "b5a89e3a5ffe4810b0fca965191a8020": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7afa82ee55e94132a42e36b840994a90",
        "IPY_MODEL_ef9f0c16d43a4a21922583b73e149ef6"
       ],
       "layout": "IPY_MODEL_6e48d590ec2843d99646796ccb1b85e6"
      }
     },
     "b5a91ab8edf44141a84b5cd222bf255f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b5e3897334c243069b344369a9fa75f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1c2d69d77aa147e7b77e4c3700cc4649",
        "IPY_MODEL_51a57990af88439ba5a301fbc846442f"
       ],
       "layout": "IPY_MODEL_70ff88c614d04ecc96dc520ccd4d73ae"
      }
     },
     "b5f100e13b7f40009471a1baf851be33": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1c7d12f6bd484820b537f3215fb91551",
       "style": "IPY_MODEL_eccda12b9604438281550f14be96f8ad",
       "value": " to "
      }
     },
     "b642f52ece994add9bce0bfe4ea3b471": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b69fa21883ab4a5d8164d2bd98902152": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "b6cea8ad164348e3a579e9471253288f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "b6dbcae4cb8b434f8a68aded30f1734a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c644b732ee854e328cb92c575487bea8",
       "style": "IPY_MODEL_80622304edd347cf811b8f92d24c0fad",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Deep learning in hepatocellular carcinoma Current status and future perspectives</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.64462364);\n                color: white\n                ' title='Automated interpretation of histopathologic images from liver biopsy is another major area of medical imaging in patients with HCC where DL can be utilized. In addition to effectively replicating the human pathologists’ jobs of diagnosing and grading HCC, DL models can help identify and analyze additional complex imaging features and patterns which are related to specific mutations and disease prognosis. Lin et al[55] used images from multiphoton microscopy of 113 HCC patients to train a CNN with over 90% accuracy for determining HCC differentiation. Kiani et al[56] developed a CNN-based “Liver Cancer Assistant” which accurately differentiated hematoxylin and eosin (H&E) images of HCC and cholangiocarcinoma and helped improve the diagnostic performance of nine pathologists. Liao et al[57] used TCGA dataset for training a CNN that distinguished HCC from adjacent normal tissues with perfect performance (AUC: 1.00) and predicted the presence of specific somatic mutations with AUCs over 0.70. Wang et al[58] trained a CNN for automated segmentation and classification of individual nuclei at single-cell levels on H&E-stained tissue sections of HCC tumors from TCGA, and performed feature extraction to identify 246 quantitative image features. Then, a clustering analysis by an unsupervised learning approach identified three distinct histologic subtypes which were independent of previously established genomic clusters and had different prognosis. Chen et al[59] trained a CNN for automatic grading of HCC tumors on histopathological H&E images, which showed 96% accuracy for benign and malignant classification and 89.6% accuracy for the degree of tumor differentiation, and predicted the presence of specific genetic mutations. '>\n                            Page 6, Region 3,\n                            Score 0.64\n                        </summary>\n                        Automated interpretation of histopathologic images from liver biopsy is another major area of medical imaging in patients with HCC where DL can be utilized. In addition to effectively replicating the human pathologists’ jobs of diagnosing and grading HCC, DL models can help identify and analyze additional complex imaging features and patterns which are related to specific mutations and disease prognosis. Lin et al[55] used images from multiphoton microscopy of 113 HCC patients to train a CNN with over 90% accuracy for determining HCC differentiation. Kiani et al[56] developed a CNN-based “Liver Cancer Assistant” which accurately differentiated hematoxylin and eosin (H&E) images of HCC and cholangiocarcinoma and helped improve the diagnostic performance of nine pathologists. Liao et al[57] used TCGA dataset for training a CNN that distinguished HCC from adjacent normal tissues with perfect performance (AUC: 1.00) and predicted the presence of specific somatic mutations with AUCs over 0.70. Wang et al[58] trained a CNN for automated segmentation and classification of individual nuclei at single-cell levels on H&E-stained tissue sections of HCC tumors from TCGA, and performed feature extraction to identify 246 quantitative image features. Then, a clustering analysis by an unsupervised learning approach identified three distinct histologic subtypes which were independent of previously established genomic clusters and had different prognosis. Chen et al[59] trained a CNN for automatic grading of HCC tumors on histopathological H&E images, which showed 96% accuracy for benign and malignant classification and 89.6% accuracy for the degree of tumor differentiation, and predicted the presence of specific genetic mutations. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.6115709);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.61\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.10843022);\n                color: white\n                ' title='n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) '>\n                            Page 1, Region 5,\n                            Score 0.11\n                        </summary>\n                        n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='Randhwa et al,°’”, using support vector, with MRI images as data, made an AI tool to improve radiological image classification of HCC. This can help radiologists diagnose liver tumours early. Using regularization in the vector score in the classification stage removes the overfitting problem and leads to the accurate identification of different tumour types. A DL-based assistant has been developed to help pathologists differentiate between two subtypes of primary liver cancer, HCC and cholangiocarcinoma, on haematoxylin and eosin-stained whole-slide images (WSI), and evaluated its effect on the diagnostic performance of 11 pathologists with varying levels of expertise.°\" This DLbased assistant helped to increase the accuracy of pathologists. '>\n                            Page 7, Region 7,\n                            Score 1.0\n                        </summary>\n                        Randhwa et al,°’”, using support vector, with MRI images as data, made an AI tool to improve radiological image classification of HCC. This can help radiologists diagnose liver tumours early. Using regularization in the vector score in the classification stage removes the overfitting problem and leads to the accurate identification of different tumour types. A DL-based assistant has been developed to help pathologists differentiate between two subtypes of primary liver cancer, HCC and cholangiocarcinoma, on haematoxylin and eosin-stained whole-slide images (WSI), and evaluated its effect on the diagnostic performance of 11 pathologists with varying levels of expertise.°\" This DLbased assistant helped to increase the accuracy of pathologists. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.17994638);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.18\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.40030122);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.4\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.44119614);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.44\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5632587);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.56\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.24395914);\n                color: white\n                ' title='literature in this field. Additionally, a summary and quantitative analyses of the global development trend and research hotspots of AI in liver cancer is of great importance for future research. Bibliometrics is a method of information visualization which can achieve quantitative analysis of literature in a specific research field in a worldwide context through statistical methods and visualizing the results with the help of software (26-29). Bibliometrics plays an important role in sorting out development trends and research hotspots of a given field and has been widely used in many fields (26-29). '>\n                            Page 2, Region 7,\n                            Score 0.24\n                        </summary>\n                        literature in this field. Additionally, a summary and quantitative analyses of the global development trend and research hotspots of AI in liver cancer is of great importance for future research. Bibliometrics is a method of information visualization which can achieve quantitative analysis of literature in a specific research field in a worldwide context through statistical methods and visualizing the results with the help of software (26-29). Bibliometrics plays an important role in sorting out development trends and research hotspots of a given field and has been widely used in many fields (26-29). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.18116277);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.18\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5369835);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.54\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.1353715);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.14\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.58396643);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.58\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.067456216);\n                color: white\n                ' title='of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. '>\n                            Page 8, Region 6,\n                            Score 0.07\n                        </summary>\n                        of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.38318157);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.38\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.8394149);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.84\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.15241818);\n                color: white\n                ' title='Another rapidly growing area of research is focused on improved characterisation of indeterminate liver lesions. In clinical practice, when an abdominal ultrasound shows a new liver lesion, a patient is typically referred for further imaging, with contrast-enhanced CT or MRI. Based on the fulfilment of specific radiologic criteria, certain liver lesions may be considered as having pathognomonic features of HCC, and thus do not require liver biopsy for further histological confirmation. However, liver nodules imaged by CT or MRI often demonstrate indeterminate features, for which current recommendations include either liver biopsy or close interval follow-up with serial imaging.”° This practice is sub-optimal, resulting in numerous imaging studies, patient stress, and the potential for delayed diagnoses of liver cancer. For this reason, a growing body of recent literature has explored AI approaches to improve risk stratification of indeterminate liver lesions, to facilitate earlier and more accurate detection of HCC. '>\n                            Page 4, Region 4,\n                            Score 0.15\n                        </summary>\n                        Another rapidly growing area of research is focused on improved characterisation of indeterminate liver lesions. In clinical practice, when an abdominal ultrasound shows a new liver lesion, a patient is typically referred for further imaging, with contrast-enhanced CT or MRI. Based on the fulfilment of specific radiologic criteria, certain liver lesions may be considered as having pathognomonic features of HCC, and thus do not require liver biopsy for further histological confirmation. However, liver nodules imaged by CT or MRI often demonstrate indeterminate features, for which current recommendations include either liver biopsy or close interval follow-up with serial imaging.”° This practice is sub-optimal, resulting in numerous imaging studies, patient stress, and the potential for delayed diagnoses of liver cancer. For this reason, a growing body of recent literature has explored AI approaches to improve risk stratification of indeterminate liver lesions, to facilitate earlier and more accurate detection of HCC. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5237145);\n                color: white\n                ' title='hepatopathologists, and significant inter-observer disagreement may be observed. To address this, several recent studies have applied Al to assist with the diagnosis of liver tumours. Using 2 large data sets of H&E-stained digital slides, Liao et al. used a CNN to distinguish HCC from adjacent normal tissues, with AUCs above 0.90.7” Kiana et al. developed a tool able to classify image patches as HCC or cholangiocarcinoma. The model reached an accuracy of 0.88 on the validation set and, interestingly, the authors observed that the combination of the model and the pathologist outperformed both the model alone and the pathologist alone, suggesting that AI tools should be used to augment, rather than replace, the conventional histological diagnosis. They also showed how an incorrect prediction may negatively impact the final diagnosis made by pathologists, underscoring the need to be cautious with AI models aimed at automating diagnosis.”° '>\n                            Page 5, Region 8,\n                            Score 0.52\n                        </summary>\n                        hepatopathologists, and significant inter-observer disagreement may be observed. To address this, several recent studies have applied Al to assist with the diagnosis of liver tumours. Using 2 large data sets of H&E-stained digital slides, Liao et al. used a CNN to distinguish HCC from adjacent normal tissues, with AUCs above 0.90.7” Kiana et al. developed a tool able to classify image patches as HCC or cholangiocarcinoma. The model reached an accuracy of 0.88 on the validation set and, interestingly, the authors observed that the combination of the model and the pathologist outperformed both the model alone and the pathologist alone, suggesting that AI tools should be used to augment, rather than replace, the conventional histological diagnosis. They also showed how an incorrect prediction may negatively impact the final diagnosis made by pathologists, underscoring the need to be cautious with AI models aimed at automating diagnosis.”° \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Implementation of deep learning in liver pathology optimizes diagnosis of benign lesions and adenocarcinoma metastasis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.33412558);\n                color: white\n                ' title='In summary, we show for the first time that a comprehensive series of automated identification and classification of common benign and malignant lesions in the liver is possible by deep learning on scanned histological tissue sections. Our work can contribute to an objective and efficient workflow in routine diagnostics for highly relevant diagnostic questions, such as the differentiation between benign and malignant structures and the origin of frequent types of metastasis. This tool may aid pathologists, especially in situations where limited tissue is available, to establish and confirm the diagnosis. Furthermore, we provide an exceptional annotated liver dataset for the development and validation of deep learning algorithms which we provided to the scientific community. At the end, this may be a step towards improved personalized oncology therapy concepts, which will in the future integrate large clinical, radiological and pathological data sets using artificial intelligence. '>\n                            Page 12, Region 6,\n                            Score 0.33\n                        </summary>\n                        In summary, we show for the first time that a comprehensive series of automated identification and classification of common benign and malignant lesions in the liver is possible by deep learning on scanned histological tissue sections. Our work can contribute to an objective and efficient workflow in routine diagnostics for highly relevant diagnostic questions, such as the differentiation between benign and malignant structures and the origin of frequent types of metastasis. This tool may aid pathologists, especially in situations where limited tissue is available, to establish and confirm the diagnosis. Furthermore, we provide an exceptional annotated liver dataset for the development and validation of deep learning algorithms which we provided to the scientific community. At the end, this may be a step towards improved personalized oncology therapy concepts, which will in the future integrate large clinical, radiological and pathological data sets using artificial intelligence. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>GUT 2020 Exploring prognostic indicators in the pathological images of hepatocellular carcinoma based on deep learning</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='Figure 1 Data and workflow for the prognostic analysis of liver cancer with deep learning. We first developed the classification network using 260 whole-slide images (WSls) as the category-based sampling. The network was then used to analyse the remaining WSls and generate the segmentation maps. We randomly sampled tiles from each type of tissue based on these segmentation maps. Next, we trained the prognostic network and calculated a tumour risk score (TRS) for each patient. Finally, we used TRS to predict patient prognosis, and integrate transcriptomics, genomics and neural network heatmaps to identify interpretable features. TCGA, The Cancer Genome Atlas. '>\n                            Page 2, Region 6,\n                            Score 0.0\n                        </summary>\n                        Figure 1 Data and workflow for the prognostic analysis of liver cancer with deep learning. We first developed the classification network using 260 whole-slide images (WSls) as the category-based sampling. The network was then used to analyse the remaining WSls and generate the segmentation maps. We randomly sampled tiles from each type of tissue based on these segmentation maps. Next, we trained the prognostic network and calculated a tumour risk score (TRS) for each patient. Finally, we used TRS to predict patient prognosis, and integrate transcriptomics, genomics and neural network heatmaps to identify interpretable features. TCGA, The Cancer Genome Atlas. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "b72bc54c8f4d4ec6a8f033ab824f4a5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_8ec035b5b6ab47a08bc6adf724b34a0d",
       "style": "IPY_MODEL_d28e0ac895c54e8a964bc013589f99bc",
       "tooltip": "Retrieve related references"
      }
     },
     "b78037a9d73d4775923784e5f050f7f3": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_b48b59c4f55041c89dcc64579f8c4ca2"
      }
     },
     "b7ac38e6cf9149e2bed00d573f07893a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "b7bb540f882a48579b6aff7af05e395a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fe6412b34f584240bac944f109d00938",
        "IPY_MODEL_ad273c3dbcf24d4d889e1b39bf5e75dc"
       ],
       "layout": "IPY_MODEL_e73ab56142ad4403a217eab39e4f9807"
      }
     },
     "b7c9712132464b2a8c6418845abb428c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b7d60395b11f448887a6611e436b4e94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b83dd471daf74b07b2836d0afe82d7ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "b83f0a53d99c4b8991f760f67e643467": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f8a0b17581b64b1e873a4ee92e2e979a",
        "IPY_MODEL_de3b27879cd24730b28ba4c473d9bd47"
       ],
       "layout": "IPY_MODEL_6f2ce04dae044a82821a0fbd5a88bc73"
      }
     },
     "b8645e1167e841928545a8f0605f63bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b86ccca01f8246ef8a8db78027914ac5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_1d860d05dc8f4adaaec0a6505a34287e",
       "style": "IPY_MODEL_a324b885cff84f6c9eed967d01f6d9ef",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "b925d97facb745e9a82120a0f5ffb772": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f53da1bffc6541f8a405d929c0d45b91",
        "IPY_MODEL_087ed26a3c2142bc94fa41070cf19fee"
       ],
       "layout": "IPY_MODEL_b7ac38e6cf9149e2bed00d573f07893a"
      }
     },
     "b926ffbab3664f45b171d72d12aa6fb4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "b93aabe35cb34be794848b4c7eb37045": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b95119c1f3304ffab3841ac0bf34168c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_619554e2ac7c4e3fb2cde1216f199ce0",
       "placeholder": "",
       "style": "IPY_MODEL_9474a3d2e8b442d8b90e27d2d7f1e70a",
       "value": "500"
      }
     },
     "b987d2145da042c4a95b823f68f6151f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b9b88162452441468b455d6647a6bcad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_117acc032eb24139b0afdf4ff880e7be"
       ],
       "layout": "IPY_MODEL_0bed9d0c862840e88653256405ddbea3"
      }
     },
     "b9c8bd19b46b46459596b486c7eb4c26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "b9cf3248c5ea466ea6da82ed3652fc27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "b9df31605195437d9dd6e989afd8c148": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ba8c8b71e14e4b89aaebbb006dc0dc9b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "bae38a17fc09403d9a5c773344c5fd6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4c4b48a2199842ef96f9f4a592cf0a31",
        "IPY_MODEL_8725669db5794796af2dcd76b349d32e"
       ],
       "layout": "IPY_MODEL_1b02abbf90b1455f84dcbc811101e969"
      }
     },
     "bb5ac4e86042416e8173e912fc5fb205": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "bb7b6dac3e914ae48f1905ccf92a45a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bb9a72c2ea5e4ec38afeadb44f95307b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_633e6859e4d34fad89677f94e297e6d2",
        "IPY_MODEL_19dcb228d732487a862a32e36b6b6d90"
       ],
       "layout": "IPY_MODEL_98fcc25ef64c4c80bf0b59b8a904205d"
      }
     },
     "bba3e0b6888d41a0bdc4e02ee8a58f4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5968064dc67748c783c4f1e22ac3ca64",
        "IPY_MODEL_139dd0f09e984442bdbf0710b27b7f1f"
       ],
       "layout": "IPY_MODEL_c048ce26e2434e9b9e1969c565355971"
      }
     },
     "bbb6f2aece3c41e784760ba1892e5beb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bbec363fadd7403bbbea09a979a38bdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bc07612f60bd4f53a6a7f07484659b0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_6f44c684d1a44952be2b2cf88b26f1cc",
        "IPY_MODEL_d5adcdbc60c44db4adcc922bf9631dea"
       ],
       "layout": "IPY_MODEL_d25f0276983f4347a69c563e4ee50698"
      }
     },
     "bc6594bf512841b9a229be2d3caf3f1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_39dbd90a7c0a402b85c5dba083008b1a",
       "style": "IPY_MODEL_af9dec0af29845449304c92e9b6d90bb",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 1.0\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Deep learning in histopathology the path to the clinics</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5901692);\n                color: white\n                ' title='[H2] Weakly supervised learning. Another approach to reduce the burden of manual annotations is to consider CPATH algorithms that are trained in a weakly supervised fashion. In the context of image segmentation, weak supervision can come in the form of sparse manual annotations, for example, annotation of only small regions using dots or scribbles, as opposed to full supervision via dense annotations, in which all pixels of the image are manually labeled [80] [81]. Several groups have shown that weak supervision combined with advanced learning strategies in model development can approach the performance of fully supervised systems, particularly when sparse and dense annotations are combined. On the basis of this idea, weak supervision has been used to address several segmentation and detection problems in CPATH methods [82], [83], [41], [58], [84], [48]. '>\n                            Page 7, Region 2,\n                            Score 0.59\n                        </summary>\n                        [H2] Weakly supervised learning. Another approach to reduce the burden of manual annotations is to consider CPATH algorithms that are trained in a weakly supervised fashion. In the context of image segmentation, weak supervision can come in the form of sparse manual annotations, for example, annotation of only small regions using dots or scribbles, as opposed to full supervision via dense annotations, in which all pixels of the image are manually labeled [80] [81]. Several groups have shown that weak supervision combined with advanced learning strategies in model development can approach the performance of fully supervised systems, particularly when sparse and dense annotations are combined. On the basis of this idea, weak supervision has been used to address several segmentation and detection problems in CPATH methods [82], [83], [41], [58], [84], [48]. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.23742709);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.24\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.4106204);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.41\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.22674374);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.23\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.18913537);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.19\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.18991046);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.19\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.13575755);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.14\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.42833617);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.43\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.076764315);\n                color: white\n                ' title='68. Phan DV, Chan CL, Li AA, Chien TY, Nguyen VC. Liver cancer prediction in a viral hepatitis cohort: A deep learning approach. Int J Cancer (2020) 147(10):2871-8. doi: 10.1002/ijc.33245 '>\n                            Page 12, Region 2,\n                            Score 0.08\n                        </summary>\n                        68. Phan DV, Chan CL, Li AA, Chien TY, Nguyen VC. Liver cancer prediction in a viral hepatitis cohort: A deep learning approach. Int J Cancer (2020) 147(10):2871-8. doi: 10.1002/ijc.33245 \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>GUT 2020 Exploring prognostic indicators in the pathological images of hepatocellular carcinoma based on deep learning</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.18158);\n                color: white\n                ' title='application impracticable. A method that integrates the patient prognostication and visualisation of prognostic pathological phenotypes may facilitate oncologists to explore new pathological features. In this regard, network activations as saliency masks for visualisations have become a popular method to explain deep learning;'' '? however, their application in pathological images needs further improvement. The existing methods can only visualise local coarse features rather than accurately decode cell-level pathological features, such as nuclear atypia, mitotic activity, cellular density and tissue architecture. Additionally, a pathological image contains enormous subtle and complex information, rendering pixel-level annotations a huge challenge. Weakly supervised learning that only requires slide-level labels for training partially addresses the above considerations. However, the existing weakly supervised learning usually requires tens of thousands of samples to converge, making it not widely used. '>\n                            Page 2, Region 3,\n                            Score 0.18\n                        </summary>\n                        application impracticable. A method that integrates the patient prognostication and visualisation of prognostic pathological phenotypes may facilitate oncologists to explore new pathological features. In this regard, network activations as saliency masks for visualisations have become a popular method to explain deep learning;'' '? however, their application in pathological images needs further improvement. The existing methods can only visualise local coarse features rather than accurately decode cell-level pathological features, such as nuclear atypia, mitotic activity, cellular density and tissue architecture. Additionally, a pathological image contains enormous subtle and complex information, rendering pixel-level annotations a huge challenge. Weakly supervised learning that only requires slide-level labels for training partially addresses the above considerations. However, the existing weakly supervised learning usually requires tens of thousands of samples to converge, making it not widely used. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.017828334);\n                color: white\n                ' title='By contrast, we successfully applied weakly supervised learning on smaller data sets effectively. Training a deep neural network in image analysis is an optimisation of all possible solutions to tremendous image information. By implicitly constraining the solution space with prior knowledge, the complexity of the problem can be effectively reduced to justify the need for less data. In our study, an easily available prior knowledge for pathological images is the tissue category. Thus, we trained tissue category-based local sampling using minor manual annotation, thereby obtaining a large number of more meaningful tissue tiles on HCC slices as inputs. Using prior knowledge can effectively '>\n                            Page 8, Region 3,\n                            Score 0.02\n                        </summary>\n                        By contrast, we successfully applied weakly supervised learning on smaller data sets effectively. Training a deep neural network in image analysis is an optimisation of all possible solutions to tremendous image information. By implicitly constraining the solution space with prior knowledge, the complexity of the problem can be effectively reduced to justify the need for less data. In our study, an easily available prior knowledge for pathological images is the tissue category. Thus, we trained tissue category-based local sampling using minor manual annotation, thereby obtaining a large number of more meaningful tissue tiles on HCC slices as inputs. Using prior knowledge can effectively \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.020848786);\n                color: white\n                ' title='reduce the complexity of network optimisation and improve the performance of weakly supervised learning, even on small data sets. '>\n                            Page 8, Region 4,\n                            Score 0.02\n                        </summary>\n                        reduce the complexity of network optimisation and improve the performance of weakly supervised learning, even on small data sets. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.60181427);\n                color: white\n                ' title='In conclusion, this study proposed a weakly supervised deeplearning framework to facilitate patient prognosis in HCC. The prognostic features discovered by our deep-learning framework were formularised by TRS and visualised by heatmaps. We confirmed that tumour immune infiltration might favour and microvascularisation may compromise HCC patient survival. Our work indicates that weakly supervised deep learning is an effective and labour-saving method for predicting patient clinical outcomes and warrants further study and extensive application. '>\n                            Page 10, Region 4,\n                            Score 0.6\n                        </summary>\n                        In conclusion, this study proposed a weakly supervised deeplearning framework to facilitate patient prognosis in HCC. The prognostic features discovered by our deep-learning framework were formularised by TRS and visualised by heatmaps. We confirmed that tumour immune infiltration might favour and microvascularisation may compromise HCC patient survival. Our work indicates that weakly supervised deep learning is an effective and labour-saving method for predicting patient clinical outcomes and warrants further study and extensive application. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Deep learning in hepatocellular carcinoma Current status and future perspectives</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0025106699);\n                color: white\n                ' title='The term “artificial intelligence” encompasses a broad range of technology that enables machines to perform tasks typically thought to require human reasoning and problem-solving skills[7]. “Machine learning” is a branch of AI in which computer algorithms train on sample data to build a mathematical model that makes predictions or decisions without being explicitly programmed to do so[8]. Machine learning algorithms can be broadly divided into supervised and unsupervised learning. Supervised learning algorithms train on sample data with labeled outcome data, and their goal is to learn the relationship between the input data and the outcomes to make '>\n                            Page 2, Region 19,\n                            Score 0.0\n                        </summary>\n                        The term “artificial intelligence” encompasses a broad range of technology that enables machines to perform tasks typically thought to require human reasoning and problem-solving skills[7]. “Machine learning” is a branch of AI in which computer algorithms train on sample data to build a mathematical model that makes predictions or decisions without being explicitly programmed to do so[8]. Machine learning algorithms can be broadly divided into supervised and unsupervised learning. Supervised learning algorithms train on sample data with labeled outcome data, and their goal is to learn the relationship between the input data and the outcomes to make \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.36805403);\n                color: white\n                ' title='accurate predictions about the outcome when provided with a new set of input data [9]. Examples of supervised learning algorithms include traditional techniques such as linear regression and logistic regression, as well as more sophisticated techniques including support vector machines, random forest and gradient boosting. On the other hand, unsupervised learning algorithms train on unlabeled sample data and analyze the underlying structure or distribution within the data to discover new clusters or patterns[10]. Examples of unsupervised learning algorithms include K-means and principle component analysis among many others. '>\n                            Page 3, Region 3,\n                            Score 0.37\n                        </summary>\n                        accurate predictions about the outcome when provided with a new set of input data [9]. Examples of supervised learning algorithms include traditional techniques such as linear regression and logistic regression, as well as more sophisticated techniques including support vector machines, random forest and gradient boosting. On the other hand, unsupervised learning algorithms train on unlabeled sample data and analyze the underlying structure or distribution within the data to discover new clusters or patterns[10]. Examples of unsupervised learning algorithms include K-means and principle component analysis among many others. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Multi-task deep learning network to predict future macrovascular invasion in hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.11822987);\n                color: white\n                ' title='Deep learning algorithms have proven to be advantageous in constructing models for diagnosis and prognosis of cancers, especially for liver diseases [16—18,27]. Meanwhile, among all the types of deep learning algorithms, multi-task learning combines severally related tasks during the training process and these can benefit from each other. Multi-task learning has attracted considerable attention in the field of medical image analysis [28—29]; however, its application in HCC has been limited to microvascular invasion rather than macrovascular invasion [30]. Considering the potential advantages of multi-task learning, we constructed our MTnet to predict macrovascular invasion. '>\n                            Page 8, Region 8,\n                            Score 0.12\n                        </summary>\n                        Deep learning algorithms have proven to be advantageous in constructing models for diagnosis and prognosis of cancers, especially for liver diseases [16—18,27]. Meanwhile, among all the types of deep learning algorithms, multi-task learning combines severally related tasks during the training process and these can benefit from each other. Multi-task learning has attracted considerable attention in the field of medical image analysis [28—29]; however, its application in HCC has been limited to microvascular invasion rather than macrovascular invasion [30]. Considering the potential advantages of multi-task learning, we constructed our MTnet to predict macrovascular invasion. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Implementation of deep learning in liver pathology optimizes diagnosis of benign lesions and adenocarcinoma metastasis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.09414473);\n                color: white\n                ' title='Conclusions: Deep learning is a promising approach in surgical liver pathology supporting decision making in personalized medicine. '>\n                            Page 3, Region 5,\n                            Score 0.09\n                        </summary>\n                        Conclusions: Deep learning is a promising approach in surgical liver pathology supporting decision making in personalized medicine. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.01154893);\n                color: white\n                ' title='Al is a broad field that includes computational search algorithms, machine learning (ML) and deep learning (DL) models (Fig. 1). ML consists of a computer running repeated iterations of models in order to progressively improve performance of a specific task, such as classifying an outcome. ML models are designed to improve with time, by incorporating additional input training data and thereby optimising the parameters of an algorithm. With time and training, the desired output becomes increasingly accurate. Based on how the training process is conducted, ML may be classified as supervised or unsupervised. Supervised ML algorithms perform training on a dataset that is labelled in relation to the class of interest, and this label is available to the algorithm while the model is being created, trained, and optimised. In contrast, unsupervised ML involves training on a dataset that lacks class labels, yielding clusters of output data that subsequently require additional interpretation. '>\n                            Page 1, Region 14,\n                            Score 0.01\n                        </summary>\n                        Al is a broad field that includes computational search algorithms, machine learning (ML) and deep learning (DL) models (Fig. 1). ML consists of a computer running repeated iterations of models in order to progressively improve performance of a specific task, such as classifying an outcome. ML models are designed to improve with time, by incorporating additional input training data and thereby optimising the parameters of an algorithm. With time and training, the desired output becomes increasingly accurate. Based on how the training process is conducted, ML may be classified as supervised or unsupervised. Supervised ML algorithms perform training on a dataset that is labelled in relation to the class of interest, and this label is available to the algorithm while the model is being created, trained, and optimised. In contrast, unsupervised ML involves training on a dataset that lacks class labels, yielding clusters of output data that subsequently require additional interpretation. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Exploring pathological signatures for predicting the recurrence of early-stage hepatocellular carcinoma based on deep learning</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='Recent studies on AI reported novel prognostic models for HCC patients based on pathological images. Saillard et al. established two independent scores using an unsupervised neural network algorithm and attention mechanism according to tumoral or nontumoral annotated tiles (20). Both models showed high accuracy in survival prediction and strong correlations between clinical characteristics. Gao et al. innovatively divided HCC slides into four categories,tumor '>\n                            Page 10, Region 5,\n                            Score 0.0\n                        </summary>\n                        Recent studies on AI reported novel prognostic models for HCC patients based on pathological images. Saillard et al. established two independent scores using an unsupervised neural network algorithm and attention mechanism according to tumoral or nontumoral annotated tiles (20). Both models showed high accuracy in survival prediction and strong correlations between clinical characteristics. Gao et al. innovatively divided HCC slides into four categories,tumor \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "bc73d82b677846ce9b5be0671d010a80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc92874a8e134ed6936827e894ef485e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "bcf4d595b8724032a15d96793164c8b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bd41edc7affb48a6ad73b0cc7eb60d1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ed553411b272454ba7d1fdf9f4ed4552",
        "IPY_MODEL_cfef1ff33aca4ee3929515d72508a006"
       ],
       "layout": "IPY_MODEL_b43653dd95254f6e9ecd8aa91d3cbb26"
      }
     },
     "bd660b8f603241a283a579c1fb3ea7e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bd90e9786de5477eb0f3613e7ff3d42f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "bda170f4a7c04a988375f7fa42bc5313": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bda1ae3304f54cdbbf4578b84eecf6b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5cd4a3acae5743da8188eceec657de92",
        "IPY_MODEL_d10a3e5d83cd4c1aa35442f7a05339bc"
       ],
       "layout": "IPY_MODEL_36c8e10a87f54e1f8711cab7a7b9089f"
      }
     },
     "bda2ba9588fd4e21be36644adb04e80c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "bdb1cc8a491a4fe7bbf769446604b9dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ed1b4e17e08a40589107cc72fd2d7547",
        "IPY_MODEL_ac8410b9d12840d8b3c183983eba5c16",
        "IPY_MODEL_6467021beddf42fcad1213af136094ba"
       ],
       "layout": "IPY_MODEL_533ac58145364543bfb69dad7676ac31"
      }
     },
     "bdbbd12760824bb2b5ff3a5da666f193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_80c99dd52110462e8d66b2447d3e0341",
       "placeholder": "20",
       "style": "IPY_MODEL_2534b6fec4244ed7895ae18bd4d65f3b",
       "value": "20"
      }
     },
     "bdef0015671645a4ac541ef76728e5c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bdef7d4c5c4b497f982d193535955163": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_01a7b4b31eb143228b6ba5c097e04d31",
       "style": "IPY_MODEL_b424a2223b9040058d660aa7c164de4c",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "bdfe669853b543599b52968afef029e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_feca6d14eb094f9780aa9513396b207a",
        "IPY_MODEL_f94fa13511a8419eaf09d9c0f74d80dc"
       ],
       "layout": "IPY_MODEL_b69fa21883ab4a5d8164d2bd98902152"
      }
     },
     "be7a8b057b934ee78bf66c0375e3eb6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_cc2a63b2061d4277b834690ab2b8e176",
       "placeholder": "",
       "style": "IPY_MODEL_109df0bdc9a84e25a8d06c8ab2984f17"
      }
     },
     "be8fbc4f296649a6b9b22ec9c8bc4e24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_99225c51f7924cc4914943885e3a9e97",
       "style": "IPY_MODEL_63309b071af745ea9d7ee40d0b6c8f6a"
      }
     },
     "be9b38f12d234b57bcaa5a1b27213fde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_233121d2ebd14ab08f9c0af5820169dd",
        "IPY_MODEL_75995e7b104d422fb0edf89fea22a7f6",
        "IPY_MODEL_f35bd451c57940c890cbd7a63260754e"
       ],
       "layout": "IPY_MODEL_9e862949626542aeba72f248a4c1f05d"
      }
     },
     "bf43af8e93574919ab5d94d5bc466f82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_67fa6afacace435aadfb8b4b670ff1d4",
       "style": "IPY_MODEL_244cb21f24fe4ad8946dd01b0d8477e0",
       "tooltip": "Retrieve related references"
      }
     },
     "bf5b7e9e1e3a414eaaa6eaac290fc092": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bf62e3b66277477f80533c55103e6c4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5532e57e35d746afbfba4386b9929838",
       "style": "IPY_MODEL_d5c5354c43c3414a826f7d96b60d3089",
       "value": " to "
      }
     },
     "bf8195064fa34cf28ff33ada6b745179": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "bf904de75cd54c44b8cd8a6666192488": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e6049b9e76ec4eb6a98f33e6edc8ca04",
        "IPY_MODEL_46642b3842c5432691c9084c21faed64",
        "IPY_MODEL_79d2d56a8b9b4f659f5871492915fdb8"
       ],
       "layout": "IPY_MODEL_4cba08dec39f48d7827ddd54489f05e1"
      }
     },
     "bf927469e1d04715bafcf045a369127f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "bfa9ffe01a7d4abcb3d5dd71185b00f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_50752c2a8fcb459ca14221524764541b",
        "IPY_MODEL_26a384dfe9f048dc91bd0de948341341"
       ],
       "layout": "IPY_MODEL_6ddddbb262a24b5492abe6c7a1572ef9"
      }
     },
     "bfc0c0ca24914d69bb49de0e486e7553": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bfca834b697b46f0bc2884c0c1483cc3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c01285c1c274460da50b09fa8a0e54a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "c01ce5358eb94e0181a33b128281938c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_47af31b1512941fcbfb8102d302ed7fc",
        "IPY_MODEL_efd59f63844e46909069c7d2d1e945f2"
       ],
       "layout": "IPY_MODEL_8084c35185f44bbf918def7196cfb78f"
      }
     },
     "c048ce26e2434e9b9e1969c565355971": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "c0a7858b7c8e49608e252b556ba51931": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_1acc435fe085486a95e3aedb7be9a7ed",
       "style": "IPY_MODEL_a06c88a131894251a2b2c5da6d21a602",
       "value": "3.1.4.1: Triplet network"
      }
     },
     "c0d51b7feddb4b95b2fafa261096b6dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "c1220eab179a49be8e3d6d38dbdcea1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e004f3931bd04cf4a75da0cf009174d2",
       "style": "IPY_MODEL_96874bfd9fbc4073a6faf8e986cecb74",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.45799854);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.46\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5202133);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.52\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5011344);\n                color: white\n                ' title='literature in this field. Additionally, a summary and quantitative analyses of the global development trend and research hotspots of AI in liver cancer is of great importance for future research. Bibliometrics is a method of information visualization which can achieve quantitative analysis of literature in a specific research field in a worldwide context through statistical methods and visualizing the results with the help of software (26-29). Bibliometrics plays an important role in sorting out development trends and research hotspots of a given field and has been widely used in many fields (26-29). '>\n                            Page 2, Region 7,\n                            Score 0.5\n                        </summary>\n                        literature in this field. Additionally, a summary and quantitative analyses of the global development trend and research hotspots of AI in liver cancer is of great importance for future research. Bibliometrics is a method of information visualization which can achieve quantitative analysis of literature in a specific research field in a worldwide context through statistical methods and visualizing the results with the help of software (26-29). Bibliometrics plays an important role in sorting out development trends and research hotspots of a given field and has been widely used in many fields (26-29). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.6880878);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.69\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.73861223);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.74\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.60669667);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.61\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.26349318);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.26\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.14031506);\n                color: white\n                ' title='In the cross-analysis of data types and diseases, we found that biopsy was used as an important data type in studies of AI in liver fibrosis. This is mainly because the histopathological examination of liver biopsy is still the gold standard for the diagnosis of liver fibrosis (45). Conventional CT/MRI examinations can observe morphological changes of the liver; however, quantitative assessment of early-stage liver fibrosis is still difficult and is therefore less used. Although ultrasound elastography and magnetic resonance elastography (MRE) are highly effective non-invasive assessment methods in the diagnosis '>\n                            Page 8, Region 5,\n                            Score 0.14\n                        </summary>\n                        In the cross-analysis of data types and diseases, we found that biopsy was used as an important data type in studies of AI in liver fibrosis. This is mainly because the histopathological examination of liver biopsy is still the gold standard for the diagnosis of liver fibrosis (45). Conventional CT/MRI examinations can observe morphological changes of the liver; however, quantitative assessment of early-stage liver fibrosis is still difficult and is therefore less used. Although ultrasound elastography and magnetic resonance elastography (MRE) are highly effective non-invasive assessment methods in the diagnosis \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.11632745);\n                color: white\n                ' title='of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. '>\n                            Page 8, Region 6,\n                            Score 0.12\n                        </summary>\n                        of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.23041633);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.23\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.59212625);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.59\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 1.0\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='Analyzing a trained model (i.e., a neural network in deep learning) to achieve insight into learned relationships is referred to as post hoc explanation. An important distinction between post hoc explanation and model-based explanation is that the former trains a neural network and subsequently attempts to explain the behavior of the ensuing black box network, whereas the latter forces the neural network to be explainable. '>\n                            Page 2, Region 13,\n                            Score 0.0\n                        </summary>\n                        Analyzing a trained model (i.e., a neural network in deep learning) to achieve insight into learned relationships is referred to as post hoc explanation. An important distinction between post hoc explanation and model-based explanation is that the former trains a neural network and subsequently attempts to explain the behavior of the ensuing black box network, whereas the latter forces the neural network to be explainable. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0030103708);\n                color: white\n                ' title='Methods that provide post hoc explanation include inspection of learned features, feature importance, and interaction of features (Abbasi-As] and Yu, 2017; Olden et al., 2004; Tsang et al. 2018; as well as visual explanation by saliency maps (Selvaraju et al., 2017; Simonyan et al., 2013; Springenberg et al., 2014; Zeiler and Fergus, 2014; Zhou et al., 2016). '>\n                            Page 2, Region 14,\n                            Score 0.0\n                        </summary>\n                        Methods that provide post hoc explanation include inspection of learned features, feature importance, and interaction of features (Abbasi-As] and Yu, 2017; Olden et al., 2004; Tsang et al. 2018; as well as visual explanation by saliency maps (Selvaraju et al., 2017; Simonyan et al., 2013; Springenberg et al., 2014; Zeiler and Fergus, 2014; Zhou et al., 2016). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0014274124);\n                color: white\n                ' title='Textual explanation is a form of XAI that provides textual descriptions. Such descriptions include relatively simple characteristics (e.g. ‘spiculated mass’), up to entire medical reports. We will describe three types of textual explanation: image captioning, image captioning with visual explanation, and testing with concept attribution. '>\n                            Page 8, Region 20,\n                            Score 0.0\n                        </summary>\n                        Textual explanation is a form of XAI that provides textual descriptions. Such descriptions include relatively simple characteristics (e.g. ‘spiculated mass’), up to entire medical reports. We will describe three types of textual explanation: image captioning, image captioning with visual explanation, and testing with concept attribution. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.313501);\n                color: white\n                ' title='The computational costs of the post hoc textual explanation TCAV and the post hoc example-based explanation of influence functions in medical image analysis has not rigorously been reported. '>\n                            Page 13, Region 8,\n                            Score 0.31\n                        </summary>\n                        The computational costs of the post hoc textual explanation TCAV and the post hoc example-based explanation of influence functions in medical image analysis has not rigorously been reported. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.49019387);\n                color: white\n                ' title='The post hoc textual explanation TCAV requires some finetuning with respect to the concepts that will be tested. The post hoc example-based explanation technique of influence functions requires definition of the functions of which the influence is to be measured. '>\n                            Page 13, Region 14,\n                            Score 0.49\n                        </summary>\n                        The post hoc textual explanation TCAV requires some finetuning with respect to the concepts that will be tested. The post hoc example-based explanation technique of influence functions requires definition of the functions of which the influence is to be measured. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.16462512);\n                color: white\n                ' title='We have discussed 223 papers on eXplainable Artificial Intelligence (XAI) for deep learning in medical image analysis. We categorized the papers based on the XAI-frameworks proposed by Adadi and Berrada (2018) and Murdoch et al. (2019). Some trends were noticeable in the surveyed papers. The majority of the papers used post hoc explanation as contrasted with model-based explanation, i.e., the explanation was provided on a neural network that had already been trained, instead of being incorporated in neural network training. Both model-specific (e.g., specifically designed for CNNs) and model-agnostic explanation methods were used. Furthermore, most of the papers investigated provided local explanation rather than global explanation, i.e., the explanation was provided per case (e.g. per patient), rather than on a datasetlevel (e.g. for all patients). Since we focus on deep learning in medical image analysis, these trends were to be expected. Most readily available XAI methods suitable for CNNs are saliency mapping techniques, which often provide post hoc, model-specific, and local explanation. Furthermore, post hoc XAI methods can be used after a neural network has been trained, making them more accessible than model-based XAI. '>\n                            Page 13, Region 20,\n                            Score 0.16\n                        </summary>\n                        We have discussed 223 papers on eXplainable Artificial Intelligence (XAI) for deep learning in medical image analysis. We categorized the papers based on the XAI-frameworks proposed by Adadi and Berrada (2018) and Murdoch et al. (2019). Some trends were noticeable in the surveyed papers. The majority of the papers used post hoc explanation as contrasted with model-based explanation, i.e., the explanation was provided on a neural network that had already been trained, instead of being incorporated in neural network training. Both model-specific (e.g., specifically designed for CNNs) and model-agnostic explanation methods were used. Furthermore, most of the papers investigated provided local explanation rather than global explanation, i.e., the explanation was provided per case (e.g. per patient), rather than on a datasetlevel (e.g. for all patients). Since we focus on deep learning in medical image analysis, these trends were to be expected. Most readily available XAI methods suitable for CNNs are saliency mapping techniques, which often provide post hoc, model-specific, and local explanation. Furthermore, post hoc XAI methods can be used after a neural network has been trained, making them more accessible than model-based XAI. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.059483804);\n                color: white\n                ' title='n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) '>\n                            Page 1, Region 5,\n                            Score 0.06\n                        </summary>\n                        n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.016653573);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.02\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "c15076ed26b944c7bae6e13787f799e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c2012f04077745f6936e69865dc121c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5caffb89e88a4ede9d23913794ac9bac",
        "IPY_MODEL_888a266f77124e87a51cd88bb6ae6f1b"
       ],
       "layout": "IPY_MODEL_77045c62c02e40a2bfd8b818870f393c"
      }
     },
     "c24a396a5ebb46f1b55bcce0c8d7683b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_27d0890347504c618d60e635275f53b0",
        "IPY_MODEL_eedfce06b0ff4ee488e1014b00f2ffe0"
       ],
       "layout": "IPY_MODEL_373ae0308aa14f16a5a43465d1f28055"
      }
     },
     "c282d596d59e4cd095287cfbaf098519": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0cf5a5bb3b5e4c758284dccf3a15bc80",
        "IPY_MODEL_77fc4118e8354015adf3afb4da82b225"
       ],
       "layout": "IPY_MODEL_3eef018eb9ae42c1a8158a24911388e5"
      }
     },
     "c29de9e3e6e34b82beb1c91922894b6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_10cdfd0781e04962876bed0fd4d0b2ec",
        "IPY_MODEL_2671bfe6e5b74c79b48eac9f30c54e58"
       ],
       "layout": "IPY_MODEL_a55f422440a04dcca2d01d978f631274"
      }
     },
     "c2ed7da3480746648eceb3372b7bf7c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7198459c258440488804f5e7716940f1",
       "style": "IPY_MODEL_7e51b5bc8b1d4e90b60cfbd12aab53ef"
      }
     },
     "c31a28229a8e4dc1a37612bff1d22ba3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "c36fd5a559354cb0bbd596a0bc92fb59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_180620e441c044a9bed20459fc22b38a",
        "IPY_MODEL_28ed839b291c4979b2f87cc83b8bb903"
       ],
       "layout": "IPY_MODEL_aba82963d7d245089408a4cf53e795ff"
      }
     },
     "c3770c4a3d82421c8abb0c52d6d6f4cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "c39226cff0ac47f5bfd44c139b597dd9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c39424f084f747669536c49e43eed374": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ed4d6f4754404719aca068960ca143a0",
        "IPY_MODEL_329fd034917840b197e88d37c91eb321"
       ],
       "layout": "IPY_MODEL_a77bfc54cdf64065b96fce3a316ddb9e"
      }
     },
     "c3993b7251b2419dad813cec6cc44366": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_79a6d033333c427ca084c7f5f191c9b6"
      }
     },
     "c3ca9851b1884b2c8077901072df6698": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "c41821b0adc94b048f6a24fc206cfd21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_fe2538abc04f457c8b13fa35e6ccad37",
       "style": "IPY_MODEL_30aabef282004dca9eb276e09b577269",
       "value": "3.1.3.2: Image captioning with visual explanation"
      }
     },
     "c463cc84c8364315a7b677aed6f97dbe": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_17a0cc8185d94467a193d0893e267ff5"
      }
     },
     "c53eabe819984411a7fb30a623e9c18a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "c544dd9a020e4be7bb9dce87f2816def": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e703a9b045774b108b56dab68512608c",
       "style": "IPY_MODEL_d7aabad3f1d4481496a401e73414ef07"
      }
     },
     "c54aeee8ed4a4bc3b9b533b797f74cfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "c57faf48b230421383edafb3ad6c063a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "c592e75539d846d8ac99fa8d5afb791b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "c5ac14f0cb7948d98539b80b37437f2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_92bdb55fa4e447499f4be6ad3a273ca1",
       "style": "IPY_MODEL_8a78f1a949734fa6a9a8afd3bb50659e",
       "value": "words:"
      }
     },
     "c5bff01515bf4cd881ff9aad8eaf341d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7205b25f21b34149ba5bf0ece65bbcd7",
        "IPY_MODEL_412f4516628744f79d065d6eff70dff8"
       ],
       "layout": "IPY_MODEL_e8a5338026df4b8b86d061c73cfdcbf1"
      }
     },
     "c5c880e20204411491c8433160afd9f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "c644b732ee854e328cb92c575487bea8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c65f4f81e8c04333a238ee436ead0fbc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "c6893461251541af872fbe6caf19c3b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "c692a415f659487cacd30956427cac68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0fb1f4ac52ec4a3cb46992247bbd6de3",
       "style": "IPY_MODEL_49b0f761e87748ada135fc163c492a44",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.43897387);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.44\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 1.0\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.55540484);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.56\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7116784);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.71\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7414069);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.74\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2589784);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.26\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.06619939);\n                color: white\n                ' title='In the cross-analysis of data types and diseases, we found that biopsy was used as an important data type in studies of AI in liver fibrosis. This is mainly because the histopathological examination of liver biopsy is still the gold standard for the diagnosis of liver fibrosis (45). Conventional CT/MRI examinations can observe morphological changes of the liver; however, quantitative assessment of early-stage liver fibrosis is still difficult and is therefore less used. Although ultrasound elastography and magnetic resonance elastography (MRE) are highly effective non-invasive assessment methods in the diagnosis '>\n                            Page 8, Region 5,\n                            Score 0.07\n                        </summary>\n                        In the cross-analysis of data types and diseases, we found that biopsy was used as an important data type in studies of AI in liver fibrosis. This is mainly because the histopathological examination of liver biopsy is still the gold standard for the diagnosis of liver fibrosis (45). Conventional CT/MRI examinations can observe morphological changes of the liver; however, quantitative assessment of early-stage liver fibrosis is still difficult and is therefore less used. Although ultrasound elastography and magnetic resonance elastography (MRE) are highly effective non-invasive assessment methods in the diagnosis \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.038831916);\n                color: white\n                ' title='Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. '>\n                            Page 8, Region 8,\n                            Score 0.04\n                        </summary>\n                        Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.34905273);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.35\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.82938695);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.83\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.05479034);\n                color: white\n                ' title='Analyzing a trained model (i.e., a neural network in deep learning) to achieve insight into learned relationships is referred to as post hoc explanation. An important distinction between post hoc explanation and model-based explanation is that the former trains a neural network and subsequently attempts to explain the behavior of the ensuing black box network, whereas the latter forces the neural network to be explainable. '>\n                            Page 2, Region 13,\n                            Score 0.05\n                        </summary>\n                        Analyzing a trained model (i.e., a neural network in deep learning) to achieve insight into learned relationships is referred to as post hoc explanation. An important distinction between post hoc explanation and model-based explanation is that the former trains a neural network and subsequently attempts to explain the behavior of the ensuing black box network, whereas the latter forces the neural network to be explainable. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='Methods that provide post hoc explanation include inspection of learned features, feature importance, and interaction of features (Abbasi-As] and Yu, 2017; Olden et al., 2004; Tsang et al. 2018; as well as visual explanation by saliency maps (Selvaraju et al., 2017; Simonyan et al., 2013; Springenberg et al., 2014; Zeiler and Fergus, 2014; Zhou et al., 2016). '>\n                            Page 2, Region 14,\n                            Score 0.0\n                        </summary>\n                        Methods that provide post hoc explanation include inspection of learned features, feature importance, and interaction of features (Abbasi-As] and Yu, 2017; Olden et al., 2004; Tsang et al. 2018; as well as visual explanation by saliency maps (Selvaraju et al., 2017; Simonyan et al., 2013; Springenberg et al., 2014; Zeiler and Fergus, 2014; Zhou et al., 2016). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.11618407);\n                color: white\n                ' title='Model-based explanation is by definition model-specific (Adadi and Berrada, 2018), but model-specific explanation is not necessary model-based. Some post hoc saliency mapping techniques are examples of techniques that are specific to a certain class of convolutional neural networks (CNNs), but are not model-based explanation methods (Murdoch et al., 2019). '>\n                            Page 2, Region 19,\n                            Score 0.12\n                        </summary>\n                        Model-based explanation is by definition model-specific (Adadi and Berrada, 2018), but model-specific explanation is not necessary model-based. Some post hoc saliency mapping techniques are examples of techniques that are specific to a certain class of convolutional neural networks (CNNs), but are not model-based explanation methods (Murdoch et al., 2019). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.672787);\n                color: white\n                ' title='Example-based explanation is an XAI technique that provides examples relating to the data point that is currently being analyzed. This can be useful when trying to explain why a neural network came to a decision, and is related to how humans reason. For example, when a pathologist examines a biopsy of a patient that shows similarity with an earlier patient examined by the pathologist, the clinical decision may be enhanced by knowing the assessment of that earlier biopsy. '>\n                            Page 10, Region 8,\n                            Score 0.67\n                        </summary>\n                        Example-based explanation is an XAI technique that provides examples relating to the data point that is currently being analyzed. This can be useful when trying to explain why a neural network came to a decision, and is related to how humans reason. For example, when a pathologist examines a biopsy of a patient that shows similarity with an earlier patient examined by the pathologist, the clinical decision may be enhanced by knowing the assessment of that earlier biopsy. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.6770872);\n                color: white\n                ' title='Peng et al. (2019) used example-based explanation in colorectal cancer histology. They first trained a CNN using a triplet loss, hashing, and k hard-negatives to learn an embedding that preserves similarity. In testing, a coarse-to-fine search yielded the 10 nearest examples from a testing database related to the input image. This provided explanation on which images similar to the image that was being analyzed the network based a decision. '>\n                            Page 10, Region 14,\n                            Score 0.68\n                        </summary>\n                        Peng et al. (2019) used example-based explanation in colorectal cancer histology. They first trained a CNN using a triplet loss, hashing, and k hard-negatives to learn an embedding that preserves similarity. In testing, a coarse-to-fine search yielded the 10 nearest examples from a testing database related to the input image. This provided explanation on which images similar to the image that was being analyzed the network based a decision. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.18169999);\n                color: white\n                ' title='The computational costs of the post hoc textual explanation TCAV and the post hoc example-based explanation of influence functions in medical image analysis has not rigorously been reported. '>\n                            Page 13, Region 8,\n                            Score 0.18\n                        </summary>\n                        The computational costs of the post hoc textual explanation TCAV and the post hoc example-based explanation of influence functions in medical image analysis has not rigorously been reported. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.45291856);\n                color: white\n                ' title='The post hoc textual explanation TCAV requires some finetuning with respect to the concepts that will be tested. The post hoc example-based explanation technique of influence functions requires definition of the functions of which the influence is to be measured. '>\n                            Page 13, Region 14,\n                            Score 0.45\n                        </summary>\n                        The post hoc textual explanation TCAV requires some finetuning with respect to the concepts that will be tested. The post hoc example-based explanation technique of influence functions requires definition of the functions of which the influence is to be measured. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2895432);\n                color: white\n                ' title='We have discussed 223 papers on eXplainable Artificial Intelligence (XAI) for deep learning in medical image analysis. We categorized the papers based on the XAI-frameworks proposed by Adadi and Berrada (2018) and Murdoch et al. (2019). Some trends were noticeable in the surveyed papers. The majority of the papers used post hoc explanation as contrasted with model-based explanation, i.e., the explanation was provided on a neural network that had already been trained, instead of being incorporated in neural network training. Both model-specific (e.g., specifically designed for CNNs) and model-agnostic explanation methods were used. Furthermore, most of the papers investigated provided local explanation rather than global explanation, i.e., the explanation was provided per case (e.g. per patient), rather than on a datasetlevel (e.g. for all patients). Since we focus on deep learning in medical image analysis, these trends were to be expected. Most readily available XAI methods suitable for CNNs are saliency mapping techniques, which often provide post hoc, model-specific, and local explanation. Furthermore, post hoc XAI methods can be used after a neural network has been trained, making them more accessible than model-based XAI. '>\n                            Page 13, Region 20,\n                            Score 0.29\n                        </summary>\n                        We have discussed 223 papers on eXplainable Artificial Intelligence (XAI) for deep learning in medical image analysis. We categorized the papers based on the XAI-frameworks proposed by Adadi and Berrada (2018) and Murdoch et al. (2019). Some trends were noticeable in the surveyed papers. The majority of the papers used post hoc explanation as contrasted with model-based explanation, i.e., the explanation was provided on a neural network that had already been trained, instead of being incorporated in neural network training. Both model-specific (e.g., specifically designed for CNNs) and model-agnostic explanation methods were used. Furthermore, most of the papers investigated provided local explanation rather than global explanation, i.e., the explanation was provided per case (e.g. per patient), rather than on a datasetlevel (e.g. for all patients). Since we focus on deep learning in medical image analysis, these trends were to be expected. Most readily available XAI methods suitable for CNNs are saliency mapping techniques, which often provide post hoc, model-specific, and local explanation. Furthermore, post hoc XAI methods can be used after a neural network has been trained, making them more accessible than model-based XAI. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.29477784);\n                color: white\n                ' title='Decision making in clinical routine is rarely based on a single data modality. Usually, healthcare providers integrate a number of different data types into clinical decisions. This is especially true in hepatology - a field in which it is rare for diseases to be directly observed and the differential diagnosis can be uncertain. For example, one of the most common hepatology consults is an incidental finding of elevated liver enzymes. Diagnosing the aetiology of this abnormality requires a battery of tests, including detailed clinical history, additional laboratory tests, ultrasound, and even histopathology. Supporting, and ultimately mimicking, human decision making in such complex tasks is currently out of reach for narrow and specialised AI systems. At present, different AI approaches are required to process various types of clinical '>\n                            Page 8, Region 3,\n                            Score 0.29\n                        </summary>\n                        Decision making in clinical routine is rarely based on a single data modality. Usually, healthcare providers integrate a number of different data types into clinical decisions. This is especially true in hepatology - a field in which it is rare for diseases to be directly observed and the differential diagnosis can be uncertain. For example, one of the most common hepatology consults is an incidental finding of elevated liver enzymes. Diagnosing the aetiology of this abnormality requires a battery of tests, including detailed clinical history, additional laboratory tests, ultrasound, and even histopathology. Supporting, and ultimately mimicking, human decision making in such complex tasks is currently out of reach for narrow and specialised AI systems. At present, different AI approaches are required to process various types of clinical \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.027221892);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.03\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "c6f6181d293345058bbf1a3d13d45e14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_23257a9df8ed473cad690f3037be5ed6",
       "style": "IPY_MODEL_7faf77e9325b48eab518456869ab6ede",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "c7013c381705433ba97fbfe6dbabc19d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c74425ab69804ca38714566294a924d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_5e720a4c601846fbb307450be54537b3",
       "placeholder": "20",
       "style": "IPY_MODEL_b1c8ea4c7514411c9fd01979c8be3b83",
       "value": "20"
      }
     },
     "c7717daa5b804c2981c8d097eb6da18b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c79ad72604994c249e7c42c109f05b36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3948255fe051435faf1f8deab77d07bc",
        "IPY_MODEL_6faf735937094c69bacb598c6d9975eb"
       ],
       "layout": "IPY_MODEL_f71c076471f94fc7a96b3f0d91f46ade"
      }
     },
     "c7d155ccd3ec41b1945e85f9ff03e3b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_0d84164856fd41359e810ad40a0a50db",
       "placeholder": "",
       "style": "IPY_MODEL_5ed5fe0a669346f8a290bc7897786d5d",
       "value": "500"
      }
     },
     "c7d68814abc1446e893aec9941084043": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c82937cafb1f43989d160e586c3052ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_a78c2579c1f94410adc590113fdac7cf",
       "placeholder": "",
       "style": "IPY_MODEL_169edeb383cd4663a759cf652d0aae03"
      }
     },
     "c841dd65cfc4419fb46e95eee30e33f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c85f18ac090b47dc91d3f2a618e523b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "c8ac3dbea4dc40638424daa712a651ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "c8b79be7efe2450cbe293e880cab90e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "c91ad674340249de8b85eadf723661f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "c93600fa505a4b7ca6c2ea529e15a4bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c9481ab8552f4b98b47968f00390caae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "c9beb4c5a8e44fc982802ec02cea4d41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5617c41bfc1d4e0ebc03f5cf91d557fb",
        "IPY_MODEL_fddc769aba1a45b1a831f00a6e69b4ed",
        "IPY_MODEL_04d285352d8f4f5bafaf07a0379eb7f7"
       ],
       "layout": "IPY_MODEL_fc667849c4b1494d96c7c367f3d3b07f"
      }
     },
     "ca42103a03ca4fdc98f809dc246c7b93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ca4c787f343a41018808b53430bc53e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "ca8977c5df39497d9172cf284b644ecb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cad623efd0a84e78b171be078a47e739": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_c7d68814abc1446e893aec9941084043"
      }
     },
     "caf372f3d2424e0282df147882ab3fa3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "cb0e7e7c1bc447c1a2d58cd725394b0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "cb5773adfb144ae2ac5d93fc0f7ce50b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "cb5a6fa7341e41cd8607eaa046d889df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_44b43d37a71e43a2afc99b5b5c7f9a66",
       "style": "IPY_MODEL_45592cf1cc044094b0d660d7d8a1d090",
       "value": "3.2: Post hoc explanation"
      }
     },
     "cbb115f467db48f9b31fdd5f03898e90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "none"
      }
     },
     "cc2a63b2061d4277b834690ab2b8e176": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "cc5cac6306034184a2c907e59a73e445": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_280b888d8a7044c2918c83c917b99e4c",
       "style": "IPY_MODEL_0f17ca5cb5944585a3fbf8530f88ad77",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Med Image Anal. 2022 Explainable artificial intelligence (XAI) in deep learning-based medical image analysis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5087976);\n                color: white\n                ' title='(Guided) backpropagation and deconvolution: Some of the earliest techniques to create saliency maps highlighted pixels that had the highest impact on the analysis output. Examples included visualization of partial derivatives of the output on pixel level (Simonyan et al., 2013), deconvolution (Zeiler and Fergus, 2014), and guided backpropagation (Springenberg et al., 2014). These techniques provided local, model-specific (only for CNNs), post hoc explanation. These techniques have been used in medical image analysis. For example, de Vos et al. (2019) estimated the amount of coronary artery calcium per cardiac or chest computed tomography (CT) image slice, and used deconvolution to visualize from where in the slice the decision was based on. '>\n                            Page 3, Region 13,\n                            Score 0.51\n                        </summary>\n                        (Guided) backpropagation and deconvolution: Some of the earliest techniques to create saliency maps highlighted pixels that had the highest impact on the analysis output. Examples included visualization of partial derivatives of the output on pixel level (Simonyan et al., 2013), deconvolution (Zeiler and Fergus, 2014), and guided backpropagation (Springenberg et al., 2014). These techniques provided local, model-specific (only for CNNs), post hoc explanation. These techniques have been used in medical image analysis. For example, de Vos et al. (2019) estimated the amount of coronary artery calcium per cardiac or chest computed tomography (CT) image slice, and used deconvolution to visualize from where in the slice the decision was based on. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.8870118);\n                color: white\n                ' title='Adebayo et al. (2018) performed these two tests for many visual explanation methods including backpropagation, guided backpropagation, Grad-CAM, and guided Grad-CAM. They showed that guided backpropagation and guided Grad-CAM provided a similar visual explanation in both tests, and might be emphasizing edges. Hence, caution is advised when using such methods for visualization. '>\n                            Page 12, Region 12,\n                            Score 0.89\n                        </summary>\n                        Adebayo et al. (2018) performed these two tests for many visual explanation methods including backpropagation, guided backpropagation, Grad-CAM, and guided Grad-CAM. They showed that guided backpropagation and guided Grad-CAM provided a similar visual explanation in both tests, and might be emphasizing edges. Hence, caution is advised when using such methods for visualization. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='Eitel and Ritter (2019) evaluated the robustness of visual explanation techniques guided backpropagation, layer-wise relevance '>\n                            Page 12, Region 13,\n                            Score 1.0\n                        </summary>\n                        Eitel and Ritter (2019) evaluated the robustness of visual explanation techniques guided backpropagation, layer-wise relevance \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.90164447);\n                color: white\n                ' title='For visual explanation techniques, there is a clear distinction between’ backpropagation-based and __perturbationbased techniques with respect to their computational needs. Backpropagation-based techniques typically make a_ single pass back through the neural network, which is relatively fast. Perturbation-based techniques require, however, extensive perturbation of input images to measure the influence of these perturbations on the output. Therefore, these techniques are generally more computationally-expensive. This can especially be the case in 3-dimensional, 4-dimensional, and/or multi-modality images, which often occur in medical image analysis. '>\n                            Page 13, Region 7,\n                            Score 0.9\n                        </summary>\n                        For visual explanation techniques, there is a clear distinction between’ backpropagation-based and __perturbationbased techniques with respect to their computational needs. Backpropagation-based techniques typically make a_ single pass back through the neural network, which is relatively fast. Perturbation-based techniques require, however, extensive perturbation of input images to measure the influence of these perturbations on the output. Therefore, these techniques are generally more computationally-expensive. This can especially be the case in 3-dimensional, 4-dimensional, and/or multi-modality images, which often occur in medical image analysis. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.54238564);\n                color: white\n                ' title='For visual explanation, most backpropagation techniques have a limited number of parameters to tune. For example, in Grad-CAM, the user needs to choose at which layer to inspect the activation and in Deep SHAP, one needs to choose samples from the training set to calculate a background signal. '>\n                            Page 13, Region 12,\n                            Score 0.54\n                        </summary>\n                        For visual explanation, most backpropagation techniques have a limited number of parameters to tune. For example, in Grad-CAM, the user needs to choose at which layer to inspect the activation and in Deep SHAP, one needs to choose samples from the training set to calculate a background signal. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.34956264);\n                color: white\n                ' title='Another rapidly growing area of research is focused on improved characterisation of indeterminate liver lesions. In clinical practice, when an abdominal ultrasound shows a new liver lesion, a patient is typically referred for further imaging, with contrast-enhanced CT or MRI. Based on the fulfilment of specific radiologic criteria, certain liver lesions may be considered as having pathognomonic features of HCC, and thus do not require liver biopsy for further histological confirmation. However, liver nodules imaged by CT or MRI often demonstrate indeterminate features, for which current recommendations include either liver biopsy or close interval follow-up with serial imaging.”° This practice is sub-optimal, resulting in numerous imaging studies, patient stress, and the potential for delayed diagnoses of liver cancer. For this reason, a growing body of recent literature has explored AI approaches to improve risk stratification of indeterminate liver lesions, to facilitate earlier and more accurate detection of HCC. '>\n                            Page 4, Region 4,\n                            Score 0.35\n                        </summary>\n                        Another rapidly growing area of research is focused on improved characterisation of indeterminate liver lesions. In clinical practice, when an abdominal ultrasound shows a new liver lesion, a patient is typically referred for further imaging, with contrast-enhanced CT or MRI. Based on the fulfilment of specific radiologic criteria, certain liver lesions may be considered as having pathognomonic features of HCC, and thus do not require liver biopsy for further histological confirmation. However, liver nodules imaged by CT or MRI often demonstrate indeterminate features, for which current recommendations include either liver biopsy or close interval follow-up with serial imaging.”° This practice is sub-optimal, resulting in numerous imaging studies, patient stress, and the potential for delayed diagnoses of liver cancer. For this reason, a growing body of recent literature has explored AI approaches to improve risk stratification of indeterminate liver lesions, to facilitate earlier and more accurate detection of HCC. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.23688218);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.24\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.21671037);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.22\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.18536764);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.19\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.22243349);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.22\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.073873036);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.07\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.06942587);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.07\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. '>\n                            Page 8, Region 6,\n                            Score 0.0\n                        </summary>\n                        of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.6137302);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.61\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.49985683);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.5\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.1982236);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.2\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.019199563);\n                color: white\n                ' title='Optimistically, ML/DL systems could help resolve the diagnostic, prognostic and predictive issues that limit liver histopathology image analysis. This would improve and facilitate clinical trials in liver disease in which inclusion criteria, patient strata and histological endpoints are often manually defined by pathologists and therefore subject to intra- and inter-observer variability.*° As in other disease contexts, there is a place in clinical decision making for invasive tissue-based diagnostics. ML/DL approaches could conceivably improve the consistency, quality and amount of information which researchers and healthcare providers can extract from this tissue. The benefits of these ML/DL approaches to histopathological analysis may incentivise patients to undergo an invasive procedure such as liver biopsy. However, for some problems in the management of liver disease, non-invasive radiology images, instead of invasive diagnostics, can be analysed to unveil biomarkers. In the following section, we will review the state of the art in ML/DL approaches applied to such radiology data. '>\n                            Page 4, Region 11,\n                            Score 0.02\n                        </summary>\n                        Optimistically, ML/DL systems could help resolve the diagnostic, prognostic and predictive issues that limit liver histopathology image analysis. This would improve and facilitate clinical trials in liver disease in which inclusion criteria, patient strata and histological endpoints are often manually defined by pathologists and therefore subject to intra- and inter-observer variability.*° As in other disease contexts, there is a place in clinical decision making for invasive tissue-based diagnostics. ML/DL approaches could conceivably improve the consistency, quality and amount of information which researchers and healthcare providers can extract from this tissue. The benefits of these ML/DL approaches to histopathological analysis may incentivise patients to undergo an invasive procedure such as liver biopsy. However, for some problems in the management of liver disease, non-invasive radiology images, instead of invasive diagnostics, can be analysed to unveil biomarkers. In the following section, we will review the state of the art in ML/DL approaches applied to such radiology data. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.37214324);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.37\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.041161407);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.04\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Explainable medical imaging AI needs human-centered design a systematic review</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.02726932);\n                color: white\n                ' title='specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions '>\n                            Page 7, Region 4,\n                            Score 0.03\n                        </summary>\n                        specifically, there have been surveys focused uniquely on transparent techniques for medical imaging. The interpretability methods to explain deep learning models were categorized in detail based on technical similarities, along with the progress made on the corresponding evaluation approaches in ref. °. Another overview of deep learning-based XAI in medical image analysis is presented in ref. *°, considering a variety of techniques that were adapted or developed to generate visual, textual, and example-based explanations in the medical domain. Some of the observed trends and remarks in this survey match our perspective and recommendations in the design of transparent methods for medical imaging, including the lack of evaluation as a standard practice, the user-dependent nature of explanations, and the importance of active collaboration with experts to include domain information. Instead of proposing a general perspective in a broad range of healthcare problems, some reviews focus on specific topics of medical image analysis. Transparent ML for human experts in cancer diagnosis with Al is reviewed in ref. '° with a focus on 2 aspects: ML model characteristics that are important in cancer prediction and treatment; and the application of ML in cancer cases. These two aspects are similar to our proposed theme “Interpretability” and “task”, but we summarize the two themes in the general medical image analysis area instead of limiting to cancer studies, include more on recent studies (starting from 2012), and focus on more recent ML techniques such as Convolution Neural Networks (CNNs). Likewise, transparent ML in cancer detection is also reviewed in ref. °° and structured following the same aspects of generic transparent ML techniques, such as Local vs. Global and Ad-Hoc vs. Post-Hoc. distinctions \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "cc64820d557a46cb801943e85ce22a7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cc6f5c272b554b659a834a2b982d6da3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "ccf235b2478341dbaa58250cf59278c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_48bda2fbb52d4b629eee1a7a5b6bcb8e",
       "style": "IPY_MODEL_15dd2934fb4142b58c1297de12b935a2",
       "value": "2: Current challenges limiting AI-based approaches in the management of liver cancer"
      }
     },
     "ccf328291f594acd94f2ee365cd545cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_aa11a3acdf204a31a177a7a4320d2d10",
       "style": "IPY_MODEL_8baf6f7652d9446993908eb8e961f19c",
       "value": "3.2.1.1: Backpropagation-based approaches"
      }
     },
     "cd38bd66be394ba9b4ae63a0a5874b8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_d8d0a703038c49fda7b941813440173a",
       "style": "IPY_MODEL_f3497820af8f402b9a7586470b4dacf5",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "cd4214746a904ea2b681068b3acf2472": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "cd6d5336b259466997bf7a3bc5e779bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "cd7257f8b0f24b5db34b0453d68ec418": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cd8132b79cbe4f9ea1328891bc83c322": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c5bff01515bf4cd881ff9aad8eaf341d",
        "IPY_MODEL_20fbff9590dc40edb8c6ae23bd61fc34"
       ],
       "layout": "IPY_MODEL_1a439b02ed4849e98e3b04fdcfc63b9b"
      }
     },
     "cd96534da6d0493b95b360696e3cce6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "cdc27d0adec24f29a91291a11c822ab1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cde0d3c8d99f46d087a01c1500a85c18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_7d134349b91b475bac4a8728af1548b3",
       "style": "IPY_MODEL_abcca768afbe42dc9396e9e6562043ca",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "ce2b4b108ef54fc98745a16b0bd5db67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e4c2f7ae6e9c44ebb57ae29b50ff41be",
        "IPY_MODEL_b58349a78e81470a9cf0c90c53b61ecf"
       ],
       "layout": "IPY_MODEL_fc5a8f7cf7fa4ba49c6d6b3e5fd11155"
      }
     },
     "ce3ea150d76b42ecac100e3b323c104d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "ce71b7bd16484a60ae6b0ccb435bada0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ce742e08026d4d0ea43b2b83ec4cf436": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c1220eab179a49be8e3d6d38dbdcea1e"
       ],
       "layout": "IPY_MODEL_b005e4173ca142098a18b9c4ad5d6c8e"
      }
     },
     "cf120d7f126e45caa4465161f257e939": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "cf301cf547214704aba560e219fc5be0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_ea732ca21382469bbd054f7398ec8c14",
       "style": "IPY_MODEL_eebbef83629d46a289c708aafd53b126",
       "value": "Discussion on how AI combines textual descriptions with visual explanations for a comprehensive understanding of liver cancer pathology."
      }
     },
     "cf5773a2ebfa463087bfa92844d9a350": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "cf8c8eb79aa94ee1a114eb82e9d30e66": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "cfbd614d8a2e4814b206cf59b203a956": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_9dbeee34612d4f23ba68013d1e147586",
        "IPY_MODEL_099716819ef048739cdecaebbae3c6ef"
       ],
       "layout": "IPY_MODEL_e1ce84820d794d59b318cbf57e82516e"
      }
     },
     "cfc3ad1d8f4a4844ae834f290f54ac0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7dfbd069395c4a96936cc0f3e83ab2a8",
       "style": "IPY_MODEL_6d08dc1e3d234508b4c4d873734abcd0",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.17392404);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.17\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.97474223);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.97\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.9660468);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.97\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.4063564);\n                color: white\n                ' title='According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in '>\n                            Page 2, Region 13,\n                            Score 0.41\n                        </summary>\n                        According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.53874934);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.54\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.09319585);\n                color: white\n                ' title='TABLE 2 Top 10 institutes with publications researching the use of artificial intelligence in liver cancer. '>\n                            Page 5, Region 8,\n                            Score 0.09\n                        </summary>\n                        TABLE 2 Top 10 institutes with publications researching the use of artificial intelligence in liver cancer. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.15414411);\n                color: white\n                ' title='TABLE 3. The 10 most productive authors of publications researching the use of artificial intelligence in liver cancer. '>\n                            Page 6, Region 3,\n                            Score 0.15\n                        </summary>\n                        TABLE 3. The 10 most productive authors of publications researching the use of artificial intelligence in liver cancer. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.569427);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.57\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.327376);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 0.33\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.4341915);\n                color: white\n                ' title='of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. '>\n                            Page 8, Region 6,\n                            Score 0.43\n                        </summary>\n                        of liver fibrosis, a unified MRE liver elasticity value for liver fibrosis with different etiologies has not been established (46-48). This also indicates that the use of AI to quantitatively analyze liver fibrosis by imaging is a problem worthy of further study. In studies of AI in fatty liver disease, ultrasound is the first choice, mainly because of its high sensitivity in the diagnosis of diffuse fatty liver, convenience, costeffectiveness, and safety, and plays an important role in judging the status of liver parenchyma. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.307963);\n                color: white\n                ' title='Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. '>\n                            Page 8, Region 8,\n                            Score 0.31\n                        </summary>\n                        Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.8198496);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.82\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.7116787);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.71\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 1.0\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5121605);\n                color: white\n                ' title='Current limitations of DL approaches include overfitting of data, limited ‘explainability’ of data, and the possibility of poor generalisability, due to the inherent reliance of DL models on the size and diversity of their training dataset. In this review, we will outline the rapidly evolving role and challenges for AI in the prediction, diagnosis, and prognostication of HCC. '>\n                            Page 2, Region 4,\n                            Score 0.51\n                        </summary>\n                        Current limitations of DL approaches include overfitting of data, limited ‘explainability’ of data, and the possibility of poor generalisability, due to the inherent reliance of DL models on the size and diversity of their training dataset. In this review, we will outline the rapidly evolving role and challenges for AI in the prediction, diagnosis, and prognostication of HCC. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.53927535);\n                color: white\n                ' title='Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) '>\n                            Page 1, Region 4,\n                            Score 0.54\n                        </summary>\n                        Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.1225661);\n                color: white\n                ' title='n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) '>\n                            Page 1, Region 5,\n                            Score 0.12\n                        </summary>\n                        n recent years, the development of Artificial Intelli[= (AI) in the fields of gastroenterology and hepa tology has made remarkable progress. The use of AI is studied in gastroenterology for the endoscopic evaluation of Barrett's oesophagus, oesophageal and gastric malignancies, colorectal polyp detection and characterization, evaluation of inflammatory bowel disease and capsule endoscopy for obscure gastrointestinal bleed! (Table 1). With the increased development and usage of AI in gastroenterology, research in the field of hepatology also has accelerated. AI in hepatology can be used to detect liver fibrosis, diagnose non-alcoholic fatty liver disease (NAFLD), differentiate focal liver lesions, diagnose hepatocellular cancer, prognosticate chronic liver disease (CLD) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='Viral hepatitis is a significant cause of CLD. Liver fibrosis and CLD are risk factors for hepatocellular carcinoma (HCC) and hence death. It is practically impossible to perform a liver biopsy in all patients; hence AI algorithms have been developed for non-invasive evaluation of liver fibrosis. Some of the studies done using AI algorithms will be mentioned in the following sections. Wang D. et al'° proposed a bayesian learning algorithm to develop a three-layer artificial neural network (ANN) in patients with CHB. Age, platelet count, aspartate aminotransferase (AST), alanine aminotransferase (ALT), and gammaglutamyl transferase (GGTP) were the most critical factors '>\n                            Page 4, Region 8,\n                            Score 0.0\n                        </summary>\n                        Viral hepatitis is a significant cause of CLD. Liver fibrosis and CLD are risk factors for hepatocellular carcinoma (HCC) and hence death. It is practically impossible to perform a liver biopsy in all patients; hence AI algorithms have been developed for non-invasive evaluation of liver fibrosis. Some of the studies done using AI algorithms will be mentioned in the following sections. Wang D. et al'° proposed a bayesian learning algorithm to develop a three-layer artificial neural network (ANN) in patients with CHB. Age, platelet count, aspartate aminotransferase (AST), alanine aminotransferase (ALT), and gammaglutamyl transferase (GGTP) were the most critical factors \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.19409192);\n                color: white\n                ' title='Healthcare systems in developing countries like India have a lot of challenges, especially in the rural areas. AI helps in addressing these issues by assisting the doctors in better and quick diagnosis, delivering personalized healthcare, providing high-quality healthcare to rural areas, and helping doctors and nurses in training to handle complex medical conditions. AI can help monitor a patient’s condition having chronic ailments with the help ofa smartphone. ~ Using clinical, genetic, molecular information from large datasets, AI can be helpful to find new therapeutic targets. Apart from the extensive number of AI applications being made, a lot of unmet needs are work on alcohol related liver injury, metabolic and autoimmune liver diseases. Hence there is a lot of scope for technical growth in the AI sub-speciality, paving the way to improve the accuracy of the AI tools. AI systems for liver segmentation and diagnosis should be widely available within the next 5 years, which will help in liver lesion characterization and aid in liver transplantation. Working in isolation from AI and data scientists will be a hindrance to the growth of clinical medicine. Hence, the adoption of coordinated research opportunities will facilitate the development of many clinically useful tools. '>\n                            Page 11, Region 2,\n                            Score 0.19\n                        </summary>\n                        Healthcare systems in developing countries like India have a lot of challenges, especially in the rural areas. AI helps in addressing these issues by assisting the doctors in better and quick diagnosis, delivering personalized healthcare, providing high-quality healthcare to rural areas, and helping doctors and nurses in training to handle complex medical conditions. AI can help monitor a patient’s condition having chronic ailments with the help ofa smartphone. ~ Using clinical, genetic, molecular information from large datasets, AI can be helpful to find new therapeutic targets. Apart from the extensive number of AI applications being made, a lot of unmet needs are work on alcohol related liver injury, metabolic and autoimmune liver diseases. Hence there is a lot of scope for technical growth in the AI sub-speciality, paving the way to improve the accuracy of the AI tools. AI systems for liver segmentation and diagnosis should be widely available within the next 5 years, which will help in liver lesion characterization and aid in liver transplantation. Working in isolation from AI and data scientists will be a hindrance to the growth of clinical medicine. Hence, the adoption of coordinated research opportunities will facilitate the development of many clinically useful tools. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2353215);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.24\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "cfc7d3c753a240a89071e8092d5f4cbd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cfef1ff33aca4ee3929515d72508a006": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_614e2079ddd748c98801e6317c8f489a",
        "IPY_MODEL_e1160a58a29141e8ad85961cf882b6cb"
       ],
       "layout": "IPY_MODEL_afdd62bbfb2e41e6b9e7c37b4379e067"
      }
     },
     "d06260bef4f042fd850834dd2754aa1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "d087f20b19f04dca80c0f3fe78ffa252": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_cfc3ad1d8f4a4844ae834f290f54ac0d"
       ],
       "layout": "IPY_MODEL_0180cb96e13a4e3a9efab2828f1925a3"
      }
     },
     "d0b905ca6d3d410a91683eb0e65bb2ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "d0bc09d14720467196b07aff9385657c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_57e449eeb61a4dd7b8d7eee941756933",
       "style": "IPY_MODEL_9da176d99e3e411dba14cedb0d67cd8b",
       "value": "Insight into how textual explanations can be used in post hoc analysis of AI's decisions in liver cancer."
      }
     },
     "d0d24538e4534a49a0150cd4d04a7b08": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d0dd7962bf1a43079001d3be18d2659c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_7850f6f1b2bb4b978cdd682b97c0c4f3",
       "placeholder": "",
       "style": "IPY_MODEL_e4f050095afa46529ccf7255a9c118de",
       "value": "500"
      }
     },
     "d10a3e5d83cd4c1aa35442f7a05339bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c692a415f659487cacd30956427cac68"
       ],
       "layout": "IPY_MODEL_032c5cfd840947039785186dc90d78d5"
      }
     },
     "d13edf35e53f42a09484242611a9cbee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "d16cb108a21442aba8b26952501a03ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "d178c9db9bc447b28ffb92a4f0e4c109": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "d18ed878510241d7820448dc19b86049": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_01a7b4b31eb143228b6ba5c097e04d31",
       "style": "IPY_MODEL_63e576f577f9415c8e8cd57089a1e47f",
       "tooltip": "Retrieve related references"
      }
     },
     "d1977101a1a54fc6a24596b8ad28c697": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_19145ed162844bdfad6e74a019f65ea3",
       "placeholder": "20",
       "style": "IPY_MODEL_ddac66445b12422d80ec5c0645fe0561",
       "value": "20"
      }
     },
     "d1c61156c0c646e791769a72988339bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "d1ea06430e344566affe843dad9e0ead": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_55db91a9330747dfb014fa80d646d8ba",
        "IPY_MODEL_5a9d3aacd763457ca6dc2b11fb547aba"
       ],
       "layout": "IPY_MODEL_bb5ac4e86042416e8173e912fc5fb205"
      }
     },
     "d1fc91d952f04138888c2991ab06d1b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8fb8642dcfa2493fbce94a302af00b10",
        "IPY_MODEL_e9728ea817964193a4e5956ad6e9ed1d"
       ],
       "layout": "IPY_MODEL_6e923a41b8904be98b5865b4c7d5dac5"
      }
     },
     "d25f0276983f4347a69c563e4ee50698": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "d27236d330ff48c2931d720b879d3be1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d28e0ac895c54e8a964bc013589f99bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "d28e32602d9341d69e8f632c91772a5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_0a6ead8eded74242a2301b10d30f7432",
       "style": "IPY_MODEL_56b5005f6e354e49a9d6dc106061b7bf",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "d2d207a872944e6f926a715238b88735": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2fe474438df45e58e5fd0151f55bb6b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "d313704fced44c96a94df964e6b39302": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d37366d4603c4d10835b5b37c37c0097": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_529598a3d9b44947a26c2cab9cf429bd",
        "IPY_MODEL_7f4ab3ece61a410c8c1b8495e7ee2379",
        "IPY_MODEL_adc56abe37e04c6b83380b03c61c5df3"
       ],
       "layout": "IPY_MODEL_34688c69670f460585cf3a881cd59c92"
      }
     },
     "d375c9d62415434f9f306d5a38427813": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d3796c231ed44b7e9e20e989177b6914": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "d37a0ce7bf5045e08b14e71212994f98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9955f46ea6784f609906ff99ab8bb240",
       "style": "IPY_MODEL_ef6f6c939cc84e839cf888adc35ac615",
       "value": "queries,"
      }
     },
     "d3cca40740e344f894d06d10eca29ad5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_aa0c55d1084f46fd827d1fed66405f32",
       "placeholder": "",
       "style": "IPY_MODEL_4d69e2f061994e448ecbb616b4a0b1d9",
       "value": "500"
      }
     },
     "d3ec155a1825423bafabc0f50e1c5a08": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d40f855db0224e08aed7eef31b4a0d1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "d4490c93a5e447e9acc538bb136028bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "d469e27f87794c0e90e6392801741e21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "d493c9acebbb4c7baa92699f28075d98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "none"
      }
     },
     "d4e64d8d9dd74381ada8b52136c6d552": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "d4f999481b66475dbc771c7f3f57224b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dbcc8dffe9ad44c4b073868c9665c0bb",
       "style": "IPY_MODEL_9a24c0facce242158b83e764d796096c"
      }
     },
     "d51bcd9074e046fb980ef31dc6fd7965": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d543a9cb32714f0d804f1aaa59fffde0": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_eba448d6f7b3488cbdc189fda643a78d"
      }
     },
     "d5adcdbc60c44db4adcc922bf9631dea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f2aa817e40604577a69959437b80ec3a",
        "IPY_MODEL_328e6c7985d7490e8acc5e6fb02ec5ff"
       ],
       "layout": "IPY_MODEL_3996119138314cc9aa0c1e47f65178c6"
      }
     },
     "d5b2a3fa33194d92abd9b8528952a823": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_39e696510c81435ea9f64bd33b8af672",
        "IPY_MODEL_42a9cb3b15f94b28aed3d725b446f73b"
       ],
       "layout": "IPY_MODEL_e80608cdf9a44548aca8b2ec9c4d9c1f"
      }
     },
     "d5c5354c43c3414a826f7d96b60d3089": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d62407c3557f48ed99bc8c024f473671": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "d65418de9605423895928d09a6b6ec64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "d655a1de46544df3ad189db56b8d603d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "d663fe5c85b94ee7a14d851d765847b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_744efa524e774b1da21e060627183d9d",
        "IPY_MODEL_b9b88162452441468b455d6647a6bcad"
       ],
       "layout": "IPY_MODEL_9e3369feb3cd4c55a816c9131163510e"
      }
     },
     "d67d7ce6bdb44d78a0b1682c4dc9e61c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d6b548f994354693a6e2c42bab71b49c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3c76b27383eb4125a9e4ddec00c303aa",
        "IPY_MODEL_93255c424df3402087329cbd640fb1af"
       ],
       "layout": "IPY_MODEL_2d647745b3a7494086728ac0d0206c43"
      }
     },
     "d6b9d9989c0142ceb3c2992e549a274a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c74425ab69804ca38714566294a924d0",
        "IPY_MODEL_18fab915fde544cbbb95f41c6edf7e82"
       ],
       "layout": "IPY_MODEL_2a38bf4720084ac88066f03acc8717b9"
      }
     },
     "d6ca7d4d089a4cc9950fb37fd48d48bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d6d4a90c77804c7c89a52449fb2aae0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d7256ec849b144c79b3711924b2ee7d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f21688bec44e47dd951aba1e7052836f",
       "style": "IPY_MODEL_904a4f6e750e477b8e63776ffa6f8abf",
       "value": "words:"
      }
     },
     "d737400c1a594029a77bebd193bc1339": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "d7452445c15842549dd17f430b8a8db6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d76445354e7e4469a13abc3c1951276b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c41821b0adc94b048f6a24fc206cfd21",
        "IPY_MODEL_cd8132b79cbe4f9ea1328891bc83c322"
       ],
       "layout": "IPY_MODEL_bf8195064fa34cf28ff33ada6b745179"
      }
     },
     "d7aabad3f1d4481496a401e73414ef07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d7d679c5a281483594b57d42b03b3fb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "d7ed3d813b304f0e96e6a447ccc73134": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bc6594bf512841b9a229be2d3caf3f1a"
       ],
       "layout": "IPY_MODEL_f9510574599346149edd3579a885c079"
      }
     },
     "d7f5b73991104ed3aebe6a72b7a9e903": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7ccdeebe361e440c88fc6d26cb08d2a3",
        "IPY_MODEL_5780f840227143f2b9f27e133d8bc587"
       ],
       "layout": "IPY_MODEL_9826976d4abe480c9bb6b318cf867f34"
      }
     },
     "d84d5a81000c437eade95c1959e1d9da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_3cb8886645194155b5b564b67e1dfa8c",
       "placeholder": "",
       "style": "IPY_MODEL_8dc5da1f5cd0432a925c87b005053c72",
       "value": "500"
      }
     },
     "d84e4670add64588ba4df0fb2c4e4f9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0c258597214043a5a55d8f97cb955c9e",
        "IPY_MODEL_75de1810e2ec4b169cf1fd95cdde269c"
       ],
       "layout": "IPY_MODEL_2c593427fbb944dda492f9a33bc0cd09"
      }
     },
     "d858ff8ad659472fb60187b5ce9dafc0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d85c881b431c46c08c641cb43970b8cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_50331502177a4790ac3eb467ec6b43d0",
       "placeholder": "20",
       "style": "IPY_MODEL_e99d4239d85742e685e757f0216a4ade",
       "value": "20"
      }
     },
     "d87d7b3d76ef443f97f0d6ad6df1ff78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d888dccf610b4dd4b25ecb31491ba152": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "d89462cc617343de9265601d2d18b591": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d8d0a703038c49fda7b941813440173a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "d8db84ac9922450d8f747a52dcf3e656": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "d8dd6d655a9a48cf8994d778bc67238a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_1d860d05dc8f4adaaec0a6505a34287e",
       "style": "IPY_MODEL_2bbfda8ed4244bb8ac1401244513754a",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "d9018113b7594fc4b768b79497796765": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_5cda57119e00425fb600d97320bd3758",
       "placeholder": "",
       "style": "IPY_MODEL_f266b66dbd074a0aa8ffa26f93d1276a"
      }
     },
     "d9187795e6c44a1da7ec86088e97ec9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_89cf022f35724c77870e34db236211ab",
       "style": "IPY_MODEL_8493f59216474df58401db9996a30d64",
       "value": " to "
      }
     },
     "d928f5be474946819cea45f7e68f1546": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_176174276647453eb3876528451c410e",
       "style": "IPY_MODEL_2b6ad6fa778646b199de5bd0f5f49ba3",
       "value": "queries,"
      }
     },
     "d93ec48e687346b3aed13436cd31b916": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d944210efc184ebdb5eb697c90513801": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_16c6dd32fad14307b3ffe2f3c4bf7f40",
       "placeholder": "20",
       "style": "IPY_MODEL_3a664ebc911448c2a101c5eb1fb7b079",
       "value": "20"
      }
     },
     "d94c9508308c43809286df9645c88f22": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_9cc5ddec9dae4b2c8dbb30b3987771c6"
      }
     },
     "d94d173cd6314c92ba5bf8c6bd275fb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d97b185418ca48a88bc90f26d046ed64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_649e318fa3454104bc1450d3df87116b",
        "IPY_MODEL_319e47decfc2443da090149db53bc7d7"
       ],
       "layout": "IPY_MODEL_318b9501a0394080a4c3c95fc63b52b9"
      }
     },
     "d9aa1e7e5007425fab3f53ad85fffb29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d9df366f03ea4446ba93cf92e9474a58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d9f7cef8733042b5aa1c29244486c940": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "da586491d78d426499ed3016b4634f54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6ebf61513af84eaea5f0c6098ec4377c",
       "style": "IPY_MODEL_bb7b6dac3e914ae48f1905ccf92a45a6",
       "value": " to "
      }
     },
     "da648c9f646c4a02a5345e33490c6922": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "daed058da2b94ab1ac76c81f41e975a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "daf86b1d122e4b4094cad4fd143210f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d85c881b431c46c08c641cb43970b8cb",
        "IPY_MODEL_1de826bcc02141b084a52bf669141bf4"
       ],
       "layout": "IPY_MODEL_ca42103a03ca4fdc98f809dc246c7b93"
      }
     },
     "db3bc2a8cd73452fb6dc16d7d097e0ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "description": "Prompt",
       "layout": "IPY_MODEL_741ebfaae67149bcbfb0750f4a2787e4",
       "placeholder": "",
       "style": "IPY_MODEL_2e0453f834b841628cd141850571f337"
      }
     },
     "db477aba29654c2e800ff389cd17b6c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_24674b33400046c68c65e0d99767624e",
       "style": "IPY_MODEL_5cdb9ac5a7114081968ce2c0643c3c74"
      }
     },
     "db5ee9dc278543809d1e0e5bbbcc27e3": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_6c7eadb925874ed2b6a708dccfa45f47"
      }
     },
     "dbaf3c24684a4926aaec39c120f2490c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "dbcc8dffe9ad44c4b073868c9665c0bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dbebef3b6ac34d4ab5df0d6dcf3dc580": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b7bb540f882a48579b6aff7af05e395a",
        "IPY_MODEL_37778f57d97648fcbbca108937fce16c"
       ],
       "layout": "IPY_MODEL_dd8e271e1e254512ac1fde12e9835157"
      }
     },
     "dc15c0ac00f346f19eeb738cf6552548": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_0a6ead8eded74242a2301b10d30f7432",
       "style": "IPY_MODEL_713d99e386ed401fa0321bd9a9f35e8b",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "dc45549bbeac466f9cd8a86879e70453": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "dc9736c27e8447219b7a0d325f077f53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_2c96c71c83e84f3d9644263c0f2c30e3",
       "style": "IPY_MODEL_c57faf48b230421383edafb3ad6c063a",
       "value": "A comparative study of different AI models like support vector machines, random forests and deep learning for liver cancer analysis."
      }
     },
     "dc9e5c5d9ba04caea0e7f954ac602a56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "dcd3778529de4f57a33359b9daae0bbb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "dd0fab92c00643b29bbdba7d87cd2364": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dd46d1296f1f42708a6342dd26ece555": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_93dcafd63ad048569e9749d029cae9c7",
        "IPY_MODEL_8138b247e1e3490c956213b9afa75999"
       ],
       "layout": "IPY_MODEL_9419dddda7ab49d989eda8b1be800193"
      }
     },
     "dd639006f3ed459298a66f6b1e404f0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_d06260bef4f042fd850834dd2754aa1b",
       "style": "IPY_MODEL_697580f9c0ec4db6b86218a3469cb8aa",
       "value": "3.1.3.1: Image captioning"
      }
     },
     "dd8a625922e245f491d68a2faed704fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dd8e271e1e254512ac1fde12e9835157": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "ddac66445b12422d80ec5c0645fe0561": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ddb4d2cb810549ffa89be527ff1af753": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "de008c269a894c19965e95e37be8872d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "de1efe48578c414da4a157b6194455a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "de3b27879cd24730b28ba4c473d9bd47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_314a018c20624ed2917b66bafe98a513",
        "IPY_MODEL_747c51833dbc48e1a3444f971cc21fbb"
       ],
       "layout": "IPY_MODEL_62dcc27e7a7b4e13993a698d96c1d0de"
      }
     },
     "deb80324abf2464ba8eb328d9d95d044": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "ded7b25b60aa4cb0b7da281e597c4c28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "dee9890805cc4112b11b7859008c754d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "df4434e0eb2d48e4865348a6b1dd8e28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_296dfc1185604fdd90e8b0867e5aaf83",
        "IPY_MODEL_46e7716369364a6ea9742ae15e971a84"
       ],
       "layout": "IPY_MODEL_e772dc0df0f748f187b5f7d540c7373b"
      }
     },
     "df643d3bd93b4f649e409a37128000bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Retrieve",
       "icon": "list",
       "layout": "IPY_MODEL_d65418de9605423895928d09a6b6ec64",
       "style": "IPY_MODEL_f9d9397d753448f19b80a9a2dad8f0ca",
       "tooltip": "Retrieve related references"
      }
     },
     "df962dd95868437b80884bb4c874abfb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "dfb591bf136e4d5e8c1b93dd1aff8e08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_7c1ac99db34a44449c7bea9396771d90",
       "placeholder": "",
       "style": "IPY_MODEL_ca8977c5df39497d9172cf284b644ecb",
       "value": "500"
      }
     },
     "dfed4df6f441483b96006443fffc5f32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f4889ffe3db14e5585048de6c047d5ce"
       ],
       "layout": "IPY_MODEL_d93ec48e687346b3aed13436cd31b916"
      }
     },
     "dff09f6b015c4f878fa1ab9a47941a0c": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_f6ec410caa994547bc426959ea9de7b5"
      }
     },
     "e004f3931bd04cf4a75da0cf009174d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e00eccbe1cd94c15886b1b7f9f35a89f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "e084cd765b024bfa855eb1cd61d59429": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e0b459e14621424593223bc34aa84ac1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_e57c1327f3aa4eaabd1aa47142027d8c",
       "style": "IPY_MODEL_3a171fb18bc4439ab505e53ae41482c5",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "e0d3e5511dd54e518fa071c67f11b9c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e0dffe8fe097464bb1be36cb679fd3be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_d8d0a703038c49fda7b941813440173a",
       "style": "IPY_MODEL_148e68ae28474ea0a98010b45ccbe98c",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "e1160a58a29141e8ad85961cf882b6cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b5f100e13b7f40009471a1baf851be33",
        "IPY_MODEL_624d97c3eac14b7d8454adebce47251d",
        "IPY_MODEL_02d9fe82014f43baadd1b3646e13b3bd"
       ],
       "layout": "IPY_MODEL_ab96a947c25b41569298c749c766cd7c"
      }
     },
     "e16593fd7ba3478b845bd28b8297b29e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e180d3ca1235425ba72c665cc74c303f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e1c053ad4ad44f33ad27139fd2cb6e09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "e1ca0ad9af244524b6957a06bec7cdc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_a17cfba9bd274f61aabb41f9592d6dfc",
       "placeholder": "20",
       "style": "IPY_MODEL_bbec363fadd7403bbbea09a979a38bdd",
       "value": "20"
      }
     },
     "e1ce84820d794d59b318cbf57e82516e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "e1ffc9e73d8548e9bf7b3a111526ae29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d9187795e6c44a1da7ec86088e97ec9a",
        "IPY_MODEL_03aaa7d656bf44de8717b52f7bda2636",
        "IPY_MODEL_fec1ea69678e4a7fab611bad1a5d1690"
       ],
       "layout": "IPY_MODEL_2b42733244a84ec2b99fc0cb51cd7265"
      }
     },
     "e2054c2a31c24711a1bf1aa9de559753": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e2170c5149164c96a5f82ab174b0cbcb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_17487543800443c5a776ae63d0415979",
       "style": "IPY_MODEL_30e4cccc81f54c5fa4cb9ad1fae53a6b",
       "value": "words:"
      }
     },
     "e2bc128729b0418088be7cfe6f56f981": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "e2d69f623e97480ba02b12bc814e6ee6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_88f1f5652a6347ad9f76609285e4873d",
       "placeholder": "20",
       "style": "IPY_MODEL_526106c89b0a47b69db2b9efbe58331d",
       "value": "20"
      }
     },
     "e3143352597b4dccaa77e23bd12622e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_64cbee8cad354d20a3d7cda4e3b192bb",
        "IPY_MODEL_bfa9ffe01a7d4abcb3d5dd71185b00f7"
       ],
       "layout": "IPY_MODEL_778d769071394172afa6aa2873d66d19"
      }
     },
     "e35117e077324a3fbff5de1863715381": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "e355792bf99249e8a7f1e9b30d903d91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e3a7571e4a4a4368b45eea9902c1265d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_a677615226344d4cae803b74c63eb02b",
       "placeholder": "",
       "style": "IPY_MODEL_8ec44b551f93419ebc1e4417c2da82b1",
       "value": "500"
      }
     },
     "e3a7667b8af74643916d2902c35e6901": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e3b2bc00dabc4c96b1c5e6d80cc635d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "e3e088995f2d4ebbb57a85e4c54e9c3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e4158ca0d44247de9cf06f251078dab6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_e6ad5e5dd0a649789cc610c5752b74ca",
       "style": "IPY_MODEL_1b08e623febd451cb5451c4df7845106",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "e43783a04cac4df9859b7fbb28a75e91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "e4563c0ece72441c95fad22cfbac3b6c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "e4c2f7ae6e9c44ebb57ae29b50ff41be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_281c4bd01b0b479d83f2a2aa5b3ea1b9",
        "IPY_MODEL_7449895b932147f4852825608a4f1541"
       ],
       "layout": "IPY_MODEL_50c0cdb21ed74813be1eb36b13024a92"
      }
     },
     "e4f050095afa46529ccf7255a9c118de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e522a6b3d41f4c69a916674afd5a2a02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e547c4f8555540f1a0c072c6389a71a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "e57c1327f3aa4eaabd1aa47142027d8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "e5804e555fa74570b532499bf9c146ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c15076ed26b944c7bae6e13787f799e4",
       "style": "IPY_MODEL_099a20e196eb49a4912a638f45b06379"
      }
     },
     "e5c4246a5a72462c8d1e974b550c731c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_33578c858bd54c1fa3e24fa9bda8b10d",
       "style": "IPY_MODEL_a8c28c50dc894eff88f42fdd72ce7ed5",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "e5c58669b4a0409e976de0470c749ec7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "e5c7bca9f5424f18a50e8166f0eca60a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6049b9e76ec4eb6a98f33e6edc8ca04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5e95dc8b292d409fb84f5fbc6d442a30",
       "style": "IPY_MODEL_d9aa1e7e5007425fab3f53ad85fffb29",
       "value": " to "
      }
     },
     "e63b6721ce424effab2471be829f2e51": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "e67b1290a30448e0a303d6c8bb650308": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_8ae23346cff94d4fbe129d46ba197d09",
       "placeholder": "20",
       "style": "IPY_MODEL_8f787aa01b6a487fbe30eae647d6f3bd",
       "value": "20"
      }
     },
     "e68259c5526c4e5eb0096910bd4e5914": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "solid 1px gray",
       "border_left": "solid 1px gray",
       "border_right": "solid 1px gray",
       "border_top": "solid 1px gray"
      }
     },
     "e6980b87c9b6418a890f43ffc01965ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6ad5e5dd0a649789cc610c5752b74ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "e703a9b045774b108b56dab68512608c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e73ab56142ad4403a217eab39e4f9807": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "e772dc0df0f748f187b5f7d540c7373b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "e7a41e5faf1c4ef59245fd65b47b5925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_868bf9d6b3a648769af2b2f32c12068b",
       "placeholder": "",
       "style": "IPY_MODEL_c93600fa505a4b7ca6c2ea529e15a4bf",
       "value": "500"
      }
     },
     "e7b62835cddc4a298e46a977b4f5a0de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_27c31f332df94feebc843774c0f6b228",
       "style": "IPY_MODEL_bc92874a8e134ed6936827e894ef485e",
       "value": "A detailed analysis of the existing challenges and limitations in the use of AI for liver cancer management."
      }
     },
     "e80608cdf9a44548aca8b2ec9c4d9c1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "e8248055741648c1b87ce3580466773d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d6ca7d4d089a4cc9950fb37fd48d48bd",
       "style": "IPY_MODEL_376a0d0d8de04bcb86e9d8b2664c478c",
       "value": " to "
      }
     },
     "e852f8b85262443c8b34265f57b63076": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e87314c58adf489e92373607805ddf37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "e8a5338026df4b8b86d061c73cfdcbf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "e8dcec46504848c6be14eb9b2bf884be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e9728ea817964193a4e5956ad6e9ed1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_818cb8e99cee497db955410f24a053f1",
        "IPY_MODEL_87d0427d7bba4dba8d424e8dc4ab2ac1"
       ],
       "layout": "IPY_MODEL_1ff4573f9968423692866165feab6c9f"
      }
     },
     "e99d4239d85742e685e757f0216a4ade": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ea12afe4face41a7b2fc1a5981bb1ce5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_c01285c1c274460da50b09fa8a0e54a8",
       "style": "IPY_MODEL_b536ace00c0d4fcb8f53a1d1d5f9a2b6",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "ea158190faa341d982ec238ff53239ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3e2966e92f374e348ff15cc369ca7c31",
        "IPY_MODEL_7436da714fcb4972923b303fd692f9ec"
       ],
       "layout": "IPY_MODEL_12cfbab39eef4050bacc83c02b13d156"
      }
     },
     "ea237a0eb50f4b19bfaec399156130d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_130772658b3643aa94f3e329d9f3bd0d",
        "IPY_MODEL_74c0d2198ed0423c85ccfa52d980eea0"
       ],
       "layout": "IPY_MODEL_580a2031be4a4124afb2eb8e30e86a6a"
      }
     },
     "ea5b1e11162b470da532a4a1f850e307": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_017b6bc779b340c680c852779581e949",
       "style": "IPY_MODEL_c3770c4a3d82421c8abb0c52d6d6f4cb",
       "value": "A thorough comparison of supervised, weakly supervised, and unsupervised learning approaches in AI for liver cancer analysis."
      }
     },
     "ea6c38dac39b4edbaacd75e931cd2566": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_95a8fc5a389f444089ba551b7883ab9a",
        "IPY_MODEL_925e2e9fb0c447b18dbcb52bb64707e1"
       ],
       "layout": "IPY_MODEL_14ab93e0ac024bf68443cfdbfb38c0d8"
      }
     },
     "ea732ca21382469bbd054f7398ec8c14": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "eae845c9baba4c73aa8599fabc4e2391": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eb9e037dc44542cca077bb1a3f86c2bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eba448d6f7b3488cbdc189fda643a78d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eba8d90e47a245d3ae253c4b571baf29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec0e9407a1cc4f74a3d0eb10e002cc5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ecc1a8016e214ec288ec15a96d235631": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2391a92c08be4f59a6e4011031abe0df",
       "style": "IPY_MODEL_4fc5f91eb37c41269af94d2e69592bb3",
       "value": " to "
      }
     },
     "eccda12b9604438281550f14be96f8ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ed1b4e17e08a40589107cc72fd2d7547": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6c31b413fe48455d901beee1350acf2e",
       "style": "IPY_MODEL_b11ca6d3b9b045b895494b23b9ff057e",
       "value": " to "
      }
     },
     "ed2f371e2b8642ea93a3e2e2687b3045": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "ed4d6f4754404719aca068960ca143a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_438851cad19d4fb98b190a2010fa5472",
        "IPY_MODEL_4e7b557b9ebc4537927678d04e32dcf3"
       ],
       "layout": "IPY_MODEL_ed2f371e2b8642ea93a3e2e2687b3045"
      }
     },
     "ed553411b272454ba7d1fdf9f4ed4552": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5d2c41e6de0044feba55fdf6ddb12fa4",
        "IPY_MODEL_f13265d198a940dfaef339c8ceaa26b1"
       ],
       "layout": "IPY_MODEL_72750fe1c1b4452eb28ed183cd3bd51c"
      }
     },
     "ed7a54c72ad248de88064026b7cf63ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "edc5115e432546e6b68b5d5260ae0789": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "edee4603c7dc4544b9ef3cab703a3700": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_813af26bfe564838b8614ed4b0d319fb",
        "IPY_MODEL_d37a0ce7bf5045e08b14e71212994f98"
       ],
       "layout": "IPY_MODEL_7f879a0e1d1f41338f62df8012251214"
      }
     },
     "ee16a713233a4ae7bc3ba1de5d4b8c2b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "ee459328e1814e3681ef821b44be48c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "ee96b1ee6c504054badf6b88fb7a0121": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ee9d5c45f57d40559134e1d1a712b9fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "eebbef83629d46a289c708aafd53b126": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "transparent",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "eedfce06b0ff4ee488e1014b00f2ffe0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4c0534c9b4b04d6f98c9c1875a7556f2",
        "IPY_MODEL_d3cca40740e344f894d06d10eca29ad5",
        "IPY_MODEL_4662dd073a534092aac3e27f04095afb"
       ],
       "layout": "IPY_MODEL_19a3642b6c3f4209a7ac5a3ee55ef5cf"
      }
     },
     "eef3512720ae44a3ab8f1592c1c62a13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6dd9063a6f7c41a892490cda805df9b5",
       "style": "IPY_MODEL_e3e088995f2d4ebbb57a85e4c54e9c3c",
       "value": "queries,"
      }
     },
     "ef6f6c939cc84e839cf888adc35ac615": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef81e602cf7e4ce5a1241754a3458681": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f3859045b08a4d968d3d866f854db324",
       "style": "IPY_MODEL_aa631bcbc80a4a1680116964c2c94bd6",
       "value": " to "
      }
     },
     "ef9f0c16d43a4a21922583b73e149ef6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d5b2a3fa33194d92abd9b8528952a823",
        "IPY_MODEL_101981998945498a8ad2c9894819e529"
       ],
       "layout": "IPY_MODEL_eb9e037dc44542cca077bb1a3f86c2bf"
      }
     },
     "efcea57926e244589e664b2db0da7be9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "efd59f63844e46909069c7d2d1e945f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_21c567080bbc498b8d3b44f9c14226e9",
        "IPY_MODEL_dff09f6b015c4f878fa1ab9a47941a0c"
       ],
       "layout": "IPY_MODEL_b7d60395b11f448887a6611e436b4e94"
      }
     },
     "f0294cf7333b424da9c416ea8707b261": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0b4540fcbd37436c998a7b16102e7190",
        "IPY_MODEL_bda1ae3304f54cdbbf4578b84eecf6b6"
       ],
       "layout": "IPY_MODEL_0edadb5292334c539eef006d6be75894"
      }
     },
     "f03e7ab62044409ab5cb15afe65fd610": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f0b2bac40e2c448282a5c4f9c5dabef3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f0c116f5b89247e6a5d885611346a7a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f0d9675d1b244ed795dbc3c6a6122c0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f116a9bd3db24418851f7a1a5d251b8e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "f1198faa874746c5b3a68e8782279ee5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_df643d3bd93b4f649e409a37128000bb",
        "IPY_MODEL_8309b005ad2b4a45990991b1246f4a79"
       ],
       "layout": "IPY_MODEL_201ee6145ae2408495899be455562164"
      }
     },
     "f13265d198a940dfaef339c8ceaa26b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e67b1290a30448e0a303d6c8bb650308",
        "IPY_MODEL_b1c791cc493c44d49da569856080efff"
       ],
       "layout": "IPY_MODEL_7e7b5922b10041e18e12d47976803acc"
      }
     },
     "f176e64767d74a79ad1f46e49366f397": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_854dd39513f84d41a7d8812c8dc8d68d",
       "style": "IPY_MODEL_e0d3e5511dd54e518fa071c67f11b9c7",
       "value": "queries,"
      }
     },
     "f1797caaeb2d4d8da66408249ebc4d0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bd41edc7affb48a6ad73b0cc7eb60d1f",
        "IPY_MODEL_b78037a9d73d4775923784e5f050f7f3"
       ],
       "layout": "IPY_MODEL_7f851422a0d14cd893c8c6df995bab66"
      }
     },
     "f1eb106fb4a14bcb99bdf66ce5d0c05a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_01ee66589d6d4ed4881446b86156d9c0",
        "IPY_MODEL_9eff32b2b9b84e7a8afdb2bc499d0b31"
       ],
       "layout": "IPY_MODEL_6e4a338982a54de6aae032892831c15c"
      }
     },
     "f21688bec44e47dd951aba1e7052836f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f2427886019b41b49ba047df07966100": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_12e15e667e7a4ab6ab4cc78fc947f1c9",
       "style": "IPY_MODEL_9cd265360e254111b6975a7953ca437d",
       "value": "words:"
      }
     },
     "f266b66dbd074a0aa8ffa26f93d1276a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "f28ecc4de59e4b36be273d5bc3175b19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "f2aa817e40604577a69959437b80ec3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_59b95a3ead924587879e1d7becb4a122",
       "style": "IPY_MODEL_a849e0e203fd4a48ab8f8f703046ea6f",
       "value": "A comprehensive exploration of the current advances, challenges, and strategies in applying AI for pathological analysis of liver cancer."
      }
     },
     "f2e430c3aae74dcb96b3ffa169525c2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d89462cc617343de9265601d2d18b591",
       "style": "IPY_MODEL_22837da3bdde45b0aab15a3b8c04d49f",
       "value": "words:"
      }
     },
     "f2f44090f31b440dbf4141d327b43195": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f3497820af8f402b9a7586470b4dacf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "f35bd451c57940c890cbd7a63260754e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9fa873799b4747ce866fecf1c809ddaa",
       "style": "IPY_MODEL_ee96b1ee6c504054badf6b88fb7a0121",
       "value": "words:"
      }
     },
     "f3859045b08a4d968d3d866f854db324": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f3e294b6c53349c7b0d870285323155e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_da586491d78d426499ed3016b4634f54",
        "IPY_MODEL_d84d5a81000c437eade95c1959e1d9da",
        "IPY_MODEL_d7256ec849b144c79b3711924b2ee7d4"
       ],
       "layout": "IPY_MODEL_95d52bb292f3466f8632022c2afd356a"
      }
     },
     "f4172ce9ecb441218808a5dc3d1211b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f467b4b728864207ab004172245cba9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f4889ffe3db14e5585048de6c047d5ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_21999ae90f8543389b23f5a735ff9a89",
       "style": "IPY_MODEL_f87cbfc376eb4bd7b16145432b6a137e",
       "value": "\n        <details open>\n            <summary>\n                Related References\n            </summary>\n            <div class='query_results'>\n                <ol>\n                    <li><h3>JOH 2022 Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.90962255);\n                color: white\n                ' title='It has been posited that improved HCC risk prediction models leveraging AI techniques could be used to personalise HCC surveillance strategies by improving risk stratification of patients with chronic liver disease. For example, Ioannou and colleagues found that targeting patients with the uppermost 51% of their NN-derived HCC risk score would include 80% of patients who would develop HCC within the subsequent 3 years.° Such an approach could be useful in resource-limited settings that do not have sufficient capacity for regular HCC surveillance in all at-risk patients. However, to date, the clinical utility of this and other Al-based scores for predicting risk of HCC is unclear, particularly as these data have limited generalisability, given their reliance on the size and diversity of the training dataset. '>\n                            Page 3, Region 5,\n                            Score 0.91\n                        </summary>\n                        It has been posited that improved HCC risk prediction models leveraging AI techniques could be used to personalise HCC surveillance strategies by improving risk stratification of patients with chronic liver disease. For example, Ioannou and colleagues found that targeting patients with the uppermost 51% of their NN-derived HCC risk score would include 80% of patients who would develop HCC within the subsequent 3 years.° Such an approach could be useful in resource-limited settings that do not have sufficient capacity for regular HCC surveillance in all at-risk patients. However, to date, the clinical utility of this and other Al-based scores for predicting risk of HCC is unclear, particularly as these data have limited generalisability, given their reliance on the size and diversity of the training dataset. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.20295301);\n                color: white\n                ' title='The past 20 years have witnessed an explosion in the availability of large, complex data sets with genomic and molecular data from bulk tissues and from single cells. Consequently, AI algorithms leveraging integrative multiomics approaches have also been designed to improve the detection and characterisation of HCC tumours. Such integrated algorithms have shown promise for informing disease diagnosis and staging, and for the prediction of disease recurrence and _ therapeutic response.?**° '>\n                            Page 6, Region 4,\n                            Score 0.2\n                        </summary>\n                        The past 20 years have witnessed an explosion in the availability of large, complex data sets with genomic and molecular data from bulk tissues and from single cells. Consequently, AI algorithms leveraging integrative multiomics approaches have also been designed to improve the detection and characterisation of HCC tumours. Such integrated algorithms have shown promise for informing disease diagnosis and staging, and for the prediction of disease recurrence and _ therapeutic response.?**° \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Quantitative analysis of artificial intelligence on liver cancer</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.3124013);\n                color: white\n                ' title='With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). '>\n                            Page 2, Region 5,\n                            Score 0.31\n                        </summary>\n                        With the development of medical big data and computer technology, artificial intelligence (AI) based on machine learning and deep learning has been widely used in current medical research (3-6). Through self-learning, summary, and induction of data, it can produce an intelligent reasoning system and choose the optimal solution to guide clinical decision-making (7). Original AI was based on traditional machine-learning methods, including support vector machine and random forest models, which all relied on human experience for learning and simple summary. As early as 2003, Hussain constructed a predictive system consisting of 12 genes, with Fisher’s linear classifier, for predicting early recurrence in patients with hepatocellular carcinoma (HCC) (8). During this period, most studies have focused on simple analyses of data, such as genes and molecules (9-11). With the standardization of imaging diagnosis and its important role in the clinical diagnosis of liver cancer, AI research based on imaging has emerged by extracting high-throughput features that cannot be detected and defined by human eyes from large-scale image data to establish an intelligent decision -making model to assist clinical decisionmaking (12, 13). In particular, deep learning based on convolutional neural networks (CNNs) has promoted progress in liver cancer research (14-19). \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.60153913);\n                color: white\n                ' title='As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all '>\n                            Page 2, Region 6,\n                            Score 0.6\n                        </summary>\n                        As more and more researchers are interested in the use of AI in liver cancer, a large number of related studies have started being published. For example, reviews describing an overview of deep learning, convolutional neural networks and other AI technologies applications in liver cancer (20-22), reviews on the applications of AI on assisted imaging in diagnosis, prognosis and detection of liver cancer (23-25), and explained the latest research, on limitations and future development trends of AI have all been recently published. However, current reviews may be unable to explore grasp the latest research trends and hotspots in this field because of lack of a large number of publications. Meanwhile, there is a lack of quantitative analysis of all \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.57191336);\n                color: white\n                ' title='Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. '>\n                            Page 2, Region 8,\n                            Score 0.57\n                        </summary>\n                        Therefore, we aimed to quantitatively analyze existing studies involving AI in liver cancer using bibliometrics to provide the current research progress, hotspots, and emerging trends for AI in liver cancer which may help researchers better understand grasp future research interest. Information was collated regarding countries/regions, institutions, authors, and journals with the highest citations and publications and keywords. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.19579212);\n                color: white\n                ' title='According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in '>\n                            Page 2, Region 13,\n                            Score 0.2\n                        </summary>\n                        According to our research area, which focuses on the applications of AI in liver cancer, we designed the following search items: the papers for analysis were restricted to those that (1) were written in \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.16946259);\n                color: white\n                ' title='The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. '>\n                            Page 3, Region 5,\n                            Score 0.17\n                        </summary>\n                        The analysis of the global trend of publications and citations and productive countries/regions is mainly to comprehensively understand the development trends of AI on liver cancer from beginning to end. The analysis of institutions, authors, and co-cited authors can quantitatively describe the strength of the cooperation between authors and institutions (30-32). Additionally, the analysis of top journals can analyze the level of cooperation and relationships in the concentrated fields of journals, which is beneficial to cross field cooperation in research (32). In particular, cluster co-occurrence analysis of keywords from different perspectives such as disease, data type, clinical goals, and clinical methods can help us understand the main topics and research trends in the current field of AI in liver cancer field. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.17327148);\n                color: white\n                ' title='In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. '>\n                            Page 7, Region 5,\n                            Score 0.17\n                        </summary>\n                        In this quantitative study, in order to systematically and quantitatively analyze the research status of AI in liver cancer, and explore the future research trends and hotspots in this field, we used a bibliometrics method to analyze the current research status of AI in liver cancer in terms of publication and citation trends, countries/ regions and institutions, authors and co-cited authors, journals, cited references and co-citation references, and keywords. Ultimately, 1724 articles focusing on Al in liver cancer were collected from the WoSCC database and analyzed. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 1.0);\n                color: white\n                ' title='Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. '>\n                            Page 8, Region 4,\n                            Score 1.0\n                        </summary>\n                        Regarding data type, studies of AI in liver cancer started from the simple data modeling of genetic or molecular data (9-11). With the development of medical imaging, research on medical imaging has been gradually increasing. CT, ultrasound, and MRI are the top three most used data types. First, this may because CT and MRI can be used as the basis for clinical treatment strategies for patients with liver cancer based on guidelines for liver cancer diagnosis. Moreover, ultrasound, as a screening method for patients at high risk of liver cancer, needs to be checked every six months. Therefore, the data volume of these three imaging methods has greatly increased, which has promoted the development of AI in liver cancer (37-39). Second, compared with MRI, CT has the advantages of fast inspection speed and cost-effectiveness, and is an indispensable and important imaging method in the diagnosis and treatment of liver cancer. Finally, although ultrasound is widely used in clinical practice, its image acquisition is seriously affected by the doctor’s operation technique and machine model, the resolution is low, and the processing is difficult. Therefore, it is used less often than CT. However, it is worth noting that contrast-enhanced ultrasound has now been included as a recommended imaging modality for the diagnosis of liver cancer (40, 41) and is also widely used in the development and prognostic evaluation of ultrasound-guided radiofrequency ablation. This suggests that we could pay attention to the important role of ultrasound in liver cancer clinics in future research. At the same time, few studies used pathological, genetic, and other clinical data (42-44). The main reason may be that the medical cost of genetic examination is high and the realization of AI in multiomics research is difficult. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.65393263);\n                color: white\n                ' title='Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. '>\n                            Page 8, Region 8,\n                            Score 0.65\n                        </summary>\n                        Studies on the treatment and prognosis of liver cancer mainly focused on the survival of a specific surgical method (59-66), such as radiofrequency ablation, transarterial chemoembolization and etc. Reports have proven that the modern therapies integrate a variety of neoadjuvant and adjuvant strategies have achieved dramatic improvements in survival, especially for patients with advanced HCC (66, 67). But the division of the patient population, the choice of potentially disclosing novel biomarkers still are controversies and the decision-making of precision treatment methods adapted to the specific patients, AI can play a role in this, but related research has not yet been seen. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.1601749);\n                color: white\n                ' title='Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a '>\n                            Page 9, Region 5,\n                            Score 0.16\n                        </summary>\n                        Previous meta-analyses and literature reviews focused on the applications of specific technologies in liver cancer or the development status of specific liver disease (22-29), such as reviewing studies on AI on assisted imaging in the diagnosis, prognosis and detection of liver cancer, or explaining the latest research, limitations, and future development trends of AI in a \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.0);\n                color: white\n                ' title='certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. '>\n                            Page 9, Region 6,\n                            Score 0.0\n                        </summary>\n                        certain direction. However, they lack a quantitative analysis based on the available literatures. Therefore, a bibliometrics analysis was conducted in our study to summary the research status of AI in liver cancer. Bibliometrics analysis uses mathematical and statistical methods to study the literature system and bibliometric characteristics in a given field to mine the distribution structure, quantitative relationships, and changes of literature in this field. Visual display with the help of special software plays an important role in understanding the current development status and development trend of the field. However, our research also has limitations. First, we only included English articles in the WoSCC database and did not include articles in other databases or languages, which could lead to the omission of many studies. Second, keyword screening may not be perfect and could lead to omission of literature. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.8894144);\n                color: white\n                ' title='This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. '>\n                            Page 10, Region 2,\n                            Score 0.89\n                        </summary>\n                        This study used bibliometrics to conduct an in-depth analysis of the published literature on AI in liver cancer. The results showed that AI has undergone rapid development and has a wide application in the diagnosis and treatment of liver diseases, especially in China, which has one of the highest incidences of liver cancer compared to other countries the world. In addition, intelligent analysis of imaging data is the hotspot and focus of current research in this field. However, combined with the current clinical difficulties such as accurate screening of early-stage liver cancer patients and high-risk patients, and selection of reasonable treatment decisions for advanced liver cancer patients, the use of AI for the fusion analysis of multiple types data in the process of diagnosis and treatment of liver cancer and multi-modal treatment decision-making for liver cancer are still relatively rare, and may become a future research trend. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial Intelligence in Hepatology Ready for the Primetime</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.2702545);\n                color: white\n                ' title='Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) '>\n                            Page 1, Region 4,\n                            Score 0.27\n                        </summary>\n                        Artificial Intelligence (AI) is a mathematical process of computer mediating designing of algorithms to support human intelligence. AI in hepatology has shown tremendous promise to plan appropriate management and hence improve treatment outcomes. The field of AI is in a very early phase with limited clinical use. AI tools such as machine learning, deep learning, and ‘big data’ are in a continuous phase of evolution, presently being applied for clinical and basic research. In this review, we have summarized various AI applications in hepatology, the pitfalls and AI's future implications. Different AI models and algorithms are under study using clinical, laboratory, endoscopic and imaging parameters to diagnose and manage liver diseases and mass lesions. AI has helped to reduce human errors and improve treatment protocols. Further research and validation are required for future use of AI in hepatology. (J Ciin Exp HepaTor 2023;13:149-161) \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.5063327);\n                color: white\n                ' title='Healthcare systems in developing countries like India have a lot of challenges, especially in the rural areas. AI helps in addressing these issues by assisting the doctors in better and quick diagnosis, delivering personalized healthcare, providing high-quality healthcare to rural areas, and helping doctors and nurses in training to handle complex medical conditions. AI can help monitor a patient’s condition having chronic ailments with the help ofa smartphone. ~ Using clinical, genetic, molecular information from large datasets, AI can be helpful to find new therapeutic targets. Apart from the extensive number of AI applications being made, a lot of unmet needs are work on alcohol related liver injury, metabolic and autoimmune liver diseases. Hence there is a lot of scope for technical growth in the AI sub-speciality, paving the way to improve the accuracy of the AI tools. AI systems for liver segmentation and diagnosis should be widely available within the next 5 years, which will help in liver lesion characterization and aid in liver transplantation. Working in isolation from AI and data scientists will be a hindrance to the growth of clinical medicine. Hence, the adoption of coordinated research opportunities will facilitate the development of many clinically useful tools. '>\n                            Page 11, Region 2,\n                            Score 0.51\n                        </summary>\n                        Healthcare systems in developing countries like India have a lot of challenges, especially in the rural areas. AI helps in addressing these issues by assisting the doctors in better and quick diagnosis, delivering personalized healthcare, providing high-quality healthcare to rural areas, and helping doctors and nurses in training to handle complex medical conditions. AI can help monitor a patient’s condition having chronic ailments with the help ofa smartphone. ~ Using clinical, genetic, molecular information from large datasets, AI can be helpful to find new therapeutic targets. Apart from the extensive number of AI applications being made, a lot of unmet needs are work on alcohol related liver injury, metabolic and autoimmune liver diseases. Hence there is a lot of scope for technical growth in the AI sub-speciality, paving the way to improve the accuracy of the AI tools. AI systems for liver segmentation and diagnosis should be widely available within the next 5 years, which will help in liver lesion characterization and aid in liver transplantation. Working in isolation from AI and data scientists will be a hindrance to the growth of clinical medicine. Hence, the adoption of coordinated research opportunities will facilitate the development of many clinically useful tools. \n                    </details>\n                </li>\n\n                \n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.40912285);\n                color: white\n                ' title='AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. '>\n                            Page 11, Region 6,\n                            Score 0.41\n                        </summary>\n                        AI is an upcoming promising technology that is rapidly becoming an essential part of patient management. Applications of AI have expanded in all branches of medicines, especially endoscopy and hepatology. The conglomeration of data which can be clinical/laboratory, multi-omics, natural language processing (NLP) and Image recognition (both radiology-based and pathology-based) has contributed to the prediction of fibrosis, classification of liver masses and prediction of treatment response and transplant outcomes. ’® In this review the majority of studies mentioned focussed on diagnosis part. There are very few studies that help to predict treatment response, post-liver transplant response, and prediction of hepatotoxicity in newer drug development and more studies are needed. AI also helps for realtime biomonitoring, by identification of patients at high risk of clinical decompensation and hospital admission, so that timely intervention can be done for high-risk patients. With the increasing advancement of image capture and storage, AI will bring striking changes to the diagnosis of various liver diseases with the ‘big data’ being available. However, there are many hurdles to overcome, which researchers will do in the near future using validation studies and molecular research. It is expected that gastroenterology and hepatology will be one of the first areas in medicine to introduce AI tools on a wide-scale basis, due to its inherent reliance on endoscopic and radiological imaging. Hence, GI and liver specialists should be proud that our field sets the ground for AI development in medicine. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Implementation of deep learning in liver pathology optimizes diagnosis of benign lesions and adenocarcinoma metastasis</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.3179367);\n                color: white\n                ' title='In summary, we show for the first time that a comprehensive series of automated identification and classification of common benign and malignant lesions in the liver is possible by deep learning on scanned histological tissue sections. Our work can contribute to an objective and efficient workflow in routine diagnostics for highly relevant diagnostic questions, such as the differentiation between benign and malignant structures and the origin of frequent types of metastasis. This tool may aid pathologists, especially in situations where limited tissue is available, to establish and confirm the diagnosis. Furthermore, we provide an exceptional annotated liver dataset for the development and validation of deep learning algorithms which we provided to the scientific community. At the end, this may be a step towards improved personalized oncology therapy concepts, which will in the future integrate large clinical, radiological and pathological data sets using artificial intelligence. '>\n                            Page 12, Region 6,\n                            Score 0.32\n                        </summary>\n                        In summary, we show for the first time that a comprehensive series of automated identification and classification of common benign and malignant lesions in the liver is possible by deep learning on scanned histological tissue sections. Our work can contribute to an objective and efficient workflow in routine diagnostics for highly relevant diagnostic questions, such as the differentiation between benign and malignant structures and the origin of frequent types of metastasis. This tool may aid pathologists, especially in situations where limited tissue is available, to establish and confirm the diagnosis. Furthermore, we provide an exceptional annotated liver dataset for the development and validation of deep learning algorithms which we provided to the scientific community. At the end, this may be a step towards improved personalized oncology therapy concepts, which will in the future integrate large clinical, radiological and pathological data sets using artificial intelligence. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Artificial intelligence in liver diseases Improving diagnostics, prognostics and response prediction</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.27707916);\n                color: white\n                ' title='To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. '>\n                            Page 5, Region 2,\n                            Score 0.28\n                        </summary>\n                        To facilitate transformation of imaging data into clinically accessible information, Al may derive predictions in a more personalised fashion. Two categories of AI that have shown promise in liver imaging are radiomics (relying on classical ML) and DL systems (relying on CNNs) (Fig. 2A). Radiomics is a strongly supervised and expert-guided approach where hardcoded algorithms extract quantitative image features that are fed into an ML algorithm.°® In contrast, DL with a CNN constitutes an automatic feature extraction where the algorithm selflearns salient features and self-optimises parameters by running an input image through mathematical operations embedded in multiple layers.°°? Because both approaches aim to predict a pre-defined “ground truth,” they are considered supervised learning approaches. Herein, we review AI tools for liver imaging in segmentation, classification of disease severity and lesions, and outcome prediction. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>Development of a deep pathomics score for predicting hepatocellular carcinoma recurrence after liver transplantation</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.15249693);\n                color: white\n                ' title='Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. '>\n                            Page 3, Region 7,\n                            Score 0.15\n                        </summary>\n                        Recent advances in artificial intelligence (AI) methodologies have made great strides in automatically quantifying pathological patterns based on digital histological slides [18]. With the integration of digital slides into the pathology workflow, advanced algorithms and computeraided techniques expand and reinforce their utilization in tumor diagnosis, prognostic prediction and therapy targeting, which enable the interpretation of information beyond human limits and ultimately, improve patient management [19-21]. For HCC, survival indicators after liver resection were proposed based on weakly supervised deep learning methods, exhibiting high accuracy [22, 23]. With largely uncovered invisible information available from HCC histology, further integration of recurrence prediction models and AI algorithms in transplant patients suffering from HCC deserve to be explored. Moreover, a comprehensive research on correlation between HCC histological structures and prognosis is urgently needed. \n                    </details>\n                </li>\n\n                \n</ol></li><li><h3>JOH 2017 Histological Subtypes of Hepatocellular Carcinoma Are Related To Gene Mutations and Molecular Tumour Classification</h3>\n\n<ol>\n\n                <li>\n                    <details>\n                        <summary style='\n                background-color: rgba(0, 150, 0, 0.10505869);\n                color: white\n                ' title='Although this increasing understanding of HCC biology holds promise for future targeted therapies,and personalized care, its translation into clinical practice will require a precise knowledge of its relationship to tumour phenotype. Here, we aimed to determine how HCC molecular features relate to its phenotype by combining comprehensive pathological analyses, gene expression profiling and gene sequencing in a series of 343 resected tumours developed in patients with various underlying liver diseases. '>\n                            Page 5, Region 6,\n                            Score 0.11\n                        </summary>\n                        Although this increasing understanding of HCC biology holds promise for future targeted therapies,and personalized care, its translation into clinical practice will require a precise knowledge of its relationship to tumour phenotype. Here, we aimed to determine how HCC molecular features relate to its phenotype by combining comprehensive pathological analyses, gene expression profiling and gene sequencing in a series of 343 resected tumours developed in patients with various underlying liver diseases. \n                    </details>\n                </li>\n\n                \n</ol></li>\n                </ol>\n            </div>\n        </details>\n        \n        <style>\n            .query_results {\n                max-height: 800px;\n                overflow-y: auto;\n                border: 1px solid gray;\n            }\n        </style>\n        "
      }
     },
     "f4a91ce12f684b4ab95e88036eefe612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d0bc09d14720467196b07aff9385657c",
        "IPY_MODEL_ce742e08026d4d0ea43b2b83ec4cf436"
       ],
       "layout": "IPY_MODEL_964b66725afc411ab06e0157af1f4f49"
      }
     },
     "f4b851fb9b144ad69b12dee269327fdd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f4bc48b06dae4a56b6267b535efd9813": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "90%"
      }
     },
     "f53da1bffc6541f8a405d929c0d45b91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c0a7858b7c8e49608e252b556ba51931",
        "IPY_MODEL_c39424f084f747669536c49e43eed374"
       ],
       "layout": "IPY_MODEL_3dd7be7cf3534ea596bd384327b59882"
      }
     },
     "f559b1667775498aa2591060573aa976": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "f57709a9953840a18ca55f7f52cf14a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "f57f070eb4a148a6b78c1443e6989b1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Summarize",
       "icon": "rocket",
       "layout": "IPY_MODEL_079cb1bc90ba4602b2792b4a330e272d",
       "style": "IPY_MODEL_adf7c53353b74c2e9802ba483daae88a",
       "tooltip": "Summarize this section based on the topic and intro"
      }
     },
     "f5bce05c77b448998c1f04b863ca28b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "f5bcfe88426b49f3b9e887e7be471c8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f5cce34691a94385ada25f2f8160dc63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f5ec7cfb0c4c4dc2a5acc8acdb861804": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f600a734e9984ff490f7c9a8e5c0d668": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f6ec410caa994547bc426959ea9de7b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f71c076471f94fc7a96b3f0d91f46ade": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "50%",
       "width": "50%"
      }
     },
     "f751eb81226b4f05bfafaa49d8dd50ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "f7b8fdc6cfc9457ebdd5ece8cf0ad97f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "f7f0fc8805204b79879b45d1d0cbb8fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f80be323676c4511bb462353674171bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid transparent",
       "border_left": "1px solid transparent",
       "border_right": "1px solid transparent",
       "border_top": "1px solid transparent",
       "display": "none",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "f87cbfc376eb4bd7b16145432b6a137e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f8a0b17581b64b1e873a4ee92e2e979a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_223f3d4b1b1348518439ac22c3da3399",
        "IPY_MODEL_3b59bc009bc740ab9496ad0f1f5c3dee"
       ],
       "layout": "IPY_MODEL_6cc05aac9a814470a58238a6b6f2c614"
      }
     },
     "f8a9bb4b64494a5896246a44eec1631a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f8aeb69a1a404afc8c48a1aa3bfef6e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e355792bf99249e8a7f1e9b30d903d91",
       "style": "IPY_MODEL_5d59e03db94b4a24899d4d4075b04180",
       "value": " to "
      }
     },
     "f94216d4ee254fc1bced67491fe482b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "60px"
      }
     },
     "f94fa13511a8419eaf09d9c0f74d80dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a6cd6c822f1941baade1e210ac6c94ae",
        "IPY_MODEL_6cc847e9e0f143ffb440f6a61e93ee2d"
       ],
       "layout": "IPY_MODEL_edc5115e432546e6b68b5d5260ae0789"
      }
     },
     "f9510574599346149edd3579a885c079": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f9d9397d753448f19b80a9a2dad8f0ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "button_color": "darkgreen",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "f9fd1c7449cf4579a273a9ead14a795f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fa34962f1e85432380922a004530595c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_701d296b44e8411ea9b1e6bef8c7e72b",
        "IPY_MODEL_2152a561b80442e89687e3fb5bf3e749"
       ],
       "layout": "IPY_MODEL_0e1847704d93448b86dadfa54e07b9f9"
      }
     },
     "fa7bc4577ba84742816c4e8f8cb9052d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fab6fff9f748475684c68d036babc64c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_56483a06b4fd4f08a43afe0cded13600",
       "style": "IPY_MODEL_76c19729a08a4d378851c0b2767e48ad",
       "value": " to "
      }
     },
     "fb2071c18bcf4e04ba8a62947386ce1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start",
       "width": "40px"
      }
     },
     "fb21a67e33aa40cb83c468fc22432767": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "justify_content": "flex-start"
      }
     },
     "fb6de84608ce4cdabc606de3f5765b21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "50%"
      }
     },
     "fb77388d36c74019b88e1f164a315618": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_1c89cc1c10624d68961688d1d1e67b9e"
      }
     },
     "fbd3061e6c594ab8856b566e6562dfa3": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_02faa3f36fab4537ac7f84f21e633f2b"
      }
     },
     "fc25b44509084625a5e7dcfbe364d18d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_a396875fff04407a8d44d40bcc0c09dc",
       "style": "IPY_MODEL_7e63a7d032624dc0b0446930d84d2ef3",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "fc5a8f7cf7fa4ba49c6d6b3e5fd11155": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fc667849c4b1494d96c7c367f3d3b07f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fcb4e51dd6c541a3b15db1a96256118f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "fd54dd6ea97c4c249fc98d5a95b64464": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Translate",
       "icon": "language",
       "layout": "IPY_MODEL_33578c858bd54c1fa3e24fa9bda8b10d",
       "style": "IPY_MODEL_a6ef25b712024faa8f858db73cc51d5e",
       "tooltip": "Click and translate the details for this section"
      }
     },
     "fd8237afc74f452a9575803f8624b16a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fdb83e2731d4477480e2398552bcdf32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_30a16f50c11340a9a9be5a0898b2399c",
       "style": "IPY_MODEL_0881152d236c40699785d38c23808917",
       "value": " to "
      }
     },
     "fdd5b1b1f4e2494eafb9dedaa7c39e68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8a311fee2adf48fe8aebaff84a95a2fa",
       "style": "IPY_MODEL_279602c0e3004ed1b75927c7fda924f4",
       "value": "words:"
      }
     },
     "fddc769aba1a45b1a831f00a6e69b4ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_0546aedaf5b545799d176f5a3b9030b8",
       "placeholder": "",
       "style": "IPY_MODEL_e084cd765b024bfa855eb1cd61d59429",
       "value": "500"
      }
     },
     "fde76fd605f941598f090c2f9126695e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4b809589de74427f82917ba7031b551d",
       "style": "IPY_MODEL_fa7bc4577ba84742816c4e8f8cb9052d",
       "value": " to "
      }
     },
     "fe2538abc04f457c8b13fa35e6ccad37": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "border_bottom": "1px solid purple",
       "border_left": "1px solid purple",
       "border_right": "1px solid purple",
       "border_top": "1px solid purple",
       "justify_content": "flex-start",
       "width": "99%"
      }
     },
     "fe275e4e0f7d49fa86980a7acdb19e85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b5a89e3a5ffe4810b0fca965191a8020",
        "IPY_MODEL_c79ad72604994c249e7c42c109f05b36"
       ],
       "layout": "IPY_MODEL_37e64dd3daf2417cbc89179cc8beab50"
      }
     },
     "fe6070b6d2b04ab9bb3740f00250374a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_0031753d6f644f52ad463a70cd991483",
       "style": "IPY_MODEL_53a8cb313322497cb1b354adf8758d4f",
       "value": "1: Current advances of AI-based approaches for clinical management of liver cancer"
      }
     },
     "fe6412b34f584240bac944f109d00938": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "layout": "IPY_MODEL_4e57f4807409466e8a5ebaff057f37ca",
       "style": "IPY_MODEL_d178c9db9bc447b28ffb92a4f0e4c109",
       "value": "4: Conclusion and future applications"
      }
     },
     "fec1ea69678e4a7fab611bad1a5d1690": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b8645e1167e841928545a8f0605f63bb",
       "style": "IPY_MODEL_8c44225f3f3043cfba9e4501567cc975",
       "value": "words:"
      }
     },
     "feca6d14eb094f9780aa9513396b207a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b20c748bc74943ddb1252023173c650c",
        "IPY_MODEL_4d78eab806be49de8ab30c1be2bfe06d"
       ],
       "layout": "IPY_MODEL_2b89e81a9a4e405a9fa3eaa7f7d0d9aa"
      }
     },
     "ff07296e2ecc4da6aa975b55ab453c41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "background": "#000000",
       "description_width": "50px",
       "font_size": null,
       "text_color": null
      }
     },
     "ff1fef0a4d514b93aa2d471f89773c34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0ed75aecce364e49bdd340dbb2ae9358"
       ],
       "layout": "IPY_MODEL_bc73d82b677846ce9b5be0671d010a80"
      }
     },
     "ff357e1b6cd0482dbdf34ab8c60453d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ffbef41f7d994e6ea7bd8bd06738cc7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ffe95e3f92fe45bc84c0933529809c72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
